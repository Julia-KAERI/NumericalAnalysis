[
  {
    "objectID": "src/numerical_analysis_using_julia/index_part2.html",
    "href": "src/numerical_analysis_using_julia/index_part2.html",
    "title": "수치해석 II",
    "section": "",
    "text": "수치해석에 대해 심화된 내용을 다룬다. 하나의 행렬을 다루기 쉬운 행렬의 곱으로 표현하는 분해 (decomposition/factorization), 선형방정식의 해를 반복법(iteration) 을 통해 구하는 법, 그리고 상미분 방정식과 편미분 방정식의 해를 구하는 법을 다룬다.",
    "crumbs": [
      "수치해석 II"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_pde.html",
    "href": "src/numerical_analysis_using_julia/A_pde.html",
    "title": "편미분 방정식의 이론적 배경",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "편미분 방정식의 이론적 배경"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_pde.html#대표적인-편미분방정식",
    "href": "src/numerical_analysis_using_julia/A_pde.html#대표적인-편미분방정식",
    "title": "편미분 방정식의 이론적 배경",
    "section": "1 대표적인 편미분방정식",
    "text": "1 대표적인 편미분방정식\n\n\n\n\n\n\n\n\n이름\n방정식\n비고\n\n\n\n\n라플라스 방정식\n\\(\\nabla^2 \\psi = 0\\)\n전기자기학, 유체역학, 열전달, 중력\n\n\n포아송 방정식\n\\(\\nabla^2 \\psi = -\\rho/\\varepsilon_0\\)\nsource term 이 있는 전기/자기 현상\n\n\n헬름홀츠 방정식, 시간 독립적인 확산 방정식\n\\(\\nabla^2 \\psi \\pm k^2 \\psi  = 0\\)\n고체의 탄성파와 음파, 전자기파, 원자력 발전\n\n\n시간 의존적인 확산방정식\n\\(\\nabla^2 \\psi = \\dfrac{1}{a^2}\\dfrac{\\partial \\psi}{\\partial t}\\)\n\n\n\n시간 의존적인 고전 파동 방정식\n\\(\\dfrac{1}{c^2}\\dfrac{\\partial^2 \\psi}{\\partial t^2} = \\nabla^2 \\psi\\)\n\n\n\n클라인-고든 방정식\n\\(\\partial^2 \\psi = -\\mu^2 \\psi\\)\n상대론적 양자 역학\n\n\n시간 독립적인 슈레딩거 방정식\n\\(i\\hbar \\dfrac{\\partial \\psi}{\\partial t} =  -\\dfrac{\\hbar^2}{2m}\\nabla^2 \\psi + V\\psi\\)\n양자역학\n\n\n시간 의존적인 슈레딩거 방정식\n\\(-\\dfrac{\\hbar^2}{2m}\\nabla^2 \\psi + V\\psi = E\\psi\\)\n양자역학",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "편미분 방정식의 이론적 배경"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_pde.html#계-편미분-방정식",
    "href": "src/numerical_analysis_using_julia/A_pde.html#계-편미분-방정식",
    "title": "편미분 방정식의 이론적 배경",
    "section": "2 1계 편미분 방정식",
    "text": "2 1계 편미분 방정식\n\n2.1 특성곡선법\n상미분방정식과 같이 선형 편미분 연산자를 \\(\\mathcal{L}\\) 이라고 하자. 다음과 같이 주어진 편미분 방정식을 생각하자.\n\\[\n\\mathcal{L}\\varphi = a\\dfrac{\\partial \\varphi}{\\partial x} + b \\dfrac{\\partial \\varphi}{\\partial y} = 0.\n\\tag{1}\\]\n이 경우 \\(s=ax+by,\\, t= bx-ay\\) 로 선형 변환을 해 주면,\n\\[\n\\begin{aligned}\n0 &= a\\dfrac{\\partial \\varphi}{\\partial x} + b \\dfrac{\\partial \\varphi}{\\partial y} = a \\left(\\dfrac{\\partial \\varphi}{\\partial s}\\dfrac{\\partial s}{\\partial x}+\\dfrac{\\partial \\varphi}{\\partial t}\\dfrac{\\partial t}{\\partial x}\\right) + b\\left(\\dfrac{\\partial \\varphi}{\\partial s}\\dfrac{\\partial s}{\\partial y}+\\dfrac{\\partial \\varphi}{\\partial t}\\dfrac{\\partial t}{\\partial y}\\right) \\\\\n&= (a^2+b^2)\\dfrac{\\partial \\varphi}{\\partial s}\n\\end{aligned}\n\\tag{2}\\]\n이 되며, 따라서 \\(\\varphi\\) 는 \\(t\\) 의 함수이다. 즉, 임의의 \\(C^1\\) 함수 \\(f\\) 에 대해 \\(\\varphi(x,\\,y) = f(bx - ay)\\) 이며 다른 조건은 더이상 찾을 수 없다. 즉 식 1 에서와 같은 편미분 방정식이 \\(\\dfrac{d\\varphi}{ds}=0\\) 인 1계 상미분방정식으로 변한다. 이렇게 편미분 방정식을 하나 혹은 그 이상의 상미분 방정식으로 변환하는 것을 특성곡선법(method of characteristics) 이라고 하며 변화된 상미분 방정식 특성 곡선 (characteristic curve) 혹은 특성 곡면(characteristic curve) 라고 한다.\n이제 식 1 을 다시 보자. \\(\\mathbb{R}^2\\) 에서 생각하면 \\(\\boldsymbol{a} = (a,\\,b)\\) 에 대해 \\(\\boldsymbol{a} \\cdot \\nabla \\varphi (x,\\,y)=0\\) 인 함수 \\(\\varphi (x,\\,y)\\) 를 찾는 것이다. \\(\\nabla \\varphi (x,\\,y)\\) 는 \\(x,\\,y\\) 에서 \\(\\varphi\\) 가 최대로 증가하는 벡터를 의미하며 \\(\\boldsymbol{a} \\cdot \\nabla \\varphi = 0\\) 이므로 \\(\\boldsymbol{a}\\) 는 \\(\\varphi (x, y)=c\\) 인 방향을 의미한다. 즉 \\(t=bx-ay\\) 를 상수로 놓으면 \\(\\varphi (x,\\,y)\\) 도 상수이다. – to be extended–\n이제 좀 더 일반적인 경우를 살펴보자. 아래와 같이 선형 미분 연산자 \\(\\mathcal{L}\\) 에 대한 inhomogeneouse 미분방정식\n\\[\n\\mathcal{L}\\varphi = a\\dfrac{\\partial \\varphi}{\\partial x} + b \\dfrac{\\partial \\varphi}{\\partial y} +q(x,\\,y)\\varphi = F(x, y)\\,.\n\\tag{3}\\]\n를 앞에서와 같이 \\(s=ax+by,\\, t=bx-ay\\) 로 변수를 변환시키면,\n\\[\n(a^2+b^2) \\left(\\dfrac{\\partial \\varphi}{\\partial s}\\right)+ \\overline{q}(s, t) \\varphi = \\overline{F}(s, t)\n\\tag{4}\\]\n이다. 여기서 \\(\\overline{q},\\,\\overline{F}\\) 는 \\(q(x,\\,y),\\, F(x,\\,y)\\) 를 \\(x=\\dfrac{as}{a^2+b^2} + \\dfrac{bt}{a^2+b^2}\\), \\(y=\\dfrac{bs}{a^2+b^2}s - \\dfrac{at}{a^2+b^2}\\) 로 변환시킨 함수이다. 이 경우 식 4 는 \\(\\varphi\\) 의 \\(s\\) 에 대한 1계 ODE 이며 \\(t\\)-constant function 이 특성곡선이 된다.",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "편미분 방정식의 이론적 배경"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_pde.html#계-pde",
    "href": "src/numerical_analysis_using_julia/A_pde.html#계-pde",
    "title": "편미분 방정식의 이론적 배경",
    "section": "3 2계 PDE",
    "text": "3 2계 PDE\n이제 LHODE 와 같이 선형 제차 편미분방정식을 LHPDE 라고 쓰자. 그리고 함수 \\(u(x, y)\\) 에 대해 \\(u_{x} = \\dfrac{\\partial u}{\\partial x}\\), \\(u_{xy}= \\dfrac{\\partial^2 u}{\\partial x\\partial y}\\) 라고 쓰기로 하자. 이것은 타이핑을 많이 줄여준다.\n\n3.1 2계 PDE 의 분류\n\n2차원의 경우\n2차원에서의 2계 LHPDE 는 다음과 같은 꼴을 갖는다.\n\\[\nAu_{xx} + Bu_{xy} + Cu_{yy} + Du_x + Eu_y + Fu = 0\n\\]\n이 때 \\(B^2-4AC &lt; 0\\) 일 때 이 PDE 를 타원형(elliptical) 이라고 하며, \\(B^2-4AC&gt;0\\) 일 때 쌍곡선형(hyperbollic) 이라고 한다. \\(B^2-4AC=0\\) 일 때는 포물선형(parabolic) 이라고 한다\\(^\\ast\\). \\(^\\ast\\) 그 이유는 \\(Ax^2 +B xy + Cy^2 + Dx + Eu + F = 0\\) 이 \\(B^2-4AC &lt;0\\) 일 때 타원, \\(B^2-4AC&gt;0\\) 일 때 쌍곡선, \\(B^2-4AC=0\\) 일 때 포물선인 데서 유래한다.\n예를 들어 실수 \\(a,\\,c\\) 에 대해\n\\[\na^2 u_{xx} - c^2 u_{yy}=0\n\\]\n의 경우는 쌍곡선형이다. 이 미분방정식은 \\((a\\partial_x+c\\partial_y) (a\\partial_x-c\\partial_y)u=0\\) 를 만족하며 \\((a\\partial_x+c\\partial_y) u=0\\) 이거나 \\((a\\partial_x-c\\partial_y) u=0\\) 의 해 모두 해가 된다. 즉 2차원 LHPDE 가 1차원 LHPDE 로 변형되었다. 또한 \\(s=ax+by,\\, t= bx-ay\\) 가 특성곡선이 된다.\n비슷한 경우로\n\\[\na^2 u_{xx} + c^2 u_{yy}=0\n\\]\n의 경우는 타원형이다. 이 미분방정식은 \\((a\\partial_x+ic\\partial_y) (a\\partial_x-ic\\partial_y)u=0\\) 를 만족한다.\n\\[\na^2 u_{xx} - 2acu_{xy} + c^2u_{yy}=0\n\\]\n의 경우 \\((a\\partial_x - c\\partial_y)^2y=0\\) 이다. 혹은\n\\[\nu_{xx} +au_t=0\n\\]\n인 경우도 포물선형이다.\n\n\n\n다차원의 경우\n주로 2차원 공간 + 시간, 3차원 공간, 3차원 공간+시간 의 경우이며 이보다 많은 차원의 경우도 생각 할 수 있다.\n\n\n\n\n3.2 경계조건\n일반적으로 경계조건은 세가지 가운데 한가지이다.\n\n디리클레 경계 조건 : 경계에서의 함수값이 정해져 있는 경우.\n노이만 경계 조건 : 경계에서의 normal derivative 가 정해져 있는 경우.\n코시 경계 조건 : 디리클레와 노이만 경계조건이 모두 다 정해져 있는 경우. 보통은 미분방정식의 해를 너무 제한하기 때문에 사용하지 않는다.",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "편미분 방정식의 이론적 배경"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_pde.html#변수분리법",
    "href": "src/numerical_analysis_using_julia/A_pde.html#변수분리법",
    "title": "편미분 방정식의 이론적 배경",
    "section": "4 변수분리법",
    "text": "4 변수분리법\n\\(n\\) 개의 변수에 대한 PDE 를 \\(n\\) 개의 ODE 로 변환시키는 방법이다.\n\n4.1 데카르트 좌표계에서\n다음과 같이 주어지는 3차원에서의 헬름홀츠 방정식을 생각하자. \\[\n\\nabla^2 \\psi + k^2 \\psi = 0.\n\\tag{5}\\]\n3차원 \\((x,\\,y,\\,z)\\) 에서 정의되는 함수 \\(\\psi\\) 를 각각 \\(x,\\,y,\\,z\\) 에 대한 함수의 곱으로 표현하자. 즉,\n\\[\n\\psi(x,\\,y,\\,z) = X(x)Y(y)Z(z)\n\\]\n라고 하자. 식 5 를 보면 모든 미분항이 하나의 변수에 대한 미분항이다. 즉 \\(\\partial_x\\partial_y\\) 같은 것이 없으며, 이 경우 각 변수에 대한 함수의 곱으로 표현했을 때 유용한 경우가 많다. 이렇게 다차원 함수를 각 변수에 대한 함수의 곱으로 표현하여 방정식을 푸는 것을 변수분리법(separation of variables) 이라고 한다. 이 경우 식 5 는 다음과 같다.\n\\[\n\\dfrac{d^2 X(x)}{dx^2} Y(y)Z(z) + X(x)\\dfrac{d^2Y(y)}{d^2y}Z(z) + X(x)Y(y)\\dfrac{d^2 Z(z)}{dz^2} + k^2 X(x)Y(y)Z(z)=0.\n\\]\n그렇다면,\n\\[\n\\dfrac{1}{X}\\dfrac{d^2 X}{dx^2}= -k^2 - \\dfrac{1}{Y}\\dfrac{d^2Y}{dy^2} - \\dfrac{1}{Z} \\dfrac{d^2Z}{dz^2}\n\\]\n이다. 그런데 좌변은 \\(x\\) 에 대한 함수이고 우변은 \\(y,\\,z\\) 에 대한 함수여야 하므로 이 식은 상수일 수 밖에 없다. 상수를 \\(-l^2\\) 라 놓으면,\n\\[\n\\begin{aligned}\n\\dfrac{1}{X}\\dfrac{d^2 X}{dx^2} &= -l^2 , \\\\\n-k^2 - \\dfrac{1}{Y}\\dfrac{d^2Y}{dy^2} - \\dfrac{1}{Z} \\dfrac{d^2Z}{dz^2} &= -l^2.\n\\end{aligned}\n\\]\n이며, 이 중 두번째 식은,\n\\[\n\\dfrac{1}{Y}\\dfrac{d^2Y}{dy^2}= -k^2+l^2 - \\dfrac{1}{Z}\\dfrac{d^2Z}{dz^2}\n\\]\n가 된다. 역시 좌변은 \\(y\\) 에 대한 함수이고 우변은 \\(z\\) 에 대한 함수이므로,\n\\[\n\\begin{aligned}\n\\dfrac{1}{Y}\\dfrac{d^2Y}{dy^2} &= -m^2 , \\\\\n-k^2+l^2 - \\dfrac{1}{Z}\\dfrac{d^2Z}{dz^2} &= -m^2.\n\\end{aligned}\n\\]\n로 놓을 수 있다. 이제\n\\[\n\\dfrac{1}{Z} \\dfrac{d^2Z}{dz^2}=-n^2\n\\]\n로 놓으면,\n\\[\n\\dfrac{1}{X}\\dfrac{d^2X}{dx^2} = -l^2,\\qquad \\dfrac{1}{Y}\\dfrac{d^2Y}{dy^2} = -m^2, \\qquad\\dfrac{1}{Z} \\dfrac{d^2Z}{dz^2}=-n^2,\\qquad l^2+m^2+n^2=k^2\n\\]\n를 얻는다. 각각의 \\(l,\\,m,\\,n\\) 에 대한 3개의 상미분방정식의 해를 \\(X_l(x),\\, Y_m(y),\\, Z_n(z)\\) 라고 하자. 당연히 \\(l^2+m^2+n^2=k^2\\) 를 만족해야 한다. 그리고 \\(\\psi_{lmn}(x,\\,y,\\,z) = X_l(x)Y_m(y)Z_n(z)\\) 라고 하자. 이제 식 5 의 일반해를 다음과 같이 기술할 수 있다.\n\\[\n\\psi = \\sum_{l, m} a_{lm} \\psi_{lmn}.\n\\]\n인덱스에서 \\(n\\) 을 뺀 이유는 \\(n^2 = k^2-l^2-m^2\\) 로 정해지기 때문이다.\n\n\n\n4.2 원통좌표계에서\n\\((\\rho,\\, \\phi,\\, z)\\) 를 변수로 하는 실린더 좌표계에서의 핼름홀즈 방정식(식 5) 은\n\\[\n\\dfrac{1}{\\rho}\\dfrac{\\partial }{\\partial \\rho} \\left(\\rho\\dfrac{\\partial \\psi}{\\partial \\rho}\\right) + \\dfrac{1}{\\rho^2}\\dfrac{\\partial^2 \\psi}{\\partial \\phi^2} + \\dfrac{\\partial^2 \\psi}{\\partial z^2} + k^2 \\psi = 0\n\\tag{6}\\]\n이 된다. 이 경우 \\(\\partial_\\rho \\partial_\\phi\\) 같은 변수가 섞인 편미분항이 없기 때문에 변수분리법을 시도해 볼 수 있다.\n\\[\n\\psi (\\rho,\\, \\phi,\\, z) = P(\\rho)\\Phi (\\phi) Z(z)\n\\]\n라고 하자. 이 경우 식 6 은\n\\[\n\\dfrac{\\Phi Z}{\\rho} \\dfrac{d}{d\\rho} \\left(\\rho \\dfrac{dP}{d\\rho}\\right) + \\dfrac{PZ}{\\rho^2} \\dfrac{d^2\\Phi}{d\\phi^2} + P\\Phi \\dfrac{d^2Z}{dz^2} + k^2 P \\Phi Z = 0\n\\]\n이 된다. 이 식은 \\[\n\\dfrac{1}{Z(z)} \\dfrac{d^2Z(z)}{dz^2} = - \\dfrac{1}{\\rho P(\\rho)} \\dfrac{d}{d\\rho} \\left(\\rho \\dfrac{dP(\\rho)}{d\\rho}\\right) - \\dfrac{1}{\\rho^2 \\Phi(\\phi)} \\dfrac{d^2\\Phi(\\phi)}{d\\phi^2} -k^2\n\\]\n이며, 좌변은 \\(z\\) 의 함수, 우변은 \\(\\rho,\\, \\phi\\) 의 함수이므로 상수 일 수 밖에 없다. 이 값을 \\(l^2\\) 라고 하면,\n\\[\n\\dfrac{d^2Z}{dz^2}= l^2 Z(z)\n\\tag{7}\\]\n이다. 또한\n\\[\n\\dfrac{1}{\\rho P(\\rho)} \\dfrac{d}{d\\rho} \\left(\\rho \\dfrac{dP(\\rho)}{d\\rho}\\right) + \\dfrac{1}{\\rho^2 \\Phi(\\phi)} \\dfrac{d^2\\Phi(\\phi)}{d\\phi^2} +k^2 = -l^2\n\\]\n로부터 \\(n^2 = k^2+l^2\\) 로 놓고 \\(\\rho^2\\) 를 곱하면,\n\\[\n\\dfrac{\\rho}{P(\\rho)}\\dfrac{d}{d\\rho} \\left(\\rho \\dfrac{dP(\\rho)}{d\\rho}\\right) + n^2\\rho^2 = - \\dfrac{1}{\\Phi (\\phi)}\\dfrac{d^2 \\Phi (\\phi)}{d\\phi^2}\n\\]\n이 되며, 역시 \\(m^2\\) 로 놓으면,\n\\[\n\\dfrac{d^2\\Phi}{d\\phi^2} = -m^2 \\Phi (\\phi)\n\\tag{8}\\]\n와\n\\[\n\\rho\\dfrac{d}{d\\rho} \\left(\\rho \\dfrac{dP(\\rho)}{d\\rho}\\right) + (n^2\\rho^2 - m^2) P(\\rho) = 0\n\\tag{9}\\]\n을 얻는다. 이 때 각에 대한 식 8 의 해인 \\(\\Phi\\) 는 \\(\\sin m\\phi\\) 와 \\(\\cos m\\phi\\) 의 선형결합이며 식 9 를 만족하는 함수 \\(P(\\rho)\\) 는 베셀 함수",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "편미분 방정식의 이론적 배경"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_generalized_inverse.html",
    "href": "src/numerical_analysis_using_julia/A_generalized_inverse.html",
    "title": "일반화된 역행렬과 의사 역행렬",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "일반화된 역행렬과 의사 역행렬"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_generalized_inverse.html#일반화된-역행렬",
    "href": "src/numerical_analysis_using_julia/A_generalized_inverse.html#일반화된-역행렬",
    "title": "일반화된 역행렬과 의사 역행렬",
    "section": "1 일반화된 역행렬",
    "text": "1 일반화된 역행렬\n\n\n\n\n\n\n\n정의 1 (일반화된 역행렬) 행렬 \\(\\boldsymbol{A} \\in \\mathbb{F}^{m \\times n}\\) 에 대해 \\(\\boldsymbol{AGA}=\\boldsymbol{A}\\) 를 만족하는 행렬 \\(\\boldsymbol{G} \\in \\mathbb{F}^{n\\times m}\\) 를 \\(\\boldsymbol{A}\\) 에 대한 일반화된 역행렬(generalized inverse matrix) 라고 한다. \\(\\boldsymbol{A}\\) 의 일반화된 역행렬을 \\(\\boldsymbol{A}^{-}\\) 로 표기한다.\n\n\n\n\n\n\n명제 1 \\(\\boldsymbol{A}\\) 가 가역행렬이면 \\(\\boldsymbol{G}=\\boldsymbol{A}^{-1}\\) 이다.\n\n\n(증명). \\(\\boldsymbol{A}\\) 가 가역행렬이면 \\(m=n\\) 이다. \\[\n\\boldsymbol{AGA} = \\boldsymbol{A} \\implies \\boldsymbol{AGAA}^{-1} = \\boldsymbol{AA}^{-1} \\implies \\boldsymbol{AG}= \\boldsymbol{I}_n \\qquad \\square\n\\]\n\n\n\n명제 2 \\(\\boldsymbol{A}\\in \\mathbb{F}^{m \\times n}\\) 와 \\(\\boldsymbol{G}\\in \\boldsymbol{G} \\in \\mathbb{F}^{n\\times m}\\) 에 대해 다음은 동치이다.\n  (\\(1\\)) \\(\\boldsymbol{G}\\) 는 \\(\\boldsymbol{A}\\) 의 일반화된 역행렬이다.\n  (\\(2\\)) \\(\\boldsymbol{AG}=\\boldsymbol{I}_m\\) 이고 \\(\\operatorname{rank}(\\boldsymbol{AG})= \\operatorname{rank}(\\boldsymbol{A})\\) 이거나 \\(\\boldsymbol{GA}= \\boldsymbol{I}_n\\) 이고 \\(\\operatorname{rank}(\\boldsymbol{GA})= \\operatorname{rank}(\\boldsymbol{G})\\) 이다.\n  (\\(3\\)) \\(\\boldsymbol{Ay}=\\boldsymbol{x}\\) 의 해는 \\(\\boldsymbol{x}=\\boldsymbol{Gy}\\) 이다.\n\n\n(증명). (\\(1\\implies 2\\))",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "일반화된 역행렬과 의사 역행렬"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/15_pde_1.html",
    "href": "src/numerical_analysis_using_julia/15_pde_1.html",
    "title": "편미분 방정식",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]\n편미분 방정식의 이론적 배경 를 참고하라. 여기서는 주로 1계와 2계 편미분 방정식에 대해 다룬다. 여기서 다루게 되는 2계 편미분 방정식은 아래와 같이 분류 될 수 있다.",
    "crumbs": [
      "수치해석 II",
      "편미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/15_pde_1.html#sec-pde_1d_parabolic",
    "href": "src/numerical_analysis_using_julia/15_pde_1.html#sec-pde_1d_parabolic",
    "title": "편미분 방정식",
    "section": "1 포물선형 편미분 방정식",
    "text": "1 포물선형 편미분 방정식\n함수 \\(u(t,\\,x)\\) 가 \\(t&gt;0,\\, a&lt;x&lt;b\\) 에서 다음의 타원형 편미분 방정식을 만족한다고 하자.\n\\[\n\\dfrac{\\partial u}{\\partial t} = D \\dfrac{\\partial^2 u}{\\partial x^2},\\qquad D &gt; 0\\text{ : constant.}\n\\tag{1}\\]\n이 미분방정식은 열전달(heat transfer) 혹은 확산(diffusion) 현상에 많이 사용된다. 초기 조건\n\\[\nu(0, x) = f(x)\n\\]\n이 주어졌으며, 경계조건은\n\\[\nu(0, a) = \\alpha,\\, u(0,b) = \\beta\n\\]\n라고 하자. 즉 이 문제는 초기조건이 경계조건을 만족하는 상태에서 주어진 가운데, \\(u(t,\\, x)\\) 가 경계조건을 만족시키면서 어떻게 변화해 가는지를 보는 미분방정식이다.\n\\([a,\\,b]\\) 를 \\(N+1\\) 개의 구간으로 나누어 \\(x_0=a,\\, x_{N+1}=b\\) 가 되도록 한다. \\(t\\) 는 \\(\\Delta t\\) 에 대한 배수로 생각한다.\n\\[\nt_j = j(\\Delta t),\\qquad x_k = a+ \\dfrac{k(b-a)}{N+1} = a+hk,\\qquad h = \\dfrac{b-a}{N+1}.\n\\]\n테일러 전개로 부터 다음을 만족하는 \\(\\tau_j \\in (t_{j},\\, t_{j+1})\\) 이 존재한다는 것을 안다.\n\\[\n\\dfrac{\\partial u}{\\partial t}(t_j, x_k) = \\dfrac{u(t_{j+1},\\, x_k) - u(t_j,\\, x_k)}{\\Delta t} - \\dfrac{\\Delta t}{2}\\dfrac{\\partial^2 u}{\\partial t^2}(\\tau_j,\\, x_k)\n\\]\n또한 역시 테일러 전개로부터 다음을 만족하는 \\(\\xi_k \\in (x_{k-1},\\, x_{k+1})\\) 이 존재한다는 것을 안다.\n\\[\n\\dfrac{\\partial^2 u}{\\partial x^2} (t_j,x_k) = \\dfrac{u(t_j, x_{k+1}) - 2u(t_j, x_k) + u(t_j, x_{k-1})}{h^2} - \\dfrac{h^2}{12}\\dfrac{\\partial^2 u}{\\partial x^2}(t_j, \\xi_k)\n\\]\n이제 \\(u_{jk} = u(t_j,\\,x_k)\\) 하면, 식 1 는 다음과 같이 변한다.\n\\[\n\\dfrac{u_{j+1,\\,k} - u_{j, k}}{\\Delta t} - D\\dfrac{u_{j,\\,k+1} -2u_{j, k} + u_{j,k-1}}{h^2} = 0 + O(\\Delta t,\\, h^2).\n\\tag{2}\\]\n여기서 \\(O(\\Delta t,\\, h^2)\\) 은 truncaiton error 이다. 이제 이 truncation error 를 제외하고 변형하면\n\\[\nu_{j+1, k} = \\left(1-2D\\dfrac{\\Delta t}{h^2}\\right)u_{j, k} + D\\dfrac{\\Delta t}{h^2} (u_{j,k+1} + u_{j, k-1})\n\\tag{3}\\]\n을 얻는다. \\(\\lambda = D(\\Delta t)/h^2\\) 로 놓으면,\n\\[\nu_{j+1, k} = \\left(1-2\\lambda \\right)u_{j, k} + \\lambda (u_{j,k+1} + u_{j, k-1})\n\\]\n이다. \\(u_{0, k} = f(x_k)\\) 이므로,\n\\[\n\\begin{aligned}\nu_{10} &= u_{00} = \\alpha, \\\\\nu_{1, k} &= (1-2\\lambda) u_{0, k} + \\lambda (u_{0, 2} + u_{0, 0}) \\qquad k=2,\\ldots,\\, N, \\\\\nu_{N+1,0} &= u_{N+1, 0} = \\beta\n\\end{aligned}\n\\tag{4}\\]\n이다. \\(\\boldsymbol{u}^{(k)} = \\begin{bmatrix} u_{k0} & \\cdots & u_{k,N+1}\\end{bmatrix}^T\\) 라 놓고, 행렬 \\(\\boldsymbol{A}\\) 를 다음과 같이 정의한다.\n\\[\n\\boldsymbol{A} = \\begin{bmatrix} 1 & 0 & 0 & 0 &\\cdots & \\cdots & 0 \\\\\n\\lambda & (1-2\\lambda) & \\lambda & 0 & \\cdots &  \\cdots & 0 \\\\\n0 & \\lambda & (1-2\\lambda) & \\lambda & 0  &\\cdots & 0 \\\\\n& & \\vdots & & & \\vdots \\\\\n0&\\cdots &\\cdots &\\cdots &\\lambda & (1-2\\lambda) & \\lambda \\\\\n0&\\cdots &\\cdots &\\cdots & \\cdots & 0 & 1 \\\\\n\\end{bmatrix}\n\\]\n그렇다면 {#eq-pde_parabolic_diffusion_2}\n\\[\n\\boldsymbol{u}^{(k+1)} = \\boldsymbol{Au}^{(k)}\n\\]\n이 성립한다. \\(u_{1,k},\\ldots,\\, u_{j,k}\\) \\((k=0,\\ldots,\\, N+1)\\) 을 알 때 식 3 를 이용하여 \\(u_{j+1,k}\\;(k=0,\\ldots,\\,N+1)\\) 을 알 수 있다.",
    "crumbs": [
      "수치해석 II",
      "편미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/15_pde_1.html#타원형-편미분-방정식",
    "href": "src/numerical_analysis_using_julia/15_pde_1.html#타원형-편미분-방정식",
    "title": "편미분 방정식",
    "section": "2 타원형 편미분 방정식",
    "text": "2 타원형 편미분 방정식\n우선 아래와 같은 \\(R=[a,\\,b]\\times [c,\\,d]\\) 에서 정의된 2차원 타원형 편미분 방정식인 푸아송 방정식을 생각하자\n\\[\n\\nabla^2 u(x,\\,y) = u_{xx} + u_{yy} = f(x,\\,y)\n\\tag{5}\\]\n여기에 디리클레 경계조건 \\(g(x,\\,y)\\) 가 부여되어 \\(R\\) 의 경계 \\(S\\) 에서 \\(u(x,\\,y) = g(x,\\,y)\\) 라고 하자. 아래 그림과 같이 \\(N \\times M\\) 그리드를 생각한다.\n\n\n\n\n\n\n그림 1: 그리드\n\n\n\n\\(x_0=a,\\, x_N=b,\\, y_0=c,\\, y_M = d\\) 이며 \\(h_x = \\dfrac{b-a}{N},\\, h_y = \\dfrac{d-c}{M}\\) 에 대해 \\(x_j = a+jh_x\\), \\(y_k = c+kh_y\\) 이다. 테일러 시리즈로부터,\n\\[\n\\begin{aligned}\n\\dfrac{\\partial^2 u(x_j,y_k)}{\\partial x^2} &=  \\dfrac{u_(x_{j+1}, y_k) - 2u(x_j, y_k) + u(x_{j-1}, y_k)}{h_x^2} - \\dfrac{h_x^2}{12}\\dfrac{\\partial^4 u(\\xi_j,\\, y_k)}{\\partial^4 x}, \\\\\n\\dfrac{\\partial^2 u(x_j,y_k)}{\\partial y^2} &=  \\dfrac{u_(x_{j}, y_{k+1}) - 2u(x_j, y_{k}) + u(x_{j}, y_{k-1})}{h_y^2} - \\dfrac{h_y^2}{12}\\dfrac{\\partial^4 u(x_j,\\, \\eta_k)}{\\partial^4 y},\n\\end{aligned}\n\\tag{6}\\]\n를 만족하는 \\(\\xi_j \\in (x_{j-1}, x_{j+1})\\) 과 \\(\\eta_k \\in (y_{k-1},y_{k+1})\\) 이 존재함을 안다. 이를 이용하면 푸아송 방정식은\n\\[\n\\begin{aligned}\n&\\dfrac{u(x_{j+1}, y_k) - 2u(x_j, y_k) + u(x_{j-1}, y_k)}{h_x^2} + \\dfrac{u_(x_{j}, y_{k+1}) - 2u(x_j, y_{k}) + u(x_{j}, y_{k-1})}{h_y^2} \\\\\n&= f(x_j,\\,y_k) + \\dfrac{h_x^2}{12}\\dfrac{\\partial^4 u(\\xi_j,\\, y_k)}{\\partial^4 x}+\\dfrac{h_y^2}{12}\\dfrac{\\partial^4 u(x_j,\\, \\eta_k)}{\\partial^4 y}\n\\end{aligned}\n\\tag{7}\\]\n이다. 이 식은 \\(j=1,\\ldots,\\,N-1\\),, \\(k=1,\\ldots,\\, M-1\\) 에 대해 성립한다. 또한 경계조건은\n\\[\n\\begin{aligned}\nu(x_0,\\, y_k) &= g(x_0,\\, y_i),\\qquad &u(x_N,\\, y_k) &= g(x_N,\\, y_k),\\qquad && k=1,\\ldots,\\, M, \\\\\nu(x_j,\\, y_0) &= g(x_j,\\, y_0),\\qquad &u(x_j,\\, y_M) &= g(x_j,\\, y_M),\\qquad &&j = 1,\\ldots,\\, N\n\\end{aligned}\n\\tag{8}\\] 이 된다.\n\n\n2.1 유한차분법\n이제 유한차분법을 생각하자 \\(u(x_j,\\, y_k)\\) 를 \\(u_{jk}\\) 로 쓰기로 한다. 그렇다면, 식 7 는 오차텀을 제외하면\n\\[\n\\dfrac{u_{j+1,k} - 2u_{jk} +u_{j-1, k}}{h_x^2} + \\dfrac{u_{j,k+1} -2u_{jk} + u_{j,k-1}}{h_y^2} = f(x_j, y_k)  \n\\]\n이며 이를 정리하면,\n\\[\n2\\left[\\left(\\dfrac{h_x}{h_y}\\right)^2+1\\right] u_{jk} - (u_{j+1,k} +u_{j-1,k}) - \\left(\\dfrac{h_x}{h_y}\\right)^2 (u_{j,k+1} + u_{j, k-1}) = -h_x^2 f(x_j,\\, y_k)\n\\]\n가 된다. 또한 경계조건 식 8 는\n\\[\n\\begin{aligned}\nu_{0k} &= g(x_0,\\, y_i),\\qquad & u_{N, k} &= g(x_N,\\, y_k),\\qquad && k=1,\\ldots,\\, M, \\\\\nu_{j0} &= g(x_j,\\, y_0),\\qquad & u_{j,M} &= g(x_j,\\, y_M),\\qquad &&j = 1,\\ldots,\\, N\n\\end{aligned}\n\\tag{9}\\]\n이다.",
    "crumbs": [
      "수치해석 II",
      "편미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/13_ode_boundary_value_problem.html",
    "href": "src/numerical_analysis_using_julia/13_ode_boundary_value_problem.html",
    "title": "상미분 방정식의 경계값 문제",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 경계값 문제"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/13_ode_boundary_value_problem.html#경계값-문제",
    "href": "src/numerical_analysis_using_julia/13_ode_boundary_value_problem.html#경계값-문제",
    "title": "상미분 방정식의 경계값 문제",
    "section": "1 경계값 문제",
    "text": "1 경계값 문제\n상미분 방정식의 해에 대해 우리는 \\([a,\\,b]\\) 구간에만 관심이 있으며 \\(\\boldsymbol{x}(a)=\\boldsymbol{x}_a,\\, \\boldsymbol{x}(b)= \\boldsymbol{x}_b\\) 형식의 경계조건이 주어졌을 경우 이를 경계값 문제라고 한다.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 경계값 문제"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/13_ode_boundary_value_problem.html#shooting-method-사격법",
    "href": "src/numerical_analysis_using_julia/13_ode_boundary_value_problem.html#shooting-method-사격법",
    "title": "상미분 방정식의 경계값 문제",
    "section": "2 Shooting method (사격법)",
    "text": "2 Shooting method (사격법)\n사격법은 경계값 문제를 초기값 문제로 바꾸어 푸는 방법이다. 사격법이라고 불리우는 이유는 이 방법이 원래 대포 탄환의 궤적을 계산하기 위해 개발되었기 때문이다. 일반적으로 이계 미분방정식의 경계값 문제를 푸는 데 사용된다.\n\n\n선형 사격법\nLinear shooting 방법은 경계값 문제를 두개의 초기값 문제로 바꾸어 푼다.\n다음은 잘 알려져 있다.\n\n\n정리 1 \\([a,\\,b]\\) 구간에서 정의된 미분방정식과 그 경계값이 다음과 같이 주어졌다고 하자. \\[\nx'' = f(t, x, x'),\\qquad x(a)=\\alpha,\\, x(b) = \\beta\n\\]\n이 때 \\(D\\{(t,\\,x,\\,x') : a\\le t \\le b,\\, x,\\, x'\\in \\mathbb{R}\\}\\) 에 대해 다음의 조건이 주어졌다고 하자.\n  (\\(1\\)) \\(x''\\) 가 \\(D\\) 에서 연속이다.\n  (\\(2\\)) \\(\\partial_x f\\) 와 \\(\\partial_{x'}f\\) 도 \\(D\\) 에서 연속이다\n  (\\(3\\)) \\(\\forall (t,\\,x,\\,x')\\in D,\\, \\partial_x f(t,\\,x,\\,x') &gt;0\\) 이다.\n  (\\(4\\)) \\(\\partial_{x'}f\\) 가 유계이다.\n이 때 미분방정식은 유일한 해를 가진다.\n\n\n\n우리가 \\(x'' = p(t)x' +  q(t)x + r(t)\\) 형태의 미분방정식과 \\(x(a)=\\alpha,\\, x(b)=\\beta\\) 의 경계조건을 가진 미분방정식에 관심이 있다고 하자. 이 때 정리 1 으로부터 다음을 알 수 있다.\n\n\n따름정리 1 정리 1 의 조건을 만족하는 경계값 문제가 다음과 같이 주어졌다고 하자.\n\\[\nx'' = f(t,\\,x,\\,x') = p(t)x' +  q(t)x + r(t),\\qquad x(a)=\\alpha,\\, x(b)=\\beta.\n\\]\n또한 다음 조건을 만족한다고 하자.\n  (\\(1\\)) \\(p(t),\\,q(t),\\,r(t)\\) 가 \\([a,\\,b]\\) 에서 연속이다.\n  (\\(2\\)) \\([a,\\,b]\\) 구간에서 \\(q(t)&gt;0\\) 이다.\n이 때 이 미분방정식의 해는 유일하게 존재한다.\n\n\n\n이제 \\(x_1(t)\\) 와 \\(x_2(t)\\) 가 각각 따름정리 1 의 조건 (\\(1\\)), (\\(2\\)) 를 만족하는 아래의 초기값 문제에 대한 해라고 하자.\n\\[\n\\begin{aligned}\nx_1'' & = p(t)x_1' + q(t)x_1 + r(t) ,\\qquad &x_1(a) = \\alpha,\\, x_1'(a) = 0,\\\\\nx_2'' & = p(t)x_2' + q(t)x_2  ,\\qquad &x_2(a) = 0,\\, x_2'(a) = 1,\\\\\n\\end{aligned}\n\\tag{1}\\]\n11장 정리 1 로부터 우리는 위의 두 방정식이 유일해를 가진다는 것을 안다. 여기서 \\(x_2(b)\\ne 0\\) 이어야 하는데, 만약 \\(x_2(b) =0\\) 이라면 \\(x_2=0\\) 밖에 해가 될수 없기 때문이다(연습문제 1 를 보라). 이제\n\\[\nx(t) := x_1(t) + \\dfrac{\\beta - x_1(b)}{x_2(b)} x_2(t)\n\\]\n로 놓으면,\n\\[\n\\begin{aligned}\nx''(t) &= x_1''(t) + \\dfrac{\\beta-x_1(b)}{x_2(b)}x_2''(t) \\\\\n& = p(t)\\left[x_1'(t) +  \\dfrac{\\beta-x_1(b)}{x_2(b)} x_2''(t)\\right]+ q(t)\\left[x_1(t) + \\dfrac{\\beta-x_1(b)}{x_2(b)} x'(t)\\right] +r(t) \\\\\n&= p(t)x'(t) + q(t)x(t) + r(t)\n\\end{aligned}\n\\]\n이며 \\(x(a) = \\alpha,\\, x(b) = \\beta\\) 가 된다. 즉 우리는 경계값 문제를 두개의 초기값 문제로 바꾼 것이다.\n이제 이것을 구현해보자. 초기값 문제는 RK4 를 이용하여 풀기로 한다. \\(\\boldsymbol{x}_1(t)\\) 를\n\\[\n\\boldsymbol{x}_1(t) = \\begin{bmatrix} x_1(t) \\\\ x_1'(t) \\\\ 1\\end{bmatrix}\n\\]\n라고 하면 미분방정식은 다음 식을 만족한다.\n\\[\n\\dfrac{d\\boldsymbol{x}_1(t)}{dt} = \\begin{bmatrix} 0 & 1 & 0 \\\\ q(t) & p (t) & r(t) \\\\ 0 & 0 & 0\\end{bmatrix}\\boldsymbol{x}_1(t)\n\\]\n\\(x_2(t)\\) 에 대해서는\n\\[\n\\boldsymbol{x}_2(t) = \\begin{bmatrix} x_2(t) \\\\ x_2'(t) \\end{bmatrix}\n\\]\n에 대해\n\\[\n\\dfrac{d\\boldsymbol{x}_2(t)}{dt} = \\begin{bmatrix} 0 & 1 \\\\ q(t) & p(t) \\end{bmatrix} \\boldsymbol{x}_2(t)\n\\]\n의 미분방정식을 만족한다. 이제 선형 사격법은 아래와 같이 구현 될 수 있다.\nusing NAJ\nfunction linear_shooting(\n    p::Function, \n    q::Function, \n    r::Function, \n    boundary,             # t_i, t_f\n    condition,            # x(t_i), x(t_f)\n    Npoints = 100)        \n    \n    @assert length(boundary) == length(condition) == 2\n    \n    de1(t, x) = [0 1 0 ; q(t) p(t) r(t);  0 0 0] * x\n    de2(t, x) = [0 1; q(t) p(t)] * x\n\n    t = range(boundary[1], boundary[2], length = Npoints)\n\n    α, β = condition[1:2]\n\n    tn, x1 = ode_rk4(de1, boundary[1], [α, 0, 1], Npoints, step(t))\n    tn, x2 = ode_rk4(de2, boundary[1], [0, 1], Npoints, step(t))\n    \n    r = @. x1[1, :] + (β-x1[1, end])/(x2[1, end]) * x2[1, :]\n\n    return tn, r\nend\n\n\n\n연습문제\n\n연습문제 1 따름정리 1 의 조건을 만족는 초기값 문제가 식 1 로 주어졌을 때 \\(x_2(b) \\ne 0\\) \u001f을 보여라.\n\n\n(해답). \n\n\n\n연습문제 2 중심이 일치하는 반지름이 각각 \\(R_1,\\,R_2 (R_1&lt;R_2)\\) 인 spherical shell 의 정전기 포텐셜 \\(\\varphi(r)\\) 은 \\(R_1 \\le r\\le R_2\\) 영역에서 아래의 미분방정식을 만족한다.\n\\[\n\\dfrac{d^2\\varphi(r)}{d^2 r} + \\dfrac{2}{r} \\dfrac{d\\varphi}{dr} = 0.\n\\]\n\\(\\varphi(R_1)= V_1\\), \\(\\varphi(R_2)=0\\) 일 때 선형사격법으로 \\(\\varphi(r)\\) 을 구하고 이것이 아래와 일치함을 보여라.\n\\[\n\\varphi(r) = \\dfrac{V_1R_1}{r} \\left(\\dfrac{R_2-r}{R_2-R_1}\\right)\n\\]\n\n\n(해답). \\(R_1 = 2,\\, R_2=10,\\, V_1=10\\) 라고 하자. 를\nusing NAJ\nusing LaTeXStrings\n\nt, φ = linear_shooting(r-&gt;-2/r, r-&gt;0, r-&gt; 0, [2, 10], [10, 0])\nfig, ax= plot(t, φ)\nt1 = 2.0:0.1:10\nu = @. 10*2/t1 *(10-t1)/(8)\nlines!(ax, t1, u, color = :red)\nax.xlabel = L\"r\"\nax.ylabel = L\"\\varphi\"\nfig\n\n\n\n\n\n\n그림 1: 결과\n\n\n\n\n\n\n\n비선형 사격법\n일반적인 \\(x'' = f(t,\\,x,\\,x')\\) 형태의 미분방정식은 선형 사격법에서 와 같이 경계값 문제를 초기값 문제로 환원시킬 수 없다. \\([a,\\,b]\\) 구간에 관심이 있으며 \\(\\alpha = x(a)\\) 와 \\(\\beta = x(b)\\) 가 주어졌다고 하자. 이 경우 새로운 변수 \\(s=x'(a)\\) 를 도입하며, \\(s\\) 값을 조절해 가며 해를 찾는다. 처음의 \\(s\\) 값은 임의로 정한다.\n\\[\nx'' = f'(t,\\,x,\\,x'),\\qquad x(a)=\\alpha,\\, x'(a)=s\n\\]\n의 해를 \\(x(t,\\,s)\\) 라고 하자. \\(x(b,\\,s)-\\beta\\) 의 차이가 클 경우 \\(y(s) = x(b,\\,s)-\\beta = 0\\) 의 해를 구하여\n\\[\nx'' = f'(t,\\,x,\\,x'),\\qquad x(a)=\\alpha,\\, x'(a)=s_{k+1}\n\\]\n의 해 \\(x_{k+1}\\) 을 풀 수 있다. 이것을 원하는 만큼 반복하는 것이 비선형 사격법의 전략이다.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 경계값 문제"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/13_ode_boundary_value_problem.html#유한-차분법",
    "href": "src/numerical_analysis_using_julia/13_ode_boundary_value_problem.html#유한-차분법",
    "title": "상미분 방정식의 경계값 문제",
    "section": "3 유한 차분법",
    "text": "3 유한 차분법\n\n선형 미분 방정식에 대한 유한 차분법\n미분방정식이 \\([a,\\,b]\\) 구간에서 경계값 \\(x(a)=\\alpha,\\, x(b)=\\beta\\) 와 함께 다음과 같이 주어졌다고 하자.\n\\[\nx'' = p(t)x' + q(t) x + r(t)\n\\tag{2}\\]\n우리는 일변수 함수의 미분과 적분 에서 차분법을 통해 미분값을 계산하였다.\n\\[\n\\begin{aligned}\nx'(t) &= \\dfrac{x(t+h)-x(t-h)}{2h} + O(h^2), \\\\\nx''(t)  &= \\dfrac{x(t+h) - 2x(t) + x(t-h)}{h^2} + O(h^2),\n\\end{aligned}\n\\]\n를 식 2 에 대입하면,\n\\[\n\\begin{aligned}\nx(t+h) -2x(t) +x(t-h) &= p(t)\\left[x(t+h) - x(t-h)\\right] \\dfrac{h}{2} \\\\\n&\\qquad \\qquad + q(t)x(t) h^2+ r(t)h^2 + (O(h^4)+p(t)O(h^2))\n\\end{aligned}\n\\]\n이므로\n\\[\n-\\left[1+\\dfrac{h}{2}p(t)\\right]x(t-h) + \\left[2+h^2q(t)\\right]x(t) -\\left[1- \\dfrac{h}{2}p(t)\\right] x(t+h) = -h^2r(t)\n\\tag{3}\\]\n이다. \\([a,\\,b]\\) 구간을 \\(t_0 = a\\) 에서 \\(t_{N+1}=b\\) 의 구간으로 분할한다고 하자. 이 때 구간 간격 \\(h\\) 는 \\(h=\\dfrac{b-a}{N+1}\\) 이다. \\(t_i\\) 과 \\(x_i\\) 을 다음과 같이 정의하자.\n\\[\n\\begin{aligned}\nt_i &= a+ \\dfrac{i}{N+1}(b-a) , \\\\\nx_i &= x(t_i).\n\\end{aligned}\n\\]\n그렇다면 식 3 는 다음과 같이 쓸 수 있다. 여기서 \\(x_0 = \\alpha,\\, x_{N+1}=\\beta\\) 이다.\n\\[\n\\begin{aligned}\n\\left[2+h^2q(t_1)\\right]x_1 -\\left[1- \\dfrac{h}{2}p(t_1)\\right] x_2 & = -h^2r(t_1) + \\left[1+\\dfrac{h}{2}p(t_1)\\right]\\alpha  , \\\\\n-\\left[1+\\dfrac{h}{2}p(t_i)\\right]x_{i-1} + \\left[2+h^2q(t_i)\\right]x_i -\\left[1- \\dfrac{h}{2}p(t_i)\\right] x_{i+1} & = -h^2r(t_i), \\\\\n-\\left[1+\\dfrac{h}{2}p(t_N)\\right]x_{N-1} + \\left[2+h^2q(t_N)\\right]x_N &= -h^2r(t_N) +\\left[1- \\dfrac{h}{2}p(t_N)\\right] \\beta\n\\end{aligned}\n\\]\n\\(N \\times N\\) 행렬 \\(\\boldsymbol{A}\\) 를 다음과 같이 정의하자. 아래의 식에 나타나지 않은 성분은 모두 \\(0\\) 이다.\n\\[\n\\begin{aligned}\n\\boldsymbol{A}_{1, 1:2} & = \\begin{bmatrix} 2+h^2q(t_1) &  -1+\\dfrac{h}{2}p(t_1)\\end{bmatrix}, \\\\\n\\boldsymbol{A}_{j, j-1:j+1} & = \\begin{bmatrix} -1-\\dfrac{h}{2}p(t_j) & 2+ h^2q(t_j) & -1 + \\dfrac{h}{2}p(t_j) \\end{bmatrix}, \\\\\n\\boldsymbol{A}_{N, N-1:N} & = \\begin{bmatrix}  -1-\\dfrac{h}{2}p(t_N) & 2+h^2 q(t_N)\\end{bmatrix}. \\\\\n\\end{aligned}\n\\]\n이 때 \\(\\boldsymbol{A}\\) 는 삼중 대각 행렬(tridiagonal matrix) 이다. 또한,\n\\[\n\\boldsymbol{x} = \\begin{bmatrix} x(t_1)\\\\ x(t_2) \\\\ \\vdots \\\\ x(t_{N-1}) \\\\ x(t_N)\\end{bmatrix}, \\qquad \\boldsymbol{b} = \\begin{bmatrix} -h^2 r(t_1) + \\left[1+ \\dfrac{h}{2}p(t_1)\\right] \\alpha \\\\ -h^2 r(t_2) \\\\ \\vdots \\\\ -h^2r(t_{N-1}) \\\\ -h^2r(t_N) + \\left[1-\\dfrac{h}{2}p(t_N)\\right] \\beta \\end{bmatrix}\n\\]\n로 놓으면 경계조건 \\(x(t_0)=\\alpha,\\, x(t_{N+1})=\\beta\\) 를 만족시키면서 선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 통해 미분방정식을 풀 수 있다.\nfunction ode_finite_difference_linear(p::Function, q::Function, r::Function, boundary, condition, Npoints=100)\n    @assert Npoints &gt; 3\n    t0 = range(boundary[1], boundary[2], length = Npoints)\n    t = t0[2:end-1]\n    h = step(t)\n\n    u = @. -1 + (h/2) * p(t[1:end-1])\n    d = @. -1 - (h/2) * p(t[2:end])\n    dd = @. 2 + (h^2)*q(t)\n    A = Tridiagonal(d, dd, u)\n\n    b = @. -h^2 * r(t)\n    b[1] = b[1] + (1+h/2*p(t[1]))*condition[1]\n    b[end] = b[end] + (1-h/2*p(t[end])) * condition[2]\n\n    return t0, Array([condition[1] (A\\b)' condition[2]][1, :])\n\nend\n\n\n\n비선형 미분방정식에 대한 유한차분법\n미분방정식이 \\([a,\\,b]\\) 구간에서 경계값 \\(x(a)=\\alpha,\\, x(b) = \\beta\\) 와 함께 다음과 같이 주어졌다고 하자.\n\\[\nx''(t) = f(t,\\, x(t),\\, x'(t)).\n\\]\n\\(D=\\{(t,\\,x,\\,x') \\in \\mathbb{R}^3 : a\\le t \\le b,\\, x,\\, x'\\in \\mathbb{R}\\}\\) 에 대해 \\(f(t,\\,x,\\,x')\\) 이 다음을 만족한다고 하자.\n  (\\(1\\)) \\(f\\) 와 \\(\\partial_x f,\\, \\partial_{x'}f\\) 는 \\(D\\) 에서 연속이다.\n  (\\(2\\)) 어떤 \\(\\epsilon&gt;0\\) 이 존재하여 모든 \\(D\\) 의 점에서 \\(\\partial_y f &gt; \\epsilon\\) 이다.\n  (\\(3\\)) \\(\\partial_y f\\) 와 \\(\\partial_{y'}f\\) 는 유계이다.\n이 조건에서라면 정리 1 에 따라 유일한 해가 존재한다.\n이제 선형 미분방정식에 대한 유한차분법에서와 같이 \\([a,\\,b]\\) 구간을 \\(t_0=a,\\, t_{N+1}=b\\) 가 되도록 등간격으로 \\(N+1\\) 개의 부분 구간으로 나눈다고 하자. 이 때\n\\[\nt_i = a+\\dfrac{i}{N+1}(b-a)\n\\]\n라고 할 수 있으며\n\\[\nx''(t_i) = f(t_i,\\, x(t_i),\\, x'(t_i))\n\\]\n로 놓자. 이 식은 차분에 대한 식으로 다음과 같이 표현 할 수 있다. 그렇다면 위의 미분방정식은 아래와 같은 연립방정식이 된다. \\(x_i = x(t_i)\\) 라고 놓자.gi\n\\[\n\\dfrac{x(t_{i+1})-2x(t_i) + x(t_{i-1})}{h^2} = f\\left(t_i,\\, x(t_i),\\, \\dfrac{x(t_{i+1})- x(t_{i-1})}{2h}\\right)\n\\]\n\\[\n\\begin{aligned}\n2x_{1} - t_2 + h^2 f(t_1,\\, x_1,\\, \\dfrac{x_2-\\alpha}{2h}) - \\alpha &= 0, \\\\\n- x_1 + 2x_2 - x_3 + h^2 f(t_2,\\,x_2,\\, \\dfrac{x_3-x_1}{2h})  &= 0, \\\\\n&\\vdots \\\\\n- x_{N-2} + 2x_{N-1} - x_N + h^2 f(t_{N-1},\\, x_{N-1},\\, \\dfrac{x_{N}-x_{N-2}}{2h}) &= 0, \\\\\n-x_{N-1} + 2x_N + h^2 f(t_N,\\,x_N,\\, \\dfrac{\\beta-x_{N-1}}{2h}) -\\beta &= 0.\n\\end{aligned}\n\\]\n\n\n\n양자 포텐셜 우물\n1차원 슈뢰딩거 방정식(Schöringer equation) 에서의 무한 포텐셜 우물은 다음과 같이 주어진다. 우리는 지금까지 \\(x\\) 를 함수 혹은 종속변수로 사용했지만 아래의 방정식에서는 독립변수이므로 지금까지 우리의 표기법에서의 \\(t\\) 의 역할을 한다는 것에 유의하라.\n\\[\n-\\dfrac{\\hbar^2}{2m}\\dfrac{d^2\\psi(x)}{dx^2} + V(x) \\psi(x) = E\\psi(x), \\qquad V(x) = \\left\\{\\begin{array}{ll}0 \\qquad &; x\\in [-a, a],  \\\\ +\\infty &; \\text{othewise}. \\end{array}\\right.\n\\]\n이것을 지금까지 우리가 사용했던 표기법 형태로 고치면,\n\\[\n\\dfrac{d\\psi(t)}{dt^2} = -\\dfrac{2mE}{\\hbar^2} \\psi(t), \\qquad \\psi(-a)= \\psi(a)=0\n\\]\n의 1차원 2차 미분방정식의 경계값 문제가 된다.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 경계값 문제"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html",
    "href": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html",
    "title": "상미분 방정식의 초기값 문제-I",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-I"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#미분방정식",
    "href": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#미분방정식",
    "title": "상미분 방정식의 초기값 문제-I",
    "section": "1 미분방정식",
    "text": "1 미분방정식\n\n\n\n\n\n\n상미분 방정식 을 미리 참고하라.\n\n\n\n\n상미분방정식과 편미분 방정식\n일변수 함수에 대한 미분 연산을 포함하는 방정식을 상미분방정식 (ordinary differential equation) 이라고 하며 다변수 함수에 대한 미분연산, 즉 편미분 연산을 표함하는 연산을 편미분 방정식(partial differential equation) 이라고 한다. 여기서는 상미분방정식에 대해 다루며 편미분 방정식에 대해서는 이후 다루기로 한다.\n\n\n\n상미분 방정식의 기본 표기법\n\\([a,\\,b]\\) 구간에서 \\(n\\) 번 미분 가능하며 그 도함수도 연속인 함수의 집합 \\(C^n_{[a,\\,b]}\\) 를 생각하자. 또한 \\(C^n_{[a,\\,b]}\\) 에서 정의된 다음과 같은 연산자를 생각하자.\n\\[\n\\mathcal{L}[\\phi(t)] = \\left[p_n(t) \\dfrac{d^n}{dt^n} + p_{n-1}(t)\\dfrac{d^{n-1}}{dt^{n-1}} \\cdots + p_1(t)\\dfrac{d}{dt} + p_0(t)\\right] \\phi(t).\n\\]\n임의의 상수 \\(a,\\,b\\) 와 \\(\\phi,\\, \\psi \\in C_{\\mathbb{R}}^\\infty\\) 에 대해\n\\[\n\\mathcal{L}\\left[a\\phi (t) + b \\psi (t)\\right] = a \\mathcal{L}\\phi(t) + b \\mathcal{L}\\psi(t)\n\\]\n를 만족하기 때문에 \\(\\mathcal{L}\\) 를 선형연산자 (linear operator) 라고 할 수 있다. 연산자에 피연산자에 대한 미분이 포함되는 연산자를 미분 연산자(differential operator) 라고 하며, 선형 미분 연산자 (linear differential operator) 는 미분연산자를 포함하는 선형 연산자를 의미한다.\n\n미분방정식이 어떤 선형 미분 연산자 \\(\\mathcal{L}\\) 에 대해 \\(\\mathcal{L}\\phi(t) = 0\\) 꼴이 되면 이 미분방정식을 동차 선형 미분 방정식(homogeneous linear differential equation) 라고 하며, 그렇지 않으면 비동차 선형 미분 방정식(inhomogeneous linear differential equation) 이라고 한다.\n미분방정식에서 최고차 미분항이 \\(n\\) 일 때 \\(n\\) 을 이 미분방정식의 차수 (order) 라고 하며, 차수가 \\(n\\) 인 미분방정식을 \\(n\\)-차 미분 방정식이라고 한다. \\(\\dfrac{d^2}{dt^2}x(t) = a\\) 는 2차 비동차 선형 미분 방정식이 된다.\n\n\n\nSuperposition\n동차 선형 미분방정식의 경우 \\(\\phi(t),\\, \\psi(t)\\) 가 각각 미분방정식의 해라면 임의의 상수 \\(a,\\,b\\) 에 대해 \\(a \\phi(t) + b \\psi(t)\\) 도 그 해가 된다. 이를 superposition 이라고 한다.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-I"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#상미분-방정식과-초기값-문제",
    "href": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#상미분-방정식과-초기값-문제",
    "title": "상미분 방정식의 초기값 문제-I",
    "section": "2 상미분 방정식과 초기값 문제",
    "text": "2 상미분 방정식과 초기값 문제\n\\(x(t)\\) 에 대한 미분방정식이 \\(x\\) 에 대해 최대 \\(n\\) 계 미분까지 포함되었을 경우, \\(n\\)-계 상미분 방정식이라 한다. 일반적으로 \\(n\\) 계 미분 방정식은 다음 꼴로 주어진다.\n\\[\nx^{(n)}(t) = f\\left(t, x(t), x'(t), \\ldots, x^{(n-1)}(t)\\right)\n\\]\n이 방정식과 초기값 \\(x(t_0),\\,x^{(1)}(t_0), \\ldots, x^{(n-1)}(t_0)\\) 을 통해 \\(f(t)\\) 를 알아내는 것을 초기값 문제 (initial value problem) 이라고 한다.\n\n\n1계 미분방정식의 경우\n\n\n\n\n\n\n표기법\n\n\n\n미분방정식 \\(x'(t) = f(t,\\,x)\\) 가 주어졌을 때 이 미분방정식의 해 \\(x(t)\\) 의 \\(t=t_i\\) 에서의 값을 \\(x(t_i)\\) 로 표기한다. 그러나 근사적인 방법으로 풀었을 때 \\(t_i\\) 의 값은 \\(x_{i}\\) 와 같은 형식으로 표기한다.\n\n\n\n가장 간단한 1계 미분방정식이라면\n\\[\nx'(t) = f(t, x(t))\n\\]\n꼴이며 이 미분방정식을\n\\[\nx(t+h) = x(t) + f(t, x(t)) h + O(h^2)\n\\]\n를 이용하여 1차 근사로 풀 수 있다. 즉 \\(t_i = t_0 +ih\\) 라고 할 때,\n\\[\nx_{i+1} = x_i + f(t_i, x_i)h, \\qquad i=0,\\,1,\\,2,\\ldots\n\\]\n로 풀 수 있다. 이것이 잠시 후 다룰 오일러 방법이다. 우리는 앞서 리차드슨 외삽법 에서 보았던 것처럼 주변의 복수의 값을 이용하여 정밀도를 높일 수 있다. 즉 \\(f(t_i, x(t_i))\\) 뿐만 아니라 몇몇 \\(\\alpha_j\\) 값에 대해 \\(f(t_i, x(t_i + \\alpha_j h))\\) 를 선형결합하여\n\\[\n\\begin{aligned}\nx(t_{i+1}) &= x(t_i) + \\sum_j c_j f(t_i, x(t_i + \\alpha_j h))h + O(h^k), \\\\\n&= x(t_i) + \\tilde{f}(t_i, h, f,\\, \\{\\alpha_i\\}) +O(h^k)\n\\end{aligned}\n\\]\n을 \\(k&gt;2\\) 에 대해 만족하도록 할 수 있다. 이 \\(\\tilde{f}(t_n, h, f,\\, \\{\\alpha_i\\})\\) 를 얻는 방법에는 오일러 방법, 룽게-쿠타 방법 등 여러가지가 존재한다.\n\n\n\n1계 상미분 방정식 시스템의 경우\n상미분 방정식의 시스템도 존재 할 수 있다. 예를 들어, \\(n\\) 개의 다음과 같은 1계 미분방정식이 존재 할 수 있다.\n\\[\nx_i(t)  = f_i (t,\\,x_1(t),\\,x_2(t),\\ldots,\\,x_n(t)),\\qquad i=1,\\ldots,n\n\\]\n이 경우 우리는 \\(\\boldsymbol{x}(t) = \\begin{bmatrix}x_1(t) & \\cdots & x_n(t)\\end{bmatrix}^T\\) 로 놓고 다음과 같이 표기 할 수 있다.\n\\[\n\\boldsymbol{x}'(t) = \\begin{bmatrix} f_1 (t,\\,\\boldsymbol{x}) \\\\ \\vdots \\\\ f_n (t,\\, \\boldsymbol{x}_n)\\end{bmatrix} = F(t,\\,\\boldsymbol{x})\n\\]\n초기조건 \\(\\boldsymbol{x}(t_0) = \\boldsymbol{x}_0\\) 를 만족하는 \\(\\boldsymbol{x}:[a,\\,b] \\to \\mathbb{R}^m\\) 을 구한다고 하자. \\(\\boldsymbol{x} = \\begin{bmatrix} x_1(t) & \\cdots & x_m(t) \\end{bmatrix}^T\\) 라 할 때 1변수의 경우와 마찬가지로\n\\[\n\\boldsymbol{x}(t_{i+1}) =  \\boldsymbol{x}(t_i) +\\tilde{F}(t_i, h, \\boldsymbol{x}, \\ldots, \\boldsymbol{x}^{(n-1)})\n\\]\n을 통해 구한다.\n\n\n\n다계 상미분 방정식 시스템의 경우\n상미분 방정식의 시스템을 이용하여 1계 상미분 방정식 뿐만 아니라 1계 이상의 상미분 방정식에 대해서도 사용 할 수 있다. 예를 들어 \\(n\\) 계 미분방정식\n\\[\nx^{(n)}(t) = f(t,\\,x,\\,x',\\ldots,\\, x^{n-1})\n\\]\n이 주어졌을 경우, \\(y_1(t) = x(t),\\, y_2 (t) = x'(t) ,\\ldots, y_n (t) = x^{(n-1)}(t)\\) 로 놓고 \\(\\boldsymbol{y}(t) = \\begin{bmatrix} y_1(t) & \\cdots & y_n(t)\\end{bmatrix}^T\\) 라 하면 위의 미분 방정식은\n\\[\n\\boldsymbol{y'}(t) = \\begin{bmatrix} y'_1(t) \\\\ \\vdots \\\\ y'_{n-1}(t) \\\\ y'_n(t)\\end{bmatrix} = \\begin{bmatrix} y_2(t) \\\\ \\vdots \\\\ y_{n}(t)  \\\\ f(t,\\, y_1,\\ldots,\\, y_{n}(t))\\end{bmatrix} = F(t,\\, \\boldsymbol{y})\n\\]\n가 된다. 다계 상미분 방정식 시스템의 경우도 같은 형태를 가진다는 것을 쉽게 예상 이해 할 수 있을 것이다.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-I"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#sec-mathematical_foundation_of_ode",
    "href": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#sec-mathematical_foundation_of_ode",
    "title": "상미분 방정식의 초기값 문제-I",
    "section": "3 상미분 방정식의 해에 대한 수학적 기반",
    "text": "3 상미분 방정식의 해에 대한 수학적 기반\n이제 일계 선형미분방정식이 해를 가질 수 있는 수학적 조건에 대해 알아보기로 하자.\n\n\n\n\n\n\n\n\n정의 1 (Lipschitz 조건) \\(D = \\{ (t,\\,\\boldsymbol{x}): a \\le t \\le b,\\, \\boldsymbol{x}\\in \\mathbb{R}^n\\}\\) 에서 연속인 함수 \\(F(t, \\boldsymbol{x}) : D \\to \\mathbb{R}^n\\) 가 모든 \\((t, \\boldsymbol{x}_1),\\, (t,\\boldsymbol{x}_2)\\in D\\) 에 대해 어떤 \\(L&gt;0\\) 이 존재하여\n\\[\n\\|F(t, \\boldsymbol{x}_1) - F(t, \\boldsymbol{x}_2)\\| \\le L\\| \\boldsymbol{x}_1-\\boldsymbol{x}_2\\|\n\\]\n를 만족할 때 이 함수 \\(F\\) 는 \\(D\\) 에서 Lipschitz 조건 을 만족한다 라고 하며 이때의 \\(L\\) 을 \\(F\\) 에 대한 Lipschitz 상수 라고 한다.\n\n\n\n\n\n아래의 정리는 일계 선형 미분방정식의 초기값 문제의 해의 존재성과 유일성을 말한다. 증명은 꽤나 긴데 여기서는 생략한다. 관심이 있으면 Moris Tenenbaum, Harriy Poland 의 Ordinary Differential Equations 나 J. Store, R. Bulirsch 의 Introduction to Numerical Analysis 와 거기에서의 참고목록을 참고하라.\n\n\n정리 1 \\(D = \\{ (t,\\,\\boldsymbol{x}) : a \\le t \\le b,\\, \\boldsymbol{x}\\in \\mathbb{R}^n\\}\\) 에서 정의된 연속인 함수 \\(F:D \\to \\mathbb{R}^n\\) 이 Lipshitz 조건을 만족한다면 각각의 \\((t_0,\\, \\boldsymbol{x}_0) \\in D\\) 에 대해 다음을 만족하는 \\(\\boldsymbol{x}:\\mathbb{R} \\to \\mathbb{R}^n\\) 이 항상 존재한다.\n  (\\(1\\)) \\(\\boldsymbol{x}\\in C^1_{[a,\\,b]}\\),\n  (\\(2\\)) \\(\\boldsymbol{x}'(t) = F(t,\\, \\boldsymbol{x}(t))\\),\n  (\\(3\\)) \\(\\boldsymbol{x}(t_0) =\\boldsymbol{x}_0\\).\n\n\n\n위의 정리는 \\(F(t,\\,\\boldsymbol{x})\\) 가 Lipschitz 조건을 만족한다면 어떠한 초기조건에 대해서도 \\(\\boldsymbol{x}'(t) = F(t,\\,\\boldsymbol{x})\\) 를 만족하는 \\(C^1\\) 급의 \\(\\boldsymbol{x}(t)\\) 가 항상 존재함을 보장한다.\n\n\n\n정리 2 \\(F:D \\to \\mathbb{R}\\) 이 \\(D\\) 에서 연속이며 Lipschitz 상수 \\(L\\) 에 대해 Lipschitz 조건을 만족한다고 하자. 미분방정식 \\(\\boldsymbol{x}' = F(t,\\,\\boldsymbol{x})\\) 에 대해 초기조건이 \\((t_0,\\, \\boldsymbol{x}_1)\\) 과 \\((t_0,\\, \\boldsymbol{x}_2)\\) 로 주어졌을 때 각각의 미분방정식의 해를 \\(\\boldsymbol{x}(t;\\boldsymbol{x}_1)\\), \\(\\boldsymbol{x}(t,;\\boldsymbol{x}_2)\\) 라고 하면 다음이 성립한다.\n\\[\n\\|\\boldsymbol{x}(t;\\boldsymbol{x}_1) - \\boldsymbol{x}(t; \\boldsymbol{x}_2)\\| \\le e^{L|t-t_0|} \\|\\boldsymbol{x}_1 -\\boldsymbol{x}_2\\| .\n\\]\n\n\n\n\n(증명). \\(\\boldsymbol{x}' = F(t,\\boldsymbol{x})\\) 이므로 \\(\\boldsymbol{x}(t) = \\boldsymbol{x}_0 + \\int_{t_0}^t F(t,\\boldsymbol{x})\\, dt\\) 이다. 따라서 \\[\n\\boldsymbol{x}(t;\\boldsymbol{x}_1) - \\boldsymbol{x}(t;\\boldsymbol{x}_2) = \\boldsymbol{x}_1 - \\boldsymbol{x}_2 + \\int_{t_0}^t \\left[F(t, \\boldsymbol{x}(t,\\,\\boldsymbol{x}_1)) - F(t, \\boldsymbol{x}(t;\\boldsymbol{x}_2))\\right]\\, dt\n\\]\n이며,\n\\[\n\\| \\boldsymbol{x}(t;\\boldsymbol{x}_1) - \\boldsymbol{x}(t;\\boldsymbol{x}_2)\\|  \\le \\| \\boldsymbol{x}_1 - \\boldsymbol{x}_2 \\| + \\int_{t_0}^t \\| F(t, \\boldsymbol{x}(t,\\,\\boldsymbol{x}_1)) - F(t, \\boldsymbol{x}(t;\\boldsymbol{x}_2))\\|\\, dt\n\\tag{1}\\]\n이다. Lipschitz 조건을 이용하면\n\\[\n\\| \\boldsymbol{x}(t;\\boldsymbol{x}_1) - \\boldsymbol{x}(t;\\boldsymbol{x}_2)\\|  \\le \\| \\boldsymbol{x}_1 - \\boldsymbol{x}_2 \\| +  L\\int_{t_0}^t \\| \\boldsymbol{x}(t,\\,\\boldsymbol{x}_1) - \\boldsymbol{x}(t;\\boldsymbol{x}_2)\\|\\, dt\n\\tag{2}\\]\n이다. 여기서\n\\[\n\\Phi(t) := \\int_{t_0}^t \\| \\boldsymbol{x}(t,\\,\\boldsymbol{x}_1) - \\boldsymbol{x}(t;\\boldsymbol{x}_2)\\|\\, dt\n\\]\n라고 하면 \\(\\Phi'(t) =\\| \\boldsymbol{x}(t,\\,\\boldsymbol{x}_1) - \\boldsymbol{x}(t;\\boldsymbol{x}_2)\\|\\) 이며, 식 2 에 따라\n\\[\n\\Phi'(t) \\le \\|\\boldsymbol{x}_1-\\boldsymbol{x}_2\\| + L\\Phi(t)\n\\]\n이다. \\(\\Phi(t_0)=0\\) 인데 위식을 미분방정식 \\(\\Phi'(t) = \\|\\boldsymbol{x}_1-\\boldsymbol{x}_2\\| + L\\Phi(t)\\) 와 초기조건 \\(\\Phi(t_0)=0\\) 에 대해 풀면\n\\[\n\\Phi(t) = e^{L(t-t_0)} \\int_{t_0}^t \\left(\\Phi'(s) - L \\Phi (s)\\right) e^{L(s-t_0)}\\, ds\n\\]\n이다 \\(\\Phi(t)\\) 의 정의에 의해 \\(\\Phi(t)\\ge 0\\) 이며, \\(\\|\\Phi(s)-L\\Phi(s)\\| \\le \\|\\boldsymbol{x}_1-\\boldsymbol{x}_2\\|\\) 이므로 \\(t&gt;t_0\\) 에 대해\n\\[\n0 \\le \\Phi(t) \\le e^{L(t-t_0)} \\|\\boldsymbol{x}_1-\\boldsymbol{x}_2 \\| \\int_{t_0}^t e^{L(s-t_0)}\\, ds = \\dfrac{\\|\\boldsymbol{x}_1-\\boldsymbol{x}_2\\|}{L} (e^{L(t-t_0)}-1)\n\\tag{3}\\]\n이다. 따라서 식 2 을 생각하면\n\\[\n\\|\\boldsymbol{x}(t;\\boldsymbol{x}_1) - \\boldsymbol{x}(t; \\boldsymbol{x}_2)\\| \\le e^{L|t-t_0|} \\|\\boldsymbol{x}_1-\\boldsymbol{x}_2\\|\n\\]\n이다. \\(\\square\\)\n\n\n\n\n정리 3 정리 2 의 조건에 더해 \\(F\\) 의 \\(\\boldsymbol{x}\\) 에 대한 야코비 행렬 \\(\\boldsymbol{J}_F(\\boldsymbol{x})\\) 가 \\(D\\) 에서 존재하며 연속이고 유계라면 즉, 모든 \\((t,\\,\\boldsymbol{x})\\in D\\) 에 대해\n\\[\n\\|\\boldsymbol{J}_F(\\boldsymbol{x}) F(t,\\,\\boldsymbol{x})\\| \\le L\n\\]\n라면 임의의 초기조건 \\((t_0,\\,\\boldsymbol{x}_0)\\in D\\) 에서의 미분방정식 \\(\\boldsymbol{x}'=F(t,\\boldsymbol{x})\\) 의 해 \\(\\boldsymbol{x}(t;\\boldsymbol{x}_0)\\) 는 \\(D\\) 에서 연속이며 미분가능하다. 또한 그 도함수\n\\[\nZ(t; \\boldsymbol{x}_0) := \\nabla_{\\boldsymbol{x}_0} \\boldsymbol{x}(t; \\boldsymbol{x}_0)\n\\]\n는 아래의 미분방정식의 초기값 문제의 해이다.\n\\[\n\\dfrac{d}{dt}\\boldsymbol{Z}(t; \\boldsymbol{x}_0) = \\nabla_{\\boldsymbol{x}_0} \\boldsymbol{f}(t, \\boldsymbol{x}(t; \\boldsymbol{x}_0)) \\boldsymbol{z},\\qquad \\boldsymbol{Z}(t_0,\\,\\boldsymbol{x}_0) = 1.\n\\tag{4}\\]\n\n\n\n\\(\\boldsymbol{Z}(t;\\boldsymbol{x}_0)\\) 와 \\(\\boldsymbol{Z}'(t;\\boldsymbol{x}_0)\\) 는 \\(n \\times n\\) 행렬임에 유의하라. 따라서 식 4 은 \\(n \\times n\\) 개의 미분방정식을 표현한다.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-I"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#오일러-방법-newtons-method",
    "href": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#오일러-방법-newtons-method",
    "title": "상미분 방정식의 초기값 문제-I",
    "section": "4 오일러 방법 (Newton’s method)",
    "text": "4 오일러 방법 (Newton’s method)\n오일러 방법은 초기값이 주어졌을 때 수치해석적으로 미분방정식의 해를 구하는 가장 간단한 방법이다. 실제적이고 진지한 계산에 사용하기예는 오차가 너무 크지만 상미분방정식의 초기값 문제를 푸는 여러가지 기법과 개념을 익힐 수 있다.\n\n전방 오일러 방법과 후방 오일러 방법\n아래와 같은 미분방정식\n\\[\nx'(t) = \\dfrac{dx}{dt} = f(t, x)\n\\]\n와 초기조건 \\(x(t_0)\\) 이 주어졌다고 하자. 테일러 전개에 의해\n\\[\n\\begin{aligned}\nx(t+h) &= x(t) + x'(t)h + O(h^2) =  x(t) + f(t, x)h + O(h^2)\n\\end{aligned}\n\\]\n를 이용하는 방법이다. \\(i=1,\\,2,\\ldots,\\) 에 대해 \\(t_i = t_1 + ih\\) 로 놓고\n\\[\nx_{i+1} =  x_i + f(t_i, x_i) h\n\\tag{5}\\]\n로 근사한다. 이 때 \\(t_i,\\,i=0,\\,1,\\ldots\\) 를 mesh point 라고 한다. 이렇게 \\(x_{i+1} =  x_i + f(t_i, x_i) h\\) 로 근사하는 오일러 방법을 전방 오일러 방법 (forward Euler method) 혹은 양함수적 오일러 방법(explicit Euler method) 라고 한다. 보통 오일러 방법(Euler method) 라고 할 때는 전방 오일러 방법을 의미한다. 이에 반해\n\\[\nx(t-h) = x(t) - x'(t)h + O(h) = x(t) -  f(t,\\,x)h +O(h^2)\n\\]\n로 놓고 \\(t_i = t-h,\\, t_{i+1}=t\\) 라 고 하면,\n\\[\nx_{i+1} = x_i + f(t_{i+1},\\,x_{i+1})h\n\\tag{6}\\]\n을 얻는다. 이를 통해 \\(x(t)\\) 를 얻는 방법을 후방 오일러 방법(backward Euler method) 혹은 음함수적 오일러 방법(implicit Euler method) 라고 한다. 음함수적이라는 이름이 붙는 이유는 \\(f(t,\\,x)\\) 함수가 \\(x\\) 에 의존할 경우 식 6 는 우리가 이미 알고 있는 \\(t_i\\) 와 \\(x(t_i)=x_i\\) 에 대해\n\\[\n\\overline{x} - x_i + f(t_i, \\overline{x})=0\n\\]\n이라는 음함수 형태의 방정식의 해로서 \\(x_{i+1}=\\overline{x}\\) 를 구해야 하기 때문이다. 이 경우에도 \\(f(t,\\,x)= f(t)\\) 라면 쉽게 구할 수 있다.\n\n\n\n다변수 일차 미분 방정식\n\\(\\boldsymbol{x}:\\mathbb{R}\\to \\mathbb{R}^n\\) 이며 \\(x_j(t)\\) 가 \\(\\boldsymbol{x}(t)\\) 의 \\(j\\) 번째 성분일 때 미분방정식이 \\(F:\\mathbb{R}^{n+1}\\to \\mathbb{R}^n\\) 에 대해\n\\[\n\\boldsymbol{x}'(t)=\\dfrac{d\\boldsymbol{x}}{dt} = F(t, \\boldsymbol{x})\n\\]\n로 주어지고, 초기값 \\(t_0,\\,\\boldsymbol{x}(t_0)\\) 가 주어졌을 때 가장 간단하게 \\(\\boldsymbol{x}(t)\\) 를 구하는 방법이다. 테일러 전개를 이용하면,\n\\[\nx_j(t + h) = x_j(t) + x_j'(t)h+ \\dfrac{x_j''(\\xi)}{2}h^2\n\\]\n를 만족하는 \\(\\xi\\in (t,\\, t+h)\\) 이 존재한다는 것을 안다. 이 때 \\(|h | \\ll 1\\) 이면,\n\\[\nx_j(t + h) \\approx x_j(t) + x_i'(t)h\n\\]\n임을 이용하는 것이 오일러 방법이다.\n\\(t_{i+1}=t_i+h\\) 일 때,\n\\[\n\\boldsymbol{x}_{i+1} = \\boldsymbol{x}_i + F(t_i, \\boldsymbol{x}_i) \\, h\n\\]\n를 이용하여 \\(\\boldsymbol{x}_{i+1}\\) 값을 구한다. 아래와 같이 julia 로 구현 할 수 있다.\n# 일변수에 대한 오일러 방법\nfunction ode_euler(\n    fp::Function,\n    t0::Real, \n    x0::Real,\n    Npoints::Integer, \n    h = 1.0e-6)\n    \n    tn = t0 .+ collect(0:1:(Npoints-1)) * h\n    xn = zero(tn)\n    xn[1] = x0\n    for i in 1:(Npoints-1)\n        xn[i+1] = xn[i] + fp(tn[i], xn[i])* h\n    end\n    return tn, xn\nend\n\n\n# 다변수에 대한 오일러 방법\nfunction ode_euler(\n    fp::Vector{&lt;:Function}, \n    t0::Real, \n    x0::Vector{&lt;:Real}, \n    Npoints::Integer, \n    h = 1.0e-6)\n    \n    tn = t0 .+ collect(0:1:(Npoints-1)) * h\n    xn = zeros((length(x0), length(tn)))\n    xn[:,1] = x0\n    for i in 1:(Npoints-1)\n        xn[:, i+1] = xn[:, i] .+ [f(t) for (f, t) ∈ zip(fp, tn[i])] .* h\n    end\n    return tn, xn\nend\n\\(F(x,\\,y)\\) 가 변수 2개에 대한 함수일 때는 위의 함수를 약간 수정하여 구현 할 수 있을 것이다.\n우리는 \\(y'= e^x,\\, y(0)=1\\) 일 때, \\(y=e^x\\) 임을 알고 있다. 아래 그림은 위의 ode_euler 함수를 이용하여 미분방정식을 푼 값과, 실제 함수값을 비교해 보았다.\n\nLaTeXStrings 는 \\(\\LaTeX\\) 의 수식을 사용 할 수 있도록 해 주는 패키지이다.\nusing LaTeXStrings, Plots\ntn, xn = ode_euler((t, x)-&gt;exp(t), 0.0, 1.0, 500, 1.0e-2)\nplot(tn, xn, lw = 2, frame = :box, label = \"Euler method\")#, dpi=300)\nplot!(tn, exp.(tn), lw=2, ls = :dash, label = L\"f(t) = e^t\")\n\n\n\n\n\n\n그림 1: Euler 방법\n\n\n\n\n다변수 오일러 방법을 사용해 볼 수 있는 문제중의 하나는 로렌츠 방정식이다.\n\\[\n\\begin{aligned}\n\\dfrac{dx}{dt} &  = \\sigma (y-x), \\\\\n\\dfrac{dy}{dt} & = x(\\rho - z)-y, \\\\\n\\dfrac{dz}{dt} &= xy-\\beta z.\n\\end{aligned}\n\\tag{7}\\]\n\\(\\boldsymbol{x} =\\begin{bmatrix} x & y & z\\end{bmatrix}^T\\) 로 놓고 다변수 오일러 방법을 통해 구해보자. 어떤 조건에서 위의 미분방정식의 해가 나비를 닮았기 때문에 함수 이름을 butterfly3d 로 하였다. \\(\\sigma=10\\), \\(\\rho=28\\), \\(\\beta=8/3\\) 일 때,\nfunction butterfly3d(v, σ::Real, ρ::Real, β::Real)\n    return [σ*(v[2]-v[1]), v[1]*(ρ - v[3])-v[2], v[1]*v[2]-β*v[3]]\nend\n\ntn, xn = ode_euler((t, v)-&gt;butterfly3d(v, 10, 28, 8/3), 0.0, [0.0, 1.0, 1.0], 4000, 1.0e-2)\n를 통해 얻은 궤적을 그리면 다음과 같다.\nusing Plot, LaTeXStrings\np0 = plot(xn[1,:], xn[2,:], xn[3,:], frame=:box, aspect_ratio=1.0, label=L\"xyz\", dpi=300)\np1 = plot(xn[1,:], xn[2,:], frame=:box, aspect_ratio=1.0, label=L\"xy\", color=:red)\np2 = plot(xn[2,:], xn[3,:], frame=:box, aspect_ratio=1.0, label=L\"yz\", color=:green)\np3 = plot(xn[3,:], xn[1,:], frame=:box, aspect_ratio=1.0, label=L\"zx\", color=:blue)\n\nplot!(p0, p1, p2, p3, layout=4)\n\n\n\n\n\n\n그림 2: 로렌츠 방정식\n\n\n\n\n\n\n일변수 이차 미분 방정식\n\\(\\dfrac{d^2x}{dt^2} = f(t, x, x')\\) 으로 주어진 미분방정식과 초기값 \\(x(t_1),\\, x'(t_1)\\) 이 주어졌다고 하자. \\(y(t) = x'(t)\\) 라고 하면,\n\\[\n\\begin{aligned}\ny'(t) &= f(t, x, y), \\\\\nx'(t) &= y(t)\n\\end{aligned}\n\\]\n라는 2변수에 대한 1차 미분방정식 꼴이 된다. 즉, \\(\\boldsymbol{y} = \\begin{bmatrix} x(t) \\\\ x'(t) \\end{bmatrix}\\) 로 놓았을 때의 이변수 일차 미분방정식이다.\n가장 간단한 1차원 조화진동자 문제를 보자. 미분방정식이\n\\[\n\\dfrac{d^2 x(t)}{dt^2} = -k\\cdot x(t)\n\\tag{8}\\]\n로 주어졌을 때 \\(\\boldsymbol{x}(t) = \\begin{bmatrix} x(t) \\\\ x'(t) \\end{bmatrix}\\) 로 놓으면,\n\\[\n\\boldsymbol{y}(t+h) = \\begin{bmatrix} x(t+h) \\\\ x'(t+h)\\end{bmatrix} = \\begin{bmatrix} x(t) + x'(t)h \\\\ x'(t) -k  x(t) h \\end{bmatrix} = \\boldsymbol{y}(t) + h\\begin{bmatrix} 0 & 1\\\\ -k & 0\\end{bmatrix} \\boldsymbol{y}(t)\n\\]\n이를 위해 다음과 같은 코드를 작성하였으며 그래프로 그려보면 다음과 같다.\nfunction sho1d(y::Vector{&lt;:Real}, k::Real, x0=0.0)\n    return [y[2], -k*(y[1]-x0)]\n\ntn, xn = ode_euler((t, y)-&gt;sho1d(y, 3, 1.0), 2.0, [1.1, 0.0], 10000, 1.0e-2)\nplot(tn, xn[1,: ], label = :none)\n\n\n\n\n\n\n그림 3: 오일러 방법으로 계산한 1차원 조화진동자\n\n\n\n1차원 조화진동자는 잘 알려져 있다 시피 삼각함수로 그 진폭이 커지면 안되지만 여기서는 커지는데 이것은 오일러 방법이 기본적으로 에러에 취약하기 때문이다. 이로 인해 실제 계산에서는 오일러 방법은 잘 사용되지 않는다.\n\n\n\n다변수 이차 미분 방정식\n\\(\\dfrac{d^2\\boldsymbol{x}}{dt^2} = F(t, \\boldsymbol{x}, \\boldsymbol{x}')\\) 으로 주어진 미분방정식은,\n\\[\n\\begin{aligned}\n\\boldsymbol{v}(t_{i}+h) &=\\boldsymbol{v}(t_i) + F(t, \\boldsymbol{x}(t_i), \\boldsymbol{v}(t_i)) h,\\\\\n\\boldsymbol{x}(t_i+h) &=\\boldsymbol{x}(t_i) + \\boldsymbol{v}(t_i)h\n\\end{aligned}\n\\]\n와 같이 2개의 1차 미분방정식에 대한 오일러 방법에 의한 해로 주어진다.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-I"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#고차-테일러-방법",
    "href": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#고차-테일러-방법",
    "title": "상미분 방정식의 초기값 문제-I",
    "section": "5 고차 테일러 방법",
    "text": "5 고차 테일러 방법\n미분방정식이 아래와 같이 주어졌다고 하자. \\[\n\\dfrac{d}{dt}x(t) = f(t,\\,x),\\qquad x(t_0) = x_0\n\\]\n만약 \\(f(x,\\,t)\\) 에 대한 \\(k\\) 차 도함수를 구할 수 있다면 \\(x(t)\\) 에 대한 테일러 전개는 다음과 같다. 두말하면 잔소리지만 \\(\\dfrac{df}{dt} = \\dfrac{\\partial f}{\\partial t} + \\dfrac{\\partial f}{\\partial x}\\dfrac{dx}{dt}\\) 임에 유의하라.\n\\[\nx(t_{i+1}) = x(t_i) + f(t_i,\\, x_i)h + \\dfrac{1}{2}\\dfrac{df}{dt }(t_i,\\,x_i)h^2 + \\cdots + \\dfrac{1}{(k+1)!}\\dfrac{d^k f}{dt^k}(t_i,\\,x_i) h^{k+1} + O(h^{k+2}).\n\\]\n이 방법은 \\(f(t,\\,x)\\) 에 대한 \\(k\\) 차 도함수 자체를 쉽게 구할 수 있는 경우에 사용할 수 있다. 참고로 다음에 다룰 룽게-쿠타 방법은 실제 \\(k\\) 차 도함수가 아닌 수치해석적으로 계산한 \\(k\\) 차 도함수의 값을 사용한다.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-I"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#룽게-쿠타-방법",
    "href": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#룽게-쿠타-방법",
    "title": "상미분 방정식의 초기값 문제-I",
    "section": "6 룽게-쿠타 방법",
    "text": "6 룽게-쿠타 방법\n\n2차 룽게-쿠타 방법 (RK2)\n역시 \\[\n\\dfrac{d\\boldsymbol{x}}{dt} = F(t, \\boldsymbol{x})\n\\]\n와 초기조건 \\(\\boldsymbol{x}_0 = \\boldsymbol{x}(t_0)\\) 가 주어졌다고 하자. \\(\\boldsymbol{x}\\) 와 \\(F(t,\\boldsymbol{x})\\) 의 \\(j\\) 번째 성분을 각각 \\(x_j,\\, f_j(t,\\,\\boldsymbol{x})\\) 라고 하면,\n\\[\n\\dfrac{d^2 x_j}{dt^2 } = \\dfrac{d}{dt}f_j(t, \\,\\boldsymbol{x}) = \\dfrac{\\partial f_j}{\\partial t} + \\sum_k\\dfrac{dx_k}{dt} \\cdot \\dfrac{\\partial f_j}{\\partial x_k}\n\\]\n라는 것을 안다. 이것을 이용하여 2차항까지 테일러 전개를 하면, \\[\n\\begin{aligned}\nx_j(t+h) & = x_j(t) + \\dfrac{dx_j}{dt}h  + \\dfrac{1}{2}\\dfrac{d^2 x_j}{dt^2}h^2 + \\cdots \\\\\n&= x_j(t)  + f_j(t, \\boldsymbol{x})\\, h + \\dfrac{1}{2} \\left(\\dfrac{\\partial f_j}{\\partial t} + \\sum_k \\dfrac{dx_k}{dt} \\dfrac{\\partial f_j}{\\partial x_k}\\right) h^2 + \\cdots \\\\\n&= x_j(t)  + f_j(t, \\boldsymbol{x})\\, h + \\dfrac{1}{2} \\left(\\dfrac{\\partial f_j}{\\partial t} + \\sum_k f_k(t,\\,\\boldsymbol{x}) \\dfrac{\\partial f_i}{\\partial x_k}\\right) h^2 + \\cdots\n\\end{aligned}\n\\]\n이다. 이 때,\n\\[\n\\begin{aligned}\n\\boldsymbol{k}_1 &= F(t_i, \\boldsymbol{x}(t_i)) \\,,\\\\\n\\boldsymbol{k}_2 &= F\\left( t_i + \\dfrac{h}{2},\\, \\boldsymbol{x}(t_i) + \\dfrac{h}{2} \\boldsymbol{k}_1\\right)\n\\end{aligned}\n\\]\n라 하자. 그렇다면, 야코비 행렬 \\(\\left(\\boldsymbol{J}_F(t_i, \\boldsymbol{x}(t_i))\\right)_{jk} = \\dfrac{\\partial f_j}{\\partial x_k}(t_i, \\boldsymbol{x}(t_i))\\) 에 대해\n\\[\n\\begin{aligned}\n\\boldsymbol{k}_2 &= F(t_i,\\, \\boldsymbol{x}(t_i)) +  \\dfrac{h}{2} \\dfrac{\\partial F}{\\partial t}(t_i,\\, \\boldsymbol{x}(t_n)) + \\dfrac{h}{2} \\boldsymbol{J}_F(t_i, \\boldsymbol{x}(t_i)) \\boldsymbol{k}_1 \\\\\n&= \\boldsymbol{k}_1 + \\dfrac{h}{2} \\left(\\dfrac{\\partial F}{\\partial t}(t_i,\\, \\boldsymbol{x}(t_i))  + \\boldsymbol{J}_F(t_i, \\boldsymbol{x}(t_i)) \\boldsymbol{k}_1 \\right)\n\\end{aligned}\n\\]\n이므로,\n\\[\n\\begin{aligned}\n\\dfrac{\\partial F}{\\partial t}(t_i,\\, \\boldsymbol{x}(t_i))  + \\boldsymbol{J}_F(t_i, \\boldsymbol{x}(t_i)) \\boldsymbol{k}_1 & = \\dfrac{2}{h}(\\boldsymbol{k}_2-\\boldsymbol{k}_1)\n\\end{aligned}\n\\]\n이다. 이를 이용하면,\n\\[\n\\boldsymbol{x}(t_{i+1}) = \\boldsymbol{x}(t_i) + \\boldsymbol{k}_1h + \\dfrac{h^2}{2} \\dfrac{2}{h} (\\boldsymbol{k}_2-\\boldsymbol{k}_1) = \\boldsymbol{x}(t_i)+ \\dfrac{h}{2}(\\boldsymbol{k}_1+\\boldsymbol{k}_2)\n\\]\n을 얻는다. 이것을 2차 룽게-쿠타 방법(RK2)이라 한다.\n\n\n\n4차 룽게-쿠타 방법 (RK4)\n2차 룽게 쿠타 방법은 테일러 전개의 2차항까지 사용하였다. 4차 룽게-쿠타 방법(RK4)은 테일러 전개의 4차항까지 사용한다. \\[\n\\boldsymbol{x}(t+h)  = \\boldsymbol{x}(x) + \\boldsymbol{x}'(x) h + \\dfrac{\\boldsymbol{x}''(t)}{2} h^2 + \\dfrac{\\boldsymbol{x}^{(3)}(t)}{6}h^3 + \\dfrac{\\boldsymbol{x}^{(4)}(t)}{24}h^4 + \\cdots\n\\]\n아래와 같이 \\(\\boldsymbol{k}_1,\\,\\boldsymbol{k}_2,\\,\\boldsymbol{k}_3,\\,\\boldsymbol{k}_4\\) 를 정의하면, \\[\n\\begin{aligned}\n\\boldsymbol{k}_{1} &= F(t_{i},\\, \\boldsymbol{x}(t_{i})),\\\\\n\\boldsymbol{k}_{2}&=\\ F\\!\\left( t_{i}+{\\frac {h}{2}},\\, \\boldsymbol{x}(t_{i})+ \\frac{h}{2}\\boldsymbol{k}_{1}\\right),\\\\\n\\boldsymbol{k}_{3}&=\\ F\\!\\left(t_{i}+{\\frac {h}{2}},\\,  \\boldsymbol{x}(t_{i})+\\frac{h}{2} \\boldsymbol{k}_{2}\\right),\\\\\n\\boldsymbol{k}_{4}&=\\ F\\!\\left(t_{i}+h,\\, \\boldsymbol{x}(t_{i})+h\\boldsymbol{k}_{3}\\right).\n\\end{aligned}\n\\]\n4차항까지의 테일러 전개가 아래와 같다는 것을 보일 수 있다. 아주 지루한 과정이므로 여기서는 생략한다. \\[\n\\begin{aligned}\nt_{i+1}&=t_{i}+h, \\\\\n\\boldsymbol{x}(t_{i+1})&=\\boldsymbol{x}(t_{i})+{\\frac {h}{6}}\\left(\\boldsymbol{k}_{1}+2 \\boldsymbol{k}_{2}+2\\boldsymbol{k}_{3}+\\boldsymbol{k}_{4}\\right).\n\\end{aligned}\n\\]\nRK2 및 RK4 이용하여 상미분 방정식을 푸는 julia 코드는 아래와 같다. 모두 이변수 함수 f 와 초기값 x0, y0, 위의 \\(h\\) 에 해당하는 값 epsilon 과 초기 값을 포함하여 몇개의 점에 대해 얻을지에 대한 Npoints 를 인자로 받는다.\nfunction ode_rk2(f::Function, \n    t1::Real, \n    x1::Vector{&lt;:Real}, \n    Npoints::Integer, \n    h = 1.0e-6) \n    tn = t1 .+ collect(0:1:(Npoints-1)) * h\n    xn = zeros((length(x1), length(tn)))\n    xn[:,1] = x1\n    for i in 1:(Npoints-1)\n        k1 = f(tn[i], xn[:,i])\n        k2 = f(tn[i] + h/2, xn[:, i] .+ k1.*(h/2))\n        xn[:, i+1] = xn[:,i] .+ (k1 .+ k2) .*(h/2)\n    end \n    return tn, xn\nend\n\nfunction ode_rk4(f::Function, \n    t1::Real, \n    x1::Vector{&lt;:Real}, \n    Npoints::Integer, \n    h = 1.0e-6) \n    tn = t1 .+ collect(0:1:(Npoints-1)) * h\n    xn = zeros((length(x1), length(tn)))\n    xn[:,1] = x1\n    for i in 1:(Npoints-1)\n        k1 = f(tn[i], xn[:, i])\n        k2 = f(tn[i] + h/2, xn[:, i] .+ k1.*(h/2))\n        k3 = f(tn[i] + h/2, xn[:,i] .+ k2 .*(h/2))\n        k4 = f(tn[i] + h, xn[:, i] .+ k3 .* h)\n        xn[:, i+1] = xn[:, i] .+ (k1 .+ (2.0 .* k2) .+ (2.0 .* k3) .+ k4).*(h/6)\n    end\n    return tn, xn\nend\n오일러 방법에서 사용했던 로렌츠 방저식에 대한 사용법은 아래와 같다.\ntn, xn = ode_rk2((t, x)-&gt;butterfly3d(x, 10, 28, 8/3), 0.0, [0.0, 1.0, 1.0], 50000, 1.0e-3)\nplot(xn[1,:], xn[2,:], xn[3,:], label=\"RK2\")\n\n\n\n\n\n\n그림 4: RK2를 이용한 로렌츠 방정식의 궤적\n\n\n\n\n\n\n\n\n\n그림 5: RK4를 이용한 로렌츠 방정식의 궤적",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-I"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#예제-examples",
    "href": "src/numerical_analysis_using_julia/11_ode_initial_value_problem_1.html#예제-examples",
    "title": "상미분 방정식의 초기값 문제-I",
    "section": "7 예제 (Examples)",
    "text": "7 예제 (Examples)\n\n단순 조화 진동자\n식 8 로 주어진 단순 조화 진동자 문제를 오일러 방법과 RK2, RK4 로 풀어보자. 오일러 방법에서와 같이 \\(\\boldsymbol{x}(t) = \\begin{bmatrix} x(t) \\\\ x'(t) \\end{bmatrix}\\) 로 로 놓고 수치해석적으로 풀 수 있다.\nfunction sho1d(y::Vector{&lt;:Real}, k::Real, x0=0.0)\n    return [y[2], -k*(y[1]-x0)]\nend\ntn1, xn1 = ode_rk2((t, y)-&gt;sho1d(y, 3, 1.0), 2.0, [1.1, 0.0], 10000, 1.0e-2)\ntn2, xn2 = ode_rk4((t, y)-&gt;sho1d(y, 3, 1.0), 2.0, [1.1, 0.0], 10000, 1.0e-2)\n그 결과를 그래프로 그리면 다음과 같다.\n\n\n\n\n\n\n그림 6: RK 방법으로 푼 단순조화진동자\n\n\n\n\n오일러 방법이나 RK2 방법에서는 실제의 해와 많이 다르게 진폭(amplitude) 가 증가하지만 RK4 방법에서는 최소한 우리가 관찰한 영역에서는 진폭이 증가하지 않는다.\n\n\n\n감쇄 조화 진동자\n\\[\nm \\dfrac{d^2x}{dt^2} + \\gamma \\dfrac{dx}{dt} + kx=0\n\\]\n로 기술되는 \\(x(t)\\) 를 감쇄조화진동자라 한다. 일단 방정식을 간단히 하자.\n\\[\n\\dfrac{d^2x}{dt^2} + \\dfrac{\\gamma}{m}\\dfrac{dx}{dt} + \\dfrac{k}{m} x = 0\n\\]\n\\(v=x'(t)\\) 라 하면 \\(v'(t) = -\\dfrac{\\gamma}{m}\\dfrac{dx}{dt} - \\dfrac{k}{m} x\\) 이므로,\n\\[\n\\dfrac{d}{dt} \\begin{bmatrix} x \\\\ v\\end{bmatrix} = \\begin{bmatrix} v \\\\ -\\dfrac{\\gamma}{m}v - \\dfrac{k}{m} x\\end{bmatrix}\n\\]\n이다.\n많은 고전물리학이나 미분방정식 교재에 이 감쇄조화진동자가 나와 있다. 감쇄조화진동자의 해는 다음과 같다.\n\\[\nx(t) = \\exp(-\\lambda t) \\left[ a \\exp \\left(\\sqrt{-\\omega^2} \\right)+ b \\exp \\left(-\\sqrt{-\\omega^2}\\right)\\right]\\qquad \\text{where }  \\lambda  =\\dfrac{\\gamma}{2m},\\, \\omega^2 = \\dfrac{k}{m}-\\dfrac{\\gamma^2}{4m^2}\n\\]\n이다. 여기서 \\(a,\\,b\\) 는 초기조건 \\(x(t=0),\\, \\dot{x}(t=0)\\) 에 의해 정해진다.\n\\(\\omega_0^2 &gt;0\\) 이면 \\(x(t)\\) 의 \\([\\,\\cdots]\\) 부분이 \\(\\cos\\) 함수 (혹은 \\(\\sin\\) 함수) 꼴로 나타날 수 있으므로 진폭이 점점 줄어드는 진동을 한다. 이를 underdamping 라 한다. \\(\\omega_0^2 &lt;0\\) 이면 진동 없이 감쇄만 하게 되며 이를 overdamping 라 한다. \\(\\omega_0^2 = 0\\) 일 경우를 critical damping 이라 한다.\n\\(m=1,\\, \\gamma=1,\\, k=1\\) 과 \\(x(t=0)=0,\\, \\dot{x}(t=0)=1\\) 일 때에 대해 오일러 방법, 2차 및 4차 룽게 쿠타 방법과, 해석적 해를 같이 그려보자.\n\n\n\n\n\n\n\n그림 7: 감쇄조화진동자\n\n\n\n\n\n\n진자(Pendulum) 문제\n\n\n\n\n\n\n그림 8: 진자\n\n\n\n\n중력가속도 \\(g\\) 의 영향을 받는 길이 \\(L\\)인 진자의 운동은 다음의 미분방정식으로 기술된다.\n\\[\n\\dfrac{d^2 \\theta}{dt^2} = -\\dfrac{g}{L} \\sin \\theta\n\\]\n보통 \\(\\theta\\) 가 아주 작을 때 \\(\\sin \\theta \\approx \\theta\\) 로 가정하여 풀면\n\\[\n\\dfrac{d^2\\theta}{dt^2}= -\\left( \\dfrac{g}{L} \\right)\\theta\n\\]\n이므로 단순조화진동자 문제이다.\n이제 수치적으로 풀어보자. \\(\\phi= \\dfrac{d\\theta}{dt}\\), \\(\\boldsymbol{x} = \\begin{bmatrix} \\theta \\\\ \\phi \\end{bmatrix}\\) 라 하면,\n\\[\n\\dfrac{d}{dt}\\begin{bmatrix} \\theta \\\\ \\phi \\end{bmatrix} = \\begin{bmatrix} \\phi \\\\ -g/L \\sin \\theta  \\end{bmatrix}\n\\]\n가 된다. \\(g=9.8,\\, L=5.0\\), 시간 간격을 \\(1\\times 10^{-2}\\) 잡고 미분방정식을 3가지 방법으로 풀면\nfunction pendulum(y::Vector{&lt;:Real}, g=9.8, l=2)\n    return [y[2], -g/l*sin(y[1])]\nend\ng, l = 9.8, 5.0\ntn1, xn1 = ode_euler((t, y)-&gt;pendulum(y, g, l), 0.0, [0.0, 0.04], 20000, 1.0e-2)\ntn2, xn2 = ode_rk2((t, y)-&gt;pendulum(y, g, l), 0.0, [0.0, 0.04], 20000, 1.0e-2)\ntn3, xn3 = ode_rk4((t, y)-&gt;pendulum(y, g, l), 0.0, [0.0, 0.04], 20000, 1.0e-2)\n\n\n\n\n\n\n\n그림 9: 진자\n\n\n\n\n와 같다. 오일러 방법은 비교적 빠르게, RK2 는 비교적 느리게 발산하지만 RK4 는 시간 범위 내에서 안정적임을 알 수 있다.\n\n\n\n2체 문제\n중력에 의한 2체 문제를 생각해보자. \\(m_1\\) 과 \\(m_2\\) 의 질량을 갖는 두 점입자가 오직 만유인력에 의해서만 영향을 받아 움직일 때, 그 운동은 다음의 2차 미분방정식에 의해 기술된다.\n\\[\n\\begin{aligned}\nm_1\\ddot{\\boldsymbol{r}}_1  =  - \\dfrac{Gm_1m_2}{\\|\\boldsymbol{r}_1-\\boldsymbol{r}_2\\|^3}(\\boldsymbol{r}_1 - \\boldsymbol{r}_2), \\\\\nm_2\\ddot{\\boldsymbol{r}}_2  = - \\dfrac{Gm_1m_2}{\\|\\boldsymbol{r}_1-\\boldsymbol{r}_2\\|^3}(\\boldsymbol{r}_2 - \\boldsymbol{r}_1),\n\\end{aligned}\n\\]\n\\(x,\\,y,\\,z\\)-좌표계에서 \\(m_1\\) 의 위치와 속력을 각각 \\(\\boldsymbol{r}_1 = \\begin{bmatrix} x_1 & y_1 & z_1\\end{bmatrix}^T\\), \\(\\boldsymbol{v}_1 = \\begin{bmatrix} v_1 & u_1 & w_1\\end{bmatrix}^T\\) 라고 하고 \\(m_2\\) 의 위치와 속력을 각각 \\(\\boldsymbol{r}_2 = \\begin{bmatrix} x_2 & y_2 & z_2\\end{bmatrix}^T\\), \\(\\boldsymbol{v}_2 = \\begin{bmatrix} v_2 & u_2 & w_2\\end{bmatrix}^T\\) 라고 하자. 이것을 전체적으로 기술하는 열벡터 \\(\\boldsymbol{s}\\) 를\n\\[\n\\boldsymbol{s} = \\begin{bmatrix} x_1 & y_1 & z_1 & x_2 & y_2 & z_2 & v_1 & u_1 & w_1 & v_2 & u_2 & w_2\\end{bmatrix}^T\n\\]\n라 하자. \\(R=\\|\\boldsymbol{r}_1-\\boldsymbol{r}_2\\|\\), \\(k_1 = \\dfrac{Gm_1}{R^3}, k_2 = \\dfrac{Gm_2}{R^3}\\) 이라 하면,\n\\[\n\\dfrac{d\\boldsymbol{s}}{dt} = \\begin{bmatrix} & & \\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{array} & \\\\\n& & & \\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{array} \\\\\n\\begin{array}{ccc} -k_2 & 0 & 0 \\\\ 0 & -k_2 & 0 \\\\ 0 & 0 & k_2 \\\\ k_1 & 0 & 0 \\\\ 0 & k_1 & 0 \\\\ 0 & 0 & k_1 \\end{array} & \\begin{array}{ccc} k_2 & 0 & 0 \\\\ 0 & k_2 & 0 \\\\ 0 & 0 & k_2 \\\\ -k_1 & 0 & 0 \\\\ 0 & -k_1 & 0 \\\\ 0 & 0 & -k_1 \\end{array}\n\\end{bmatrix} \\cdot \\boldsymbol{s}\n\\]\n이제 두 점입자의 초기 위치와 속도가 주어지면 \\(\\boldsymbol{s}_{i+1} = \\boldsymbol{s}_i+ \\dfrac{d\\boldsymbol{s}}{dt} h\\) 를 이용하여 이 문제를 풀 수 있다.\n\n\n\nInitialValueOdeProblem\n위의 세가지 방법은 같은 입력값에 대해 계산하는 함수들로 구현되었다. 이것을 하나의 함수로 사용하도록 아래와 같이 구현하였다.\node_initial_methods = Dict(:euler =&gt; ode_euler, :rk2 =&gt; ode_rk2, :rk4 =&gt; ode_rk4)\n\nmutable struct InitialValueOdeProblem\n    diffeq::Function\n    t1::Number\n    Npoints::Integer\n    h::Real\n    initial_value::Vector\n\n    function InitialValueOdeProblem(diffeq::Function, t1::Number, initial_value,  Npoints::Integer, h::Real)\n        @assert Npoints &gt; 3\n        @assert h &gt; 0\n        if isa(initial_value, Real) \n            initial_value = [initial_value, ]\n        end\n        return new(diffeq, t1, Npoints, h, initial_value)\n    end\nend\n\nfunction solve(ode::InitialValueOdeProblem, method::Symbol)\n    @assert method ∈ keys(ode_initial_methods)\n    return ode_initial_methods[method](ode.diffeq, ode.t1, ode.initial_value, ode.Npoints, ode.h)\n\nend\n이제 진자 문제는 다음과 같이 풀 수 있다.\nfunction de_pendulum(y::Vector{&lt;:Real}, g=9.8, l=2)\n    return [y[2], -g/l*sin(y[1])]\nend\ng, l = 9.8, 5.0\n\npendulum = InitialValueOdeProblem((t, y)-&gt;de_pendulum(y, g, l), 0.0, [0.0, 0.04], 20000, 1.0e-2)\ntn1, xn1 = solve(pendulum, :euler)\ntn2, xn2 = solve(pendulum, :rk2)\ntn3, xn3 = solve(pendulum, :rk4)\n\np1=plot(tn1, (180.0/π).* xn1[1,: ], label = \"Euler\", xlabel = L\"t\", ylabel = L\"\\textbf{Degree}\", title = L\"\\textbf{Pendulum}\", lc=:red, dpi=300)#, size=(400, 350))\nplot!(tn2, (180.0/π).* xn2[1,: ], label = \"RK2\", lc=:green)\n\nplot!(tn3, (180.0/π).* xn3[1,: ], label = \"RK4\", lc=:blue)#, xlabel = L\"t\", ylabel = L\"\\textbf{Amplitude}\", title = L\"RK4\", size=(400, 350))\n\n\n마치며\n이번 장에서는 상미분 방정식을 푸는 가장 기본적인 방법인 오일러 방법과 2차, 4차 룽게-쿠타 방법을 배웠다. 수치해석이 늘 그렇듯이 간단한 미분방정식은 잘 풀 수 있지만 다양하고 복잡한 문제에서는 더 정확한 방법을 사용해야 한다. 다음장에서는 실제 과학/공학에서 널리 활용되는 좀 더 정교한 방법에 대해 알아보기로 하자.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-I"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html",
    "href": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html",
    "title": "선형방정식과 반복법",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 II",
      "선형방정식과 반복법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#고유값-스펙트럼반경-반복법",
    "href": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#고유값-스펙트럼반경-반복법",
    "title": "선형방정식과 반복법",
    "section": "1 고유값, 스펙트럼반경, 반복법",
    "text": "1 고유값, 스펙트럼반경, 반복법\n\nLinearAlgebra.jl 에서의 고유값과 고유벡터\n행렬의 고유값과 고유벡터는 다음과 같이 구할 수 있다.\nIn [1]: using LinearAlgebra\n\nIn [2]: V=[1 2 3; 3 1 0; -1 2 0]\nOut[2]: 3×3 Matrix{Int64}:\n  1  2  3\n  3  1  0\n -1  2  0\n\nIn [3]: eig1 = eigen(V)\nOut[3]: Eigen{ComplexF64, ComplexF64, Matrix{ComplexF64}, Vector{ComplexF64}}\nvalues:\n3-element Vector{ComplexF64}:\n -0.948231872006585 - 2.1190466104580317im\n -0.948231872006585 + 2.1190466104580317im\n 3.8964637440131664 + 0.0im\nvectors:\n3×3 Matrix{ComplexF64}:\n 0.0766967-0.521334im  0.0766967+0.521334im   0.68225+0.0im\n  0.345877+0.426578im   0.345877-0.426578im  0.706638+0.0im\n -0.648636-0.0im       -0.648636+0.0im       0.187612+0.0im\n\nIn [4]: eig1.vectors\nOut[5]: 3×3 Matrix{ComplexF64}:\n 0.0766967-0.521334im  0.0766967+0.521334im   0.68225+0.0im\n  0.345877+0.426578im   0.345877-0.426578im  0.706638+0.0im\n -0.648636-0.0im       -0.648636+0.0im       0.187612+0.0im\n\nIn [5]: eig1.values\nOut[6]: 3-element Vector{ComplexF64}:\n -0.948231872006585 - 2.1190466104580317im\n -0.948231872006585 + 2.1190466104580317im\n 3.8964637440131664 + 0.0im\n\nIn [6]: V*eig1.vectors[:,3] .- eig1.values[3] .* eig1.vectors[:,3]\nOut[6]: 3-element Vector{ComplexF64}:\n 1.7763568394002505e-15 + 0.0im\n 1.7763568394002505e-15 + 0.0im\n  5.551115123125783e-16 + 0.0im\neigen(V) 는 행렬 V 의 고유값과 고유벡터를 각각 values 필드와 vectors 필드로 반환한다. \\(k\\) 번째 고유값과 고유벡터는 각각 eig1.values[k], eig1.vectors[:, k] 이다. 행렬이 실수 타입의 성분을 갖더라도 고유값과 고유벡터는 가 복소수라면 복소수로 반환된다. 실수 영역에서 고유값 분해가 된다면 실수로 반환된다.\nIn [11]: A=[4 0 1; 0 2 -1; 1 -1 3]\nOut[11]: 3×3 Matrix{Int64}:\n 4   0   1\n 0   2  -1\n 1  -1   3\n\nIn [12]: eigvals(A)\nOut[12]: 3-element Vector{Float64}:\n 1.267949192431122\n 2.9999999999999996\n 4.732050807568877\n\n고유값 혹은 고유벡터에만 관심이 있다면 각각 eigvals(A), eigvecs(A) 와 같이 할 수 있다.\n\n\n\n스펙트럼 반경\n우리는 앞서 직접적으로 선형 시스템 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 의 해를 구하였다. 그러나 선형 시스템이 매우 크며, \\(\\boldsymbol{A}\\) 의 성분 가운데 대부분이 \\(0\\) 일 경우는 직접적으로 구하지 않고 반복법을 통해 구하는 방법이 더 빠르다. 여기서는 를 알아보기로 한다. 먼저 많이 사용하게 될 개념을 우선 정하기로 하자.\n\n\n\n\n\n\n\n\n정의 1 (스펙트럼 반경(spectral radius)) \n  (\\(1\\)) 정사각 행렬 \\(\\boldsymbol{A}\\) 에 대해 \\(\\lambda(\\boldsymbol{A})\\) 는 \\(\\boldsymbol{A}\\) 의 모든 고유값들의 집합이다.\n  (\\(2\\)) 행렬 \\(\\boldsymbol{A}\\) 의 스펙트럼 반경 \\(\\rho(\\boldsymbol{A})\\) 는 \\(\\lambda(\\boldsymbol{A})\\) 의 절대값의 상한으로 정의된다. 즉,\n\\[\n\\rho (\\boldsymbol{A}) := \\max \\{ |\\lambda| : \\lambda \\in \\lambda(\\boldsymbol{A})\\}.\n\\tag{1}\\]\n\n\n\n\n\nLinearAlgebra.jl 에는 스펙트럼 반경을 반환하는 함수는 없지만 eigvals 함수를 이용하여 쉽게 작성 할 수 있다.\nIn [16]: A=[4 0 1; 0 2 -1; 1 -1 3]\nOut[16]: 3×3 Matrix{Int64}:\n 4   0   1\n 0   2  -1\n 1  -1   3\n\nIn [17]: spectral_radius(M::Matrix) = eigvals(M) .|&gt; abs |&gt; maximum\nOut[17]: spectral_radius (generic function with 1 method)\n\nIn [18]: spectral_radius(A)\nOut[18]: 4.732050807568877\n\n\\(\\rho(\\boldsymbol{A})\\) 는 행렬에 대해 고유하게 정해지는 값이지만 행렬의 노름이 될 수 없다. 예를 들어\n\\[\n\\boldsymbol{A} = \\begin{bmatrix}2 & 1\\\\  2 &-2\\end{bmatrix},\\, \\boldsymbol{B} = \\begin{bmatrix} 2 & 4 \\\\ 1 &-3\\end{bmatrix}\n\\]\n의 경우 \\(\\rho(\\boldsymbol{A}) \\approx 2.45\\), \\(\\rho(\\boldsymbol{B}) \\approx 3.70\\), \\(\\rho(\\boldsymbol{A}+\\boldsymbol{B}) \\approx 6.44\\) 이므로 \\(\\rho(\\boldsymbol{A}+\\boldsymbol{B}) &gt; \\rho(\\boldsymbol{A}) + \\rho (\\boldsymbol{B})\\) 인데 이것은 행렬의 노름이 될 조건에 위배된다. 하지만 스펙트럼 반경과 노름은 아래와 같이 밀접한 관련이 있다.\n\n\n\n명제 1 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 에 대해\n  (\\(1\\)) \\(\\|\\boldsymbol{A}\\|_2 = \\left[\\rho (\\boldsymbol{A}^T\\boldsymbol{A})\\right]^{1/2}\\) 이며,\n  (\\(2\\)) \\(\\rho(\\boldsymbol{A})\\) 는 모든 자연스러운 노름에 대해 \\(\\|\\boldsymbol{A}\\|\\) 보다 작거나 같다.\n\n\n\n\n(증명). (\\(1\\)) 증명은 Isacson, Keller 의 Analysis of numerical method (1966) 을 참고하라\n(\\(2\\)) \\(\\lambda \\in \\lambda(\\boldsymbol{A})\\) 이고 \\(\\boldsymbol{x}\\in \\mathcal{M}_n(\\mathbb{F})\\) 가 \\(\\lambda\\) 에 대한 고유벡터이며 \\(\\|\\boldsymbol{x}\\|=1\\) 일 때,\n\\[\n|\\lambda| =  \\|\\lambda \\boldsymbol{x}\\| = \\|\\boldsymbol{Ax}\\| \\le \\|\\boldsymbol{A}\\| \\|\\boldsymbol{x}\\| = \\|\\boldsymbol{A}\\|\n\\]\n\n\n\n\n\n\n\n\n\n정의 2 (행렬의 수렴) 정사각 행렬 \\(\\boldsymbol{A}\\) 이 다음 조건을 만족할 때 행렬 \\(\\boldsymbol{A}\\) 가 수렴한다고 한다.\n\\[\n\\lim_{k \\to \\infty} \\boldsymbol{A}^k = \\boldsymbol{0}.\n\\]\n\n\n\n\n\n\n\n명제 2 정사각 행렬 \\(\\boldsymbol{A}\\) 에 대해 다음은 동치이다.\n  (\\(1\\)) \\(\\boldsymbol{A}\\) 는 수렴한다.\n  (\\(2\\)) 자연스러운 행렬 노름에 대해 \\(\\displaystyle \\lim_{k \\to \\infty} \\left\\|\\boldsymbol{A}^k\\right\\|=\\boldsymbol{0}\\) 이다.\n  (\\(3\\)) \\(\\rho (\\boldsymbol{A})&lt; 1\\).\n  (\\(3\\)) \\(\\displaystyle \\lim_{k\\to \\infty} \\boldsymbol{A}^k \\boldsymbol{x} = \\boldsymbol{0}\\).\n\n\n\n\n(증명). 증명은 Isacson, Keller 의 Analysis of numerical method (1966) 을 참고하라\n\n\n\n\n반복법의 수학적 기초\n선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 의 해를 반복법으로 구하는 방법중에 가장 일반적으로 사용되는 방법은 주어진 선형 방정식을 다음의 형태로 변형하는데서 시작한다.\n\\[\n\\boldsymbol{x} = \\boldsymbol{Tx}+ \\boldsymbol{c}.\n\\]\n만약 \\(\\boldsymbol{T}\\) 가 어떤 조건을 만족하면 초기값 \\(\\boldsymbol{x}^{(0)}\\) 부터 시작하여 \\[\n\\boldsymbol{x}^{(k+1)} =  \\boldsymbol{Tx}^{(k)}  + \\boldsymbol{c}\n\\]\n에서 생성되는 수열 \\(\\langle \\boldsymbol{x}^{(k)}\\rangle\\), \\(k=0,\\,1,\\ldots\\) 는 \\(\\boldsymbol{x} = \\boldsymbol{Tx}+ \\boldsymbol{c}\\) 를 만족하는 \\(\\boldsymbol{x}\\) 로 수렴한다.\n\n\n\n보조정리 1 행렬 \\(\\boldsymbol{T}\\in \\mathbb{F}^{n\\times n}\\) 의 스펙트럼 반경이 \\(1\\) 보다 작다면 \\(\\boldsymbol{I}-\\boldsymbol{T}\\) 는 가역행렬이며,\n\\[\n(\\boldsymbol{I}-\\boldsymbol{T})^{-1} = \\sum_{j=0}^{\\infty} \\boldsymbol{T}^j\n\\]\n이다.\n\n\n\n\n(증명). \\(\\boldsymbol{x}\\) 가 \\(\\boldsymbol{T}\\) 의 고유벡터이며 그 고유값이 \\(\\lambda\\) 인것과 \\(\\boldsymbol{x}\\) 가 \\(\\boldsymbol{I}-\\boldsymbol{T}\\) 의 고유벡터이며 그 고유값이 \\((1-\\lambda)\\) 인것은 동치이다. 그런데 \\(\\boldsymbol{T}\\) 의 고유값의 절대값이 \\(1\\) 보다 작기 때문에 0 은 \\((\\boldsymbol{I}-\\boldsymbol{T})\\) 의 고유값이 될 수 없으며, 따라서 \\(\\boldsymbol{I}-\\boldsymbol{T}\\) 는 가역행렬이다. 또한\n\\[\n(\\boldsymbol{I}-\\boldsymbol{T})\\left(\\sum_{j=0}^N \\boldsymbol{T}^j\\right) = \\boldsymbol{I} -\\boldsymbol{T}^{N+1}\n\\]\n이므로 명제 2 에 의해 \\((\\boldsymbol{I}-\\boldsymbol{T})^{-1} = \\displaystyle\\sum_{j=0}^\\infty \\boldsymbol{T}^j\\) 이다. \\(\\square\\)\n\n\n\n\n정리 1 임의의 \\(\\boldsymbol{x}^{(0)},\\, \\boldsymbol{c}\\in \\mathbb{F}^n\\) 과 행렬 \\(\\boldsymbol{T}\\in \\mathbb{F}^{n \\times n}\\) 에 대해\n\\[\n\\boldsymbol{x}^{(k+1)} =  \\boldsymbol{Tx}^{(k)}+ \\boldsymbol{c}\n\\]\n에 의해 생성되는 수열 \\(\\langle \\boldsymbol{x}^{(k)}\\rangle\\) 이 \\(\\boldsymbol{x}=\\boldsymbol{Tx}+\\boldsymbol{c}\\) 를 만족하는 유일한 \\(\\boldsymbol{x}\\) 로 수렴할 필요충분조건은 \\(\\rho(\\boldsymbol{T})&lt;1\\) 이다.\n\n\n\n\n(증명). \\(\\rho(\\boldsymbol{T})&lt;1\\) 임을 가정하자.\n\\[\n\\begin{aligned}\n\\boldsymbol{x}^{(k+1)} &= \\boldsymbol{Tx}^{k}+\\boldsymbol{c} \\\\\n&= \\boldsymbol{T}\\left(\\boldsymbol{Tx}^{k-1} + \\boldsymbol{c}\\right) + \\boldsymbol{c}  \\\\\n&\\qquad \\qquad \\vdots \\\\\n&=  \\boldsymbol{T}^{k+1}\\boldsymbol{x}^{(0)} + (\\boldsymbol{T}^{k}+ \\cdots + \\boldsymbol{T}+ \\boldsymbol{I})\\boldsymbol{c}\n\\end{aligned}\n\\]\n이다. 이로부터 \\(k \\to \\infty\\) 극한에서 \\(\\boldsymbol{x}^{(\\infty)} = \\left(\\boldsymbol{I}-\\boldsymbol{T}\\right)^{-1}\\boldsymbol{c}\\) 임을 안다. 따라서 \\(\\boldsymbol{x}\\) 는 수렴하며 \\(\\boldsymbol{x}^{(\\infty)} = \\boldsymbol{Tx}^{(\\infty)} +\\boldsymbol{c}\\) 를 만족한다.\n이제 \\(\\displaystyle \\lim_{k \\to \\infty}\\boldsymbol{x}^{(k)} = \\boldsymbol{x}\\) 이며 \\(\\boldsymbol{x}\\) 가 \\(\\boldsymbol{x}=\\boldsymbol{Tx}+\\boldsymbol{c}\\) 를 만족하는 유일한 해라고 하자. 즉 \\(\\boldsymbol{I}-\\boldsymbol{T}\\) 는 가역이다. 임의의 \\(\\boldsymbol{z}\\in \\mathcal{M}_n(\\mathbb{F})\\) 에 대해 \\(\\boldsymbol{x}^{(0)} = \\boldsymbol{x}-\\boldsymbol{z}\\) 라고 하자. \\(\\boldsymbol{x}^{(k+1)} = \\boldsymbol{Tx}^{(k)} + \\boldsymbol{c}\\) 이면 \\(\\langle \\boldsymbol{x}^{k}\\rangle\\) 은 \\(\\boldsymbol{x}\\) 로 수렴하며,\n\\[\n\\boldsymbol{x}-\\boldsymbol{x}^{(k+1)} = (\\boldsymbol{Tx}+\\boldsymbol{c})-(\\boldsymbol{Tx}^{k}+\\boldsymbol{c}) =  \\boldsymbol{T}(\\boldsymbol{x}-\\boldsymbol{x}^{(k)})\n\\]\n이므로,\n\\[\n\\boldsymbol{x}-\\boldsymbol{x}^{(k+1)} = \\boldsymbol{T}^{k+1}(\\boldsymbol{x}-\\boldsymbol{x}^{(0)}) = \\boldsymbol{T}^{(k+1)}\\boldsymbol{z}\n\\]\n이다. 명제 2 에 따라 임의의 \\(\\boldsymbol{z}\\) 에 대해 \\(\\displaystyle \\lim_{k \\to \\infty} \\boldsymbol{T}^{(k+1)}\\boldsymbol{z}=0\\) 이면 \\(\\rho(\\boldsymbol{T})&lt;1\\) 이다. \\(\\square\\)\n\n\n\n\n따름정리 1 임의의 자연스러운 행렬 노름에 대해 \\(\\|\\boldsymbol{T}\\|&lt;1\\) 이면 임의의 벡터 \\(\\boldsymbol{c}\\) 에 대해 \\(\\boldsymbol{x}^{(k+1)} = \\boldsymbol{Tx}^{(k)}+\\boldsymbol{c}\\) 에 의해 생성되는 수열 \\(\\langle \\boldsymbol{x}^{(k)} \\rangle\\) 은 초기값 \\(\\boldsymbol{x}^{(0)}\\) 에 무관하게 \\(\\boldsymbol{x}=\\boldsymbol{Tx} + \\boldsymbol{c}\\) 를 만족하는 \\(\\boldsymbol{x}\\) 로 수렴한다. 또한 다음을 만족한다.\n  (\\(1\\)) \\(\\| \\boldsymbol{x}-\\boldsymbol{x}^{(k)}\\| \\le \\|\\boldsymbol{T}\\|^k \\|\\boldsymbol{x}-\\boldsymbol{x}^{(0)}\\|\\),\n  (\\(2\\)) \\(\\displaystyle \\|\\boldsymbol{x}-\\boldsymbol{x}^{(k)}\\| \\le \\dfrac{\\|\\boldsymbol{T}\\|^k}{1-\\|\\boldsymbol{T}\\|} \\|\\boldsymbol{x}^{(1)}-\\boldsymbol{x}^{(0)}\\|\\).\n\n\n\n\n(증명). \\(\\langle \\boldsymbol{x}^{(k)} \\rangle\\) 의 수렴은 정리 1 로 부터 알 수 있다. 명제 1 에 의해 \\(\\rho(\\boldsymbol{T}) \\le \\|\\boldsymbol{T}\\|&lt;1\\) 이다. 따라서\n\\[\n\\|\\boldsymbol{x}-\\boldsymbol{x}^{(k)}\\| \\le \\|\\boldsymbol{T}\\| \\cdot \\|\\boldsymbol{x}-\\boldsymbol{x}^{(k-1)}\\| \\le  \\cdots \\le \\|\\boldsymbol{T}\\|^{k} \\cdot \\|\\boldsymbol{x}-\\boldsymbol{x}^{(0)}\\|\n\\]\n이다.\n또한\n\\[\n\\begin{aligned}\n\\|\\boldsymbol{x}^{(k+1)}-\\boldsymbol{x}^{(k)}\\| & = \\|\\boldsymbol{Tx}^{(k)}- \\boldsymbol{Tx}^{(k-1)}\\|\\le \\|\\boldsymbol{T}\\|\\cdot \\| \\boldsymbol{x}^{(k)}-\\boldsymbol{x}^{(k-1)} \\| \\\\\n& \\le \\cdots \\le \\|\\boldsymbol{T}\\|^k \\cdot \\|\\boldsymbol{x}^{(1)}-\\boldsymbol{x}^{(0)}\\|\n\\end{aligned}\n\\]\n이므로 양의 정수 \\(m\\) 에 대해\n\\[\n\\begin{aligned}\n\\|\\boldsymbol{x}^{(k+m)} -\\boldsymbol{x}^{(k)}\\| & = \\| \\boldsymbol{x}^{(k+m)} - \\boldsymbol{x}^{(k+m-1)} + \\boldsymbol{x}^{(k+m-1)} - \\cdots + \\boldsymbol{x}^{(k+1)}- \\boldsymbol{x}^{(k)} \\| \\\\\n&\\le  \\| \\boldsymbol{x}^{(k+m)} - \\boldsymbol{x}^{(k+m-1)} \\| + \\cdots +  \\| \\boldsymbol{x}^{(k+1)} - \\boldsymbol{x}^{(k)}\\| \\\\\n&\\le \\|\\boldsymbol{T}\\|^{k+m-1} \\cdot \\|\\boldsymbol{x}^{(1)} - \\boldsymbol{x}^{(0)}\\| + \\cdots +\\|\\boldsymbol{T}\\|^k \\|\\boldsymbol{x}^{(1)} - \\boldsymbol{x}^{(0)}\\|  \\\\\n& = \\|\\boldsymbol{T}\\|^k \\left(1 + \\|\\boldsymbol{T}\\| + \\cdots +\\|\\boldsymbol{T}\\|^{m-1}\\right) \\|\\boldsymbol{x}^{(1)} - \\boldsymbol{x}^{(0)}\\| \\\\\n\\end{aligned}\n\\]\n이므로 \\(m \\to \\infty\\) 극한에서,\n\\[\n\\|\\boldsymbol{x}-\\boldsymbol{x}^{(k)} \\| \\le \\dfrac{\\|\\boldsymbol{T}\\|^{k}}{1-\\|\\boldsymbol{T}\\|} \\|\\boldsymbol{x}^{(1)}-\\boldsymbol{x}^{(0)}\\|\n\\]\n이다. \\(\\square\\)\n\n\n위의 따름정리는 매우 중요한데, 반복법으로 구한 해와 실제 해의 차이에 대한 상한을 정해주기 때문이다.\n\n\n\n\n\n\n\n\n정의 3 (오차벡터) \\(\\boldsymbol{A} \\in \\mathbb{F}^{n \\times n}\\) 를 이용한 선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 이 주어졌을 때 \\(\\tilde{\\boldsymbol{x}}\\in \\mathcal{M}_{n}(\\mathbb{F})\\) 에 대한 오차벡터(residual vector) \\(\\boldsymbol{r}\\) 는 다음과 같이 정의된다.\n\\[\n\\boldsymbol{r} : = \\boldsymbol{b} - \\boldsymbol{A}\\tilde{\\boldsymbol{x}}\n\\tag{2}\\]\n\n\n\n\n반복법에서 \\(k\\) 번째 임시해 \\(\\boldsymbol{x}^{(k)}\\) 에 대한 residual vector 를 \\(\\boldsymbol{r}^{(k)}\\) 라고 하면\n\\[\n\\left\\|\\boldsymbol{r}^{(k)}\\right\\| = \\left\\|\\boldsymbol{b}-\\boldsymbol{Ax}^{(k)}\\right\\|\n\\]\n이다. 즉 반복법에 의해 해를 구할 수 있다는 것은\n\\[\n\\lim_{k \\to \\infty} \\boldsymbol{r}^{(k)} = \\boldsymbol{0}\n\\]\n이라는 의미이다.",
    "crumbs": [
      "수치해석 II",
      "선형방정식과 반복법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#야코비-방법과-가우스-자이델-방법",
    "href": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#야코비-방법과-가우스-자이델-방법",
    "title": "선형방정식과 반복법",
    "section": "2 야코비 방법과 가우스-자이델 방법",
    "text": "2 야코비 방법과 가우스-자이델 방법\n앞서 말했듯이 일반적으로 반복법에 의해 선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 푸는 것은, 이 선형방정식을 변형하여\n\\[\n\\boldsymbol{x} = \\boldsymbol{Tx}+\\boldsymbol{c}\n\\]\n의 꼴로 만드는 데서 시작한다. \\(k\\) 번째 반복에 의해 얻은 \\(\\boldsymbol{x}\\) 를 \\(\\boldsymbol{x}^{(k)}\\) 라고 할 때 \\(\\boldsymbol{x}^{(k+1)} = \\boldsymbol{Tx}^{(k)} +\\boldsymbol{c}\\) 를 얻으며 \\(\\boldsymbol{x}^{(k+1)}\\) 과 \\(\\boldsymbol{x}^{(k)}\\) 의 차가 어느 기준 이하일 때 답을 얻은것으로 한다. 보통은 어떤 정해진 노름과 값 \\(\\varepsilon&gt;0\\) 에 대해 \\(\\|\\boldsymbol{x}^{(k+1)}-\\boldsymbol{x}^{(k)}\\|&lt;\\varepsilon\\) 혹은 \\(\\dfrac{\\|\\boldsymbol{x}^{(k+1)}-\\boldsymbol{x}^{(k)}\\|}{\\|\\boldsymbol{x}^{(k+1)}\\|} &lt; \\varepsilon\\) 일 때이며 이때의 기준값을 tolerence 라고 한다. Tolerance 를 계산할때는 보통 \\(L_\\infty\\) 노름을 사용한다.\n\n\n야코비 방법\n\\(\\boldsymbol{A}\\in \\mathbb{R}^{n \\times n}\\) 에 대해 \\(\\boldsymbol{D}\\) 를 \\(\\boldsymbol{A}\\) 의 대각행렬, \\(\\boldsymbol{L}\\) 과 \\(\\boldsymbol{U}\\) 를 각각 \\(\\boldsymbol{A}\\) 의 하삼각 행렬부분과 상삼각 행렬에서 대각성분을 \\(0\\) 으로 만든 행렬이라고 하자. 즉 \\(\\boldsymbol{A}=\\boldsymbol{D}+\\boldsymbol{L}+\\boldsymbol{U}\\) 이며, \\(\\boldsymbol{L}\\) 과 \\(\\boldsymbol{U}\\) 는 대각성분이 모두 \\(0\\) 인 하삼각행렬과 상삼각행렬이다. 이 때 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 는 \\((\\boldsymbol{D} + \\boldsymbol{L} +\\boldsymbol{U})\\boldsymbol{x}=\\boldsymbol{b}\\) 가 되며,\n\\[\n\\boldsymbol{Dx} = -(\\boldsymbol{L}+\\boldsymbol{U})\\boldsymbol{x}+\\boldsymbol{b}\n\\]\n이다. 모든 대각 성분이 \\(0\\) 이 아닌 대각행렬 \\(\\boldsymbol{D}\\) 는 가역행렬이므로,\n\\[\n\\boldsymbol{x} = -\\boldsymbol{D}^{-1}(\\boldsymbol{L}+\\boldsymbol{U})\\boldsymbol{x} + \\boldsymbol{D}^{-1}\\boldsymbol{b}\n\\]\n이다. 여기에 임의로 제시하는 초기벡터 \\(\\boldsymbol{x}^{(0)}\\) 가 주어지면,\n\\[\n\\boldsymbol{x}^{(k+1)} =   -\\boldsymbol{D}^{-1}(\\boldsymbol{L}+\\boldsymbol{U})\\boldsymbol{x}^{(k)} + \\boldsymbol{D}^{-1}\\boldsymbol{b}\n\\]\n를 통해 반복한다. 이 때, \\((\\boldsymbol{D}^{-1})_{ij} = \\delta_{ij}/A_{ij}\\) 이므로\n\\[\nx_{i}^{(k+1)} = \\sum_{j=1,\\, j\\ne i}^n \\left(-\\dfrac{A_{ij}}{A_{ii}}\\right)x^{(k)}_j + \\dfrac{b_i}{A_{ii}} =\\dfrac{1}{A_{ii}} \\left[ \\left( \\sum_{j=1,j\\ne i}^n -A_{ij} x_j^{(k)}\\right) - b_i\\right]\n\\]\n이다. 야코비 방법은 대각성분이 모두 \\(0\\) 이 아닌 경우에 사용 할 수 있으며 대각성분이 다른 성분보다 클 경우 빨리 수렴한다.\nJulia 코드는 다음과 같다.\nfunction iteration_jacobi(\n    A::AbstractMatrix, \n    b::Vector, \n    x0::Vector; \n    etol::Number = 1.0e-10,\n    Maxiter::Integer = 100_000)\n    n = size(A)[1]\n    @assert n == size(A)[2] == size(b)[1]\n    @assert Maxiter &gt; 3\n    x = zero(x0)\n    \n    for niter in 1:Maxiter\n        for i in 1:length(x0)\n            for j in 1:length(x0)\n                if i ≠ j\n                    x[i] += -A[i,j] * x0[j]\n                end\n            end\n            \n            x[i] = (x[i]+b[i])/A[i, i] \n        end\n        if norm(x .- x0, Inf) / norm(x, Inf) &lt; etol\n            break\n        else \n            x0 = x[:]\n            x = zero(x0)\n        end\n    end\n    return x\nend\n\n\\(\\boldsymbol{A} = \\left[ \\,\\begin{array}{rr} 10 & -1 & 2 & 0\\\\ -1 & 11 & -1& 3\\\\ 2 & -1 & 10 &-1 \\\\ 0 & 3 & -1 & 8 \\end{array}\\,\\right]\\) 와 \\(\\boldsymbol{b}= \\left[ \\begin{array}{rr}6 \\\\25 \\\\ -11 \\\\ 15 \\end{array}\\right]\\) 에 대해 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 의 해는 \\(\\boldsymbol{x} = \\left[\\, \\begin{array}{rr} 1 \\\\ 2 \\\\ -1 \\\\ 2 \\end{array}\\,\\right]\\) 이다.\nA = [10.0 -1.0 2.0 0.0; -1 11 -1 3; 2 -1 10 -1; 0 3 -1 8]\nb = [6, 25, -11, 15]\nx0 = [1, 1, -1, 1]\n\nx=iteration_jacobi(A, b, x0, etol=1.0e-5)\n를 이용하면 다음과 같은 결과를 얻는다.\n4-element Vector{Float64}:\n  0.9999964637124269\n  2.0000051877531875\n -1.0000041868849727\n  1.000006667932778\n\n\n\n가우스-자이델 방법\n\\(\\boldsymbol{A}\\in \\mathbb{R}^{n \\times n}\\) 에 대한 선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 에서 \\(\\boldsymbol{D}\\) 를 \\(\\boldsymbol{A}\\) 의 대각행렬, \\(\\boldsymbol{L}\\) 과 \\(\\boldsymbol{U}\\) 를 각각 \\(\\boldsymbol{A}\\) 의 하삼각 행렬부분과 상삼각 행렬에서 대각성분을 \\(0\\) 으로 만든 행렬이라고 하자. 선형 방정식을 다음과 같이 변화시킬 수 있다.\n\\[\n\\boldsymbol{x} = (\\boldsymbol{D}+\\boldsymbol{L})^{-1}(\\boldsymbol{b}-\\boldsymbol{Ux})\n\\]\n이다. 반복법은\n\\[\n\\boldsymbol{x}^{(k+1)} =  (\\boldsymbol{D}+\\boldsymbol{L})^{-1}(\\boldsymbol{b}-\\boldsymbol{Ux}^{(k)})\n\\]\n를 사용하여 반복법으로 선형방정식을 푼다고 하자. 그렇다면,\n\\[\n(\\boldsymbol{D}+\\boldsymbol{L})\\boldsymbol{x}^{(k+1)} = \\boldsymbol{b}-\\boldsymbol{Ux}^{(k)}\n\\]\n이제 \\(\\boldsymbol{D}+\\boldsymbol{L}\\) 은 하삼각행렬, \\(\\boldsymbol{U}\\) 는 대각성분이 \\(0\\) 인 상삼각행렬임을 이용하여 위 식의 \\(i\\) 번째 행에 대한 식을 보면\n\\[\n\\sum_{j=1}^{i-1} A_{ij}x^{(k+1)}_j + A_{ii}x^{(k)}_i = b_i - \\sum_{j=i+1}^n A_{ij}x^{(k)}_j\n\\]\n를 얻는다. 즉,\n\\[\nx_{i}^{(k+1)} = \\dfrac{1}{A_{ii}}\\left[b_i - \\sum_{j=1}^{i-1} A_{ij} x^{(k+1)}_j - \\sum_{j=i+1}^n A_{ij} x^{(k)}\\right]\n\\]\n이다. 여기서 일단 어색해 보이는 것은 좌변과 우변에 각각 \\(\\boldsymbol{x}^{(k+1)}\\) 의 성분이 들어가 있는, 그러니까 \\(\\boldsymbol{x}^{(k+1)}\\) 을 계산하기 전에 사용하는 듯한 부분이다. 그러나 \\(x^{(k+1)}_1\\) 을 계산하는 데는 \\(\\boldsymbol{x}^{(k+1)}\\) 의 성분이 사용되지 않으며 \\(x_{i}^{(k+1)}\\) 을 계산하는 데는 \\(j&lt;i\\) 에 대한 \\(x_j^{(k+1)}\\) 만을 사용한다. 즉 \\(\\boldsymbol{x}^{(k+1)}\\) 의 각각의 성분에 대해서는 계산하기 전에 사용하는 값은 없다.\nusing LinearAlgebra\nfunction iteration_seidel(\n    A::AbstractMatrix, \n    b::Vector, \n    x0::Vector; \n    etol::Number = 1.0e-10, \n    Maxiter = 100_000)\n    @assert size(A)[1] == size(A)[2] == size(b)[1]\n    \n    x = zero(x0)\n    for niter in 1:Maxiter\n        @inbounds for i in 1:length(x0)\n            @inbounds for j in 1:length(x0)\n                if j &lt; i\n                    x[i] += - A[i, j]*x[j]\n                elseif j&gt;i\n                    x[i] += - A[i, j]*x0[j] \n                end\n            end\n            @inbounds x[i] = (x[i]+b[i])/A[i, i] \n        end\n        if norm(x .- x0, Inf) / norm(x, Inf) &lt; etol\n            break\n        else \n            x0 = x[:]\n            x = zero(x0)\n\n        end\n    end\n    return x\nend\n야코비 방법과 가우스-지델 방법 모두 \\(\\boldsymbol{x}^{(k+1)} = \\boldsymbol{Tx}^{(k)} + \\boldsymbol{c}\\) 형태의 반복법을 사용한다. 우리는 정리 1 로 부터 \\(\\rho(\\boldsymbol{T})&lt;1\\) 일 경우에만 이 반복법에 의해 답을 얻을 수 있다는 것을 안다. 일반적으로 가우스-지델 방법이 야코비 방법보다 빠르게 원하는 정확도의 답을 얻지만 가우스-지델 방법이 이 조건을 충족하지 못하고 야코비 방법은 충족하는 경우 야코비 방법을 사용 할 수 있다. 물론 거꾸로 야코비 방법이 이 조건을 충족하지 못하고 가우스-지델 방법이 충족할 수 도 있다.\n\n\n\n예제 1 \\(\\boldsymbol{A} = \\left[\\, \\begin{array}{rr} 2 & -1 & 1 \\\\ 2 & 2 & 2 \\\\ -1 & -1 & 2\\end{array}\\,\\right]\\) 의 경우 \\(\\rho(\\boldsymbol{D}^{-1}(\\boldsymbol{L}+ \\boldsymbol{U}))=\\dfrac{\\sqrt{5}}{2}&gt;1\\) 이므로 야코비 방법으로는 답을 얻을 수 없지만. \\(\\rho((\\boldsymbol{D}-\\boldsymbol{L})^{-1}\\boldsymbol{U}) = \\dfrac{1}{2}&lt;1\\) 이므로 가우스-지델 방법은 사용할 수 있다. 반대로 \\(\\boldsymbol{A} = \\left[\\, \\begin{array}{rr} 1 & 2 & -2 \\\\ 1 & 1 & 1 \\\\ 2 & 2 & 1\\end{array}\\,\\right]\\) 의 경우 \\(\\rho(\\boldsymbol{D}^{-1}(\\boldsymbol{L} + \\boldsymbol{U})) = 0&lt;1\\) 이므로 야코비 방법을 사용 할 수 있지만 \\(\\rho((\\boldsymbol{D}-\\boldsymbol{L})^{-1}\\boldsymbol{U}) = 2&gt;1\\) 이므로 가우스-지델 방법은 사용 할 수 없다.",
    "crumbs": [
      "수치해석 II",
      "선형방정식과 반복법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#successive-over-relaxation-sor",
    "href": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#successive-over-relaxation-sor",
    "title": "선형방정식과 반복법",
    "section": "3 Successive Over-Relaxation (SOR)",
    "text": "3 Successive Over-Relaxation (SOR)\n선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 반복법을 이용하여 풀기 위해 \\(\\boldsymbol{x}^{(k+1)}=\\boldsymbol{Tx}^{(k)}+ \\boldsymbol{c}\\) 로 변환하였다고 하자. 우리는 \\(\\boldsymbol{T}\\) 의 스펙트럼 반경 \\(\\rho(\\boldsymbol{T})\\) 값이 작을수록 \\(\\boldsymbol{x}^{(k)}\\) 가 빨리 수렴한다는 것을 알고 있다.(explicit proof will be presented)\n\\(\\boldsymbol{D}\\) \\(\\boldsymbol{L}\\), \\(\\boldsymbol{U}\\) 를 각각 \\(\\boldsymbol{A}\\) 의 대각성분, 대각성분을 제외한 하삼각행렬, 대각성분을 제외한 상삼각행렬이라고 하자. 그렇다면 \\(\\boldsymbol{A} = \\boldsymbol{D}+\\boldsymbol{L}+\\boldsymbol{U}\\) 이다. 선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 는 임의의 \\(\\omega\\in \\mathbb{R}\\) 에 대해 아래와 같이 변형된다.\n\\[\n(\\boldsymbol{D}+\\omega \\boldsymbol{L})\\boldsymbol{x} = \\left[(1-\\omega) \\boldsymbol{D}- \\omega\\boldsymbol{U}\\right]\\boldsymbol{x} + \\omega \\boldsymbol{b}\n\\tag{3}\\]\n\\(SOR\\) 은 적절한 \\(\\omega\\) 를 선택하여 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 의 해를 구하는 것이다. 이제 아래의 반복법을 사용한다.\n\\[\n\\boldsymbol{x}^{(k+1)} = (\\boldsymbol{D}+\\omega \\boldsymbol{L})^{-1}\\left[(1-\\omega) \\boldsymbol{D}- \\omega\\boldsymbol{U}\\right]\\boldsymbol{x}^{(k)} + \\omega (\\boldsymbol{D}+\\omega \\boldsymbol{L})^{-1} \\boldsymbol{b}\n\\]\n가우스-자이델 방법에서와 같이\n\\[\n(\\boldsymbol{D}+\\omega \\boldsymbol{L})\\boldsymbol{x}^{(k+1)} = \\left[(1-\\omega) \\boldsymbol{D}- \\omega\\boldsymbol{U}\\right]\\boldsymbol{x}^{(k)} + \\omega \\boldsymbol{b}\n\\]\n의 \\(i\\) 번째 행을 비교하면,\n\\[\n\\sum_{j=1}^{i-1} \\omega A_{ij}x^{(k+1)}_j + A_{ii}x^{(k+1)}_i = (1-\\omega)A_{ii}x^{(k)}_i - \\sum_{j=i+1}^n \\omega A_{ij}x^{(k)}_j + \\omega b_i\n\\]\n이며(가우스-자이델 방법에서의 유도과 동일하다), 따라서\n\\[\nx^{(k+1)}_i = (1-\\omega) x^{(k)}_i + \\dfrac{1}{A_{ii}}\\left[ \\omega b_i - \\sum_{j=1}^{i-1} \\omega A_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^n \\omega A_{ij}x^{(k)}_j \\right]\n\\]\n이다.\nfunction iteration_sor(\n    A::AbstractMatrix, \n    b::Vector, \n    x0::Vector,\n    ω::Real; \n    etol::Number = 1.0e-10, \n    Maxiter = 100_000)\n\n    @assert 0 &lt; ω &lt; 2\n    \n    x = zero(x0)\n    for nitter in 1:Maxiter\n        @inbounds for i ∈ 1:length(x0)\n            @inbounds for j ∈ 1:length(x0)\n                if j &lt; i\n                    x[i] += -A[i, j] *x[j]\n                elseif j &gt; i\n                    x[i] += -A[i, j] *x0[j]\n                end\n            end\n            x[i] = (1-ω)*x0[i] + 1/A[i, i] * (ω*b[i] + ω *  x[i])\n        end\n        if norm(x .- x0, Inf)/norm(x, Inf)&lt; etol    \n            return x\n        else \n            x0 = x[:]\n            x = zero(x)\n        end\n    end\n    return x\nend\n\n다음의 결과를 증명 없이 사용하겠다.\n\n\n\n정리 2 (Kahan 의 정리) 모든 대각 성분이 \\(0\\) 이 아닌 정사각 행렬 \\(\\boldsymbol{A}\\) 에 대해 \\(\\boldsymbol{T}_\\omega\\) 를 다음과 같이 정의하자. \\(\\boldsymbol{D}\\) \\(\\boldsymbol{L}\\), \\(\\boldsymbol{U}\\) 는 각각 \\(\\boldsymbol{A}\\) 의 대각성분, 대각성분을 제외한 하삼각행렬, 대각성분을 제외한 상삼각행렬이라고 하자.\n\\[\n\\boldsymbol{T}_\\omega = (\\boldsymbol{D}-\\omega{L})^{-1}\\left[(1+\\omega) \\boldsymbol{D}- \\omega\\boldsymbol{U}\\right].\n\\]\n이 때\n\\[\n\\rho (\\boldsymbol{T}_\\omega) \\ge |\\omega -1|\n\\]\n이 성립한다.\n\n\n\n\n즉 \\(0 &lt;\\omega &lt;2\\) 는 해를 구할 수 있는 필요조건이다.\n\n\n\n정리 3 (Ostrowski-Reich 정리) \\(\\boldsymbol{A}\\in \\mathbb{R}^n\\) 이 positive definite 이고 \\(0&lt;\\omega &lt;2\\) 이면 SOR 에서의 \\(\\boldsymbol{x}^{(k)}\\) 는 수렴한다.\n\n\n\n\n\n정리 4 야코비 방법에서의 \\(\\boldsymbol{T}_J = -\\boldsymbol{D}^{-1}(\\boldsymbol{L} +\\boldsymbol{U})\\) 와 가우스-자이델 방법에서의 \\(\\boldsymbol{T}_G = -(\\boldsymbol{D}+\\boldsymbol{L})^{-1}\\boldsymbol{U}\\) 에 대해 \\(\\boldsymbol{A}\\) 가 positive definite 이며 삼중대각행렬이라면 \\(\\rho(\\boldsymbol{T}_G) = \\left[\\boldsymbol{T}_J\\right]^2  &lt; 1\\) 이며, SOR 에서의 최적의 \\(\\omega\\) 는\n\\[\n\\omega = \\dfrac{1}{1+\\sqrt{1-[\\rho(\\boldsymbol{T}_G)]}}\n\\]\n이다. 이 경우 \\(\\rho(\\boldsymbol{T}_\\omega) = \\omega -1\\) 이다.",
    "crumbs": [
      "수치해석 II",
      "선형방정식과 반복법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#오차벡터와-조건수",
    "href": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#오차벡터와-조건수",
    "title": "선형방정식과 반복법",
    "section": "4 오차벡터와 조건수",
    "text": "4 오차벡터와 조건수\n선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 에 대해 우리가 임시로 구한 해를 \\(\\overline{\\boldsymbol{x}}\\) 라고 할 때 \\(\\boldsymbol{r} = \\boldsymbol{b}-\\boldsymbol{A}\\overline{\\boldsymbol{x}}\\) 는 오차벡터(residual vector) 이다(정의 3). 우리는 앞서 선형방정식의 오차의 척도로 \\(\\|\\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\|\\) 나 \\(\\|\\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\|/\\|\\boldsymbol{x}\\|\\) 를 사용하였다. 정해진 행렬 \\(\\boldsymbol{A}\\) 의 노름에 대해 \\(\\|\\boldsymbol{r}\\|\\) 이 작다면 \\(\\|\\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\|\\) 혹은 \\(\\|\\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\|/\\|\\boldsymbol{x}\\|\\) 가 작을 것이라고 예상 할 수 있지만 항상 그런것은 아니다.\n\n\n\n명제 3 가역행렬 \\(\\boldsymbol{A}\\) 에 대한 선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 의 해 \\(\\boldsymbol{x}\\in \\mathbb{R}^n\\) 과 어떤 \\(\\overline{\\boldsymbol{x}} \\in \\mathbb{R}^n\\), 그리고 오차 벡터 \\(\\boldsymbol{r}= \\boldsymbol{b}-\\boldsymbol{A}\\overline{\\boldsymbol{x}}\\) 과 벡터공간에서의 자연스러운 노름 \\(\\|\\cdot \\|\\) 에 대해 다음이 성립한다.\n\\[\n\\|\\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\| \\le \\|\\boldsymbol{r}\\| \\cdot \\left\\| \\boldsymbol{A}^{-1}\\right\\|.\n\\]\n또한 \\(\\boldsymbol{x}\\ne 0,\\, \\boldsymbol{b}\\ne 0\\) 일 때 다음이 성립한다.\n\\[\n\\dfrac{\\|\\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\|}{\\|\\boldsymbol{x}\\|} \\le \\|\\boldsymbol{A}\\| \\cdot \\|\\boldsymbol{A}^{-1}\\| \\dfrac{\\|\\boldsymbol{r}\\|}{\\|\\boldsymbol{b}\\|}.\n\\]\n\n\n\n\n(증명). \\(\\boldsymbol{A}\\) 가 가역이며 행렬의 노름의 정의에 의해 \\[\n\\|\\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\| = \\|\\boldsymbol{A}^{-1}\\boldsymbol{r}\\| \\le \\|\\boldsymbol{A}^{-1}\\| \\cdot \\|\\boldsymbol{r}\\|\n\\]\n이다. \\(\\boldsymbol{b}=\\boldsymbol{Ax}\\) 이므로 \\(\\|\\boldsymbol{b}\\| \\le \\|\\boldsymbol{A}\\| \\cdot \\|\\boldsymbol{x}\\|\\) 이다. 이로부터 \\(1/\\|\\boldsymbol{x}\\|\\le \\|\\boldsymbol{A}\\| /\\|\\boldsymbol{b}\\|\\) 이므로, \\[\n\\dfrac{\\|\\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\|}{\\|\\boldsymbol{x}\\|} \\le \\|\\boldsymbol{A}\\| \\cdot \\|\\boldsymbol{A}^{-1}\\| \\dfrac{\\|\\boldsymbol{r}\\|}{\\|\\boldsymbol{b}\\|}.\n\\] 이다. \\(\\square\\)\n\n\n우리는 앞서 행렬의 조건수 에서 정사각 행렬 \\(\\boldsymbol{A}\\) 의 조건수 \\(\\kappa_\\boldsymbol{A} := \\|\\boldsymbol{A}\\|\\|\\boldsymbol{A}^{-1}\\|\\) 에 대해 알아보았다. 즉 \\(\\boldsymbol{r}\\|\\) 이 작더라도 \\(\\kappa_{\\boldsymbol{A}}\\) 가 크다면 절대오차 혹은 상대오차가 클 수 있다는 것을 알 수 있다. 좋은 조건과 나쁜 조건도 여기에 사용 할 수 있다. \\(\\kappa(\\boldsymbol{A})\\approx 1\\) 이면 오차벡터의 작은 차이가 \\(\\|\\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\|\\) 의 작은 차이를 의미하기 때문에 \\(\\boldsymbol{A}\\) 를 좋은 조건(well-conditioned) 이라고 한다. 반대로 \\(\\kappa(\\boldsymbol{A}) \\gg 1\\) 일 경우는 나쁜 조건 (ill-conditioned) 라고 한다.",
    "crumbs": [
      "수치해석 II",
      "선형방정식과 반복법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#the-conjugate-gradient-방법",
    "href": "src/numerical_analysis_using_julia/09_iterative_method_for_linear_system.html#the-conjugate-gradient-방법",
    "title": "선형방정식과 반복법",
    "section": "5 The Conjugate gradient 방법",
    "text": "5 The Conjugate gradient 방법\n\\(\\boldsymbol{A}\\) 가 positive definite 이며 내적이 유클리드 내적, 즉 \\(\\langle \\boldsymbol{x},\\,\\boldsymbol{y} \\rangle = \\boldsymbol{y}^T\\boldsymbol{x}\\) 라고 하자. \\(\\boldsymbol{A}\\) 가 positive definite 이므로 \\(\\langle \\boldsymbol{Ax},\\,\\boldsymbol{x} \\rangle = \\boldsymbol{x}^T\\boldsymbol{Ax} &gt; 0\\) 이다. 이때 우리는 다음을 보일 수 있다.\n\n\n\n명제 4 \\(\\boldsymbol{A}\\in \\mathbb{R}^n\\) 가 positive definite 이고 \\(\\boldsymbol{x}_0 \\in \\mathbb{R}^n\\) 일 때 다음은 동치이다.\n  (\\(1\\)) \\(\\boldsymbol{Ax}_0=\\boldsymbol{b}\\),\n  (\\(2\\)) \\(g(\\boldsymbol{x})= \\langle \\boldsymbol{Ax},\\,\\boldsymbol{x}\\rangle -2 \\langle \\boldsymbol{b}, \\boldsymbol{x}\\rangle\\) 의 최소값은 \\(g(\\boldsymbol{x}_0)\\) 이다.\n\n\n\n\n(증명). 우선 \\(t\\in \\mathbb{R},\\, \\boldsymbol{v}\\in \\mathbb{R}^n\\) 에 대해,\n\\[\n\\begin{aligned}\ng(\\boldsymbol{x}+t\\boldsymbol{v}) &= \\langle \\boldsymbol{A}(\\boldsymbol{x}+t\\boldsymbol{v}), \\, \\boldsymbol{x}+t\\boldsymbol{v} \\rangle - 2\\langle \\boldsymbol{b},\\, \\boldsymbol{x}+t\\boldsymbol{v}\\rangle  \\\\\n&= t^2 \\langle \\boldsymbol{Av}, \\boldsymbol{v}\\rangle - 2t \\langle \\boldsymbol{b}-\\boldsymbol{Ax}, \\, \\boldsymbol{v}\\rangle + g(\\boldsymbol{x}) \\\\\n&= \\langle \\boldsymbol{Av},\\,\\boldsymbol{v} \\rangle \\left[t-\\dfrac{\\langle \\boldsymbol{b}-\\boldsymbol{Ax}, \\, \\boldsymbol{v}\\rangle }{\\langle \\boldsymbol{Av},\\, \\boldsymbol{v}\\rangle}\\right]^2 + g(\\boldsymbol{x}) - \\dfrac{\\langle \\boldsymbol{b}-\\boldsymbol{Ax}, \\, \\boldsymbol{v}\\rangle^2 }{\\langle \\boldsymbol{Av},\\, \\boldsymbol{v}\\rangle}\n\\end{aligned}\n\\tag{4}\\]\n이다. \\(\\boldsymbol{A}\\) 가 positive definite 이므로 \\(\\langle \\boldsymbol{Av}, \\,\\boldsymbol{v}\\rangle &gt; 0\\) 이다.\n\\(\\boldsymbol{Ax_0}=\\boldsymbol{b}\\) 이면 \\(g(\\boldsymbol{x_0}+t\\boldsymbol{v}) \\ge g(\\boldsymbol{x_0})\\) 이므로 \\(g(\\boldsymbol{x})\\) 의 최소값이다. 역으로 \\(g(\\boldsymbol{x})\\) 를 최소로 하는 \\(\\boldsymbol{x}\\) 는 \\(\\boldsymbol{b}= \\boldsymbol{Ax}_0\\) 를 만족하는 \\(\\boldsymbol{x}_0\\) 임을 알 수 있다. \\(\\square\\)\n\n\n이제 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 라는 선형 시스템을 푸는 문제는 \\(g(\\boldsymbol{x})= \\langle \\boldsymbol{Ax},\\,\\boldsymbol{x}\\rangle -2 \\langle \\boldsymbol{b}, \\boldsymbol{x}\\rangle\\) 를 최소로 하는 \\(\\boldsymbol{x}_0\\) 를 구하는 문제와 동일한 문제가 된다. 주어진 \\(\\boldsymbol{x}^{(k)}\\) 를 생각하자. \\(g(\\boldsymbol{x}^{(k)} + t_k \\boldsymbol{v}^{(k)}) \\le g(\\boldsymbol{x}^{(k)})\\) 가 되도록 \\(t_k\\) 와 \\(\\boldsymbol{v}^{(k)}\\) 를 정할 수 있다면 우리는 점차 \\(g(\\boldsymbol{x})\\) 의 최소값에 다가갈 것이며, 결국은 \\(g(\\boldsymbol{x})\\) 를 최소로 하는 \\(\\boldsymbol{x}_0\\) 를 얻게 되고 이것은 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 의 해를 구하는 것이 된다.\n\n우선 \\(t_k\\) 와 \\(\\boldsymbol{v}^{(k)}\\) 를 결정하는 방법은 두가지가 있다\n\nSteepest descent\n우리는 다변수를 다루는 미적분학 혹은 해석학에서 \\(g :\\mathbb{R}^n \\to \\mathbb{R}\\) 가 \\(C^1\\) 급 함수일 때 \\(g(\\boldsymbol{x})\\) 를 증가시키는 방향이 \\(\\nabla g = (\\partial_1 g,\\ldots,\\, \\partial_n g)\\) 임을 배웠다. \\(\\boldsymbol{x} = \\begin{bmatrix} x_1 & \\cdots & x_n\\end{bmatrix}^T\\) 에 대해\n\\[\ng(\\boldsymbol{x}) = \\sum_{i, j=1}^n A_{ij}x_i x_j - 2 \\sum_{i=1}^n b_i x_i\n\\]\n이므로,\n\\[\n\\partial_i g(\\boldsymbol{x}) = 2(\\boldsymbol{Ax})_i -2b_i\n\\]\n이다. 즉,\n\\[\n\\nabla g(\\boldsymbol{x}) = 2 \\boldsymbol{Ax}-2\\boldsymbol{b}\n\\]\n이다. 우리는 \\(g(\\boldsymbol{x})\\) 를 최소로 하고자 하므로 \\(\\boldsymbol{v}^{(k)}\\) 를 \\(-\\nabla g(\\boldsymbol{x})\\) 방향으로 잡아야 한다. 즉 \\(\\boldsymbol{v}^{(k)}= \\boldsymbol{b}-\\boldsymbol{Ax}^{(k)}\\) 으로 잡을 수 있다.\n식 4 에 의해 정해진 \\(\\boldsymbol{x}^{(k)}\\) 와 \\(\\boldsymbol{v}^{(k)}\\) 로 \\(g(\\boldsymbol{x}^{(k)}+t\\boldsymbol{v}^{(k)})\\) 를 최소화 하는 \\(t\\) 는\n\\[\nt_k = \\dfrac{\\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(k)}, \\, \\boldsymbol{v}^{(k)}\\rangle }{\\langle \\boldsymbol{Av}^{(k)},\\, \\boldsymbol{v}^{(k)}\\rangle}\n\\]\n이다. 이것을 Julia 코드로 구현한 것은 아래와 같다. 위의 방법과 다른 것은 반복이 멈추는 것을 \\(\\|\\boldsymbol{b}-\\boldsymbol{Ax}^{(k)}\\|_\\infty\\) 가 etol 이라는 함수 인자보다 작을 때로 하였다는 것이다.\nfunction steepest_iteration(\n    A::AbstractMatrix, \n    b::Vector, \n    x0::Vector;\n    etol::Number = 1.0e-5, \n    Maxiter = 100_000)\n\n    x = similar(x0)\n    for i in 1:Maxiter\n        v = b - A*x0\n        t = dot(v,(b-A*x0))/dot(v, (A*v))\n        x = x0 + t*v\n        if norm(A*x-b, Inf)&lt;etol\n            nitter = i\n            return x\n        else \n            x0 = x\n        end\n    end\n    return nothing\nend\n\n그러나 이 방법은 비선형 시스템이나 최적화 문제에는 많이 사용되지만 선형 시스템에는 많이 사용되지 않는데 이는 수렴속도가 느리기 때문이다.\n\n\n\n\\(\\boldsymbol{A}\\)-직교 조건\n영벡터가 아닌 벡터의 집합 \\(\\{\\boldsymbol{v}^{(1)}, \\ldots, \\boldsymbol{v}^{(m)}\\} \\subset \\mathcal{M}_{n}(\\mathbb{R})\\) 이 \\(i \\ne j \\implies \\langle \\boldsymbol{Av}^{(i)},\\, \\boldsymbol{v}^{(j)}\\rangle = 0\\) 일 때 \\(\\{\\boldsymbol{v}^{(1)}, \\ldots, \\boldsymbol{v}^{(m)}\\}\\) 를 \\(\\boldsymbol{A}\\)-직교 벡터라고 한다.\n\n\n\n명제 5 Positive definite \\(\\boldsymbol{A}\\) 에 대해 \\(\\{\\boldsymbol{v}^{(1)}, \\ldots, \\boldsymbol{v}^{(m)}\\}\\) 가 \\(\\boldsymbol{A}\\)-직교 벡터일 때 이 벡터들은 선형독립이다.\n\n\n\n\n(증명). \\(\\sum_i c_i \\boldsymbol{v}^{(i)} = \\boldsymbol{0}\\) 일 때 임의의 \\(\\boldsymbol{v}^{(j)}\\) 에 대해\n\\[\n0 = \\left\\langle \\boldsymbol{A} \\left( \\sum_i c_i \\boldsymbol{v}^{(i)} \\right), \\, \\boldsymbol{v}^{(j)}\\right\\rangle = c_j\n\\]\n이다. 따라서 \\(c_1 = \\cdots = c_m=0\\) 이므로 \\(\\{\\boldsymbol{v}^{(1)}, \\ldots, \\boldsymbol{v}^{(m)}\\}\\) 는 선형독립이다.\n\n\n즉 \\(\\boldsymbol{A}\\in \\mathcal{M}_{n\\times n}(\\mathbb{R})\\) 에 대해 \\(\\boldsymbol{A}\\)-직교인 \\(n\\) 개의 벡터를 얻었다면 이 \\(n\\) 개의 벡터는 \\(\\mathcal{M}_n(\\mathbb{R})\\) 의 기저가 된다. 이제 \\(\\boldsymbol{A}\\)-직교인 \\(\\{\\boldsymbol{v}^{(k)} : k=1,\\ldots,\\,n\\}\\) 를 구해야 할 것 같지만 잠시 미뤄두자. 구하는 것은 복잡하거나 어렵지 않다. 일단 \\(\\boldsymbol{A}\\)-직교인 \\(\\{\\boldsymbol{v}^{(k)} : k=1,\\ldots,\\,n\\}\\) 를 안다면 우리는 최대 \\(n\\) 번의 iteration 선형방정식의 해를 이론적으로 구할 수 있다는 것을 보일 수 있다. 여기서 이론적이라는 것은 roundoff-error 등으로 인한 오차를 생각하지 않는다는 조건이다.\n\n\n\n정리 5 Positive definite \\(\\boldsymbol{A}\\in \\mathbb{R}^{n\\times n}\\) 에 대해 \\(\\{\\boldsymbol{v}^{(1)}, \\ldots,\\,\\boldsymbol{v}^{(n)}\\}\\) 이 \\(\\boldsymbol{A}\\)-직교 벡터라고 하자. 임의의 \\(\\boldsymbol{x}^{(0)}\\in \\mathbb{R}^n\\) 과 \\(k=1,\\ldots,\\,n\\) 에 대해\n\\[\n\\begin{aligned}\nt_k &= \\dfrac{\\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(k-1)}, \\, \\boldsymbol{v}^{(k)}\\rangle }{\\langle \\boldsymbol{Av}^{(k)},\\, \\boldsymbol{v}^{(k)}\\rangle}, \\\\\n\\boldsymbol{x}^{(k)} &= \\boldsymbol{x}^{(k-1)} + t_{k} \\boldsymbol{v}^{(k)}\n\\end{aligned}\n\\]\n라면, \\(\\boldsymbol{Ax}^{(n)} = \\boldsymbol{b}\\) 이다.\n\n\n\n\n(증명). 주어진 식으로부터 \\[\n\\begin{aligned}\n\\boldsymbol{Ax}^{(n)} &= \\boldsymbol{Ax}^{(n-1)} + t_ n \\boldsymbol{Av}^{(n)} \\\\\n&= \\boldsymbol{A}(\\boldsymbol{x}^{(n-2)}  + t_{n-1}\\boldsymbol{v}^{(n-1)}) + t_n \\boldsymbol{Av}^{(n)}\\\\\n& \\qquad \\qquad  \\vdots \\\\\n& = \\boldsymbol{Ax}^{(0)} + t_1\\boldsymbol{Av}^{(1)} + \\cdots + t_n \\boldsymbol{Av}^{(n)}\n\\end{aligned}\n\\tag{5}\\]\n을 얻는다. 따라서,\n\\[\n\\boldsymbol{Ax}^{(n)} - \\boldsymbol{b} = (\\boldsymbol{Ax}^{(0)} - \\boldsymbol{b})+ t_1\\boldsymbol{Av}^{(1)} + \\cdots + t_n \\boldsymbol{Av}^{(n)}\n\\]\n이다. 여기에 임의의 \\(k\\in \\{1,\\ldots,\\,n\\}\\) 에 대해 \\(\\boldsymbol{v}^{(k)}\\) 와의 내적을 구하면\n\\[\n\\langle \\boldsymbol{Ax}^{(n)} - \\boldsymbol{b},\\, \\boldsymbol{v}^{(k)}\\rangle = \\langle \\boldsymbol{Ax}^{(0)}- \\boldsymbol{b}, \\, \\boldsymbol{v}^{(k)}\\rangle + t_k \\langle \\boldsymbol{Av}^{(k)},\\, \\, \\boldsymbol{v}^{(k)}\\rangle\n\\]\n이며 \\(t_k\\) 의 정의로부터\n\\[\n\\begin{aligned}\nt_k \\langle \\boldsymbol{Av}^{(k)},\\, \\, \\boldsymbol{v}^{(k)}\\rangle &= \\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(k-1)}, \\, \\boldsymbol{v}^{(k)}\\rangle  \\\\\n&= \\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(0)},\\, \\boldsymbol{v}^{(k)}\\rangle - \\sum_{j=1}^{k-1} t_j \\langle \\boldsymbol{Av}^{(j)},\\, \\boldsymbol{v}^{(k)}\\rangle \\\\\n&= \\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(0)},\\, \\boldsymbol{v}^{(k)}\\rangle\n\\end{aligned}\n\\]\n이므로,\n\\[\n\\langle \\boldsymbol{Ax}^{(n)}-\\boldsymbol{b},\\, \\boldsymbol{v}^{(k)}\\rangle = \\boldsymbol{0}\n\\]\n임을 얻을 수 있다. 임의의 \\(k\\in \\{1,\\ldots,\\,n\\}\\) 에 대해 성립하며 \\(\\{\\boldsymbol{v}^{(1)},\\ldots,\\,\\boldsymbol{v}^{(n)}\\}\\) 이 \\(\\mathcal{M}_n(\\mathbb{R})\\) 의 기저이므로\n\\[\n\\boldsymbol{Ax}^{(n)} = \\boldsymbol{b}\n\\]\n이다. \\(\\square\\)\n\n\n이제 \\(\\boldsymbol{A}\\)-직교 벡터를 구하는 방법을 알아보기 전에 다음을 보자.\n\n\n정리 6 정리 5 의 조건에서 \\(\\boldsymbol{r}^{(k)}=\\boldsymbol{b}-\\boldsymbol{Ax}^{(k)}\\) 이라 하면 \\(j=1,\\ldots,\\,k\\) 에 대해 \\(\\langle \\boldsymbol{r}^{(k)}, \\boldsymbol{v}^{(j)}\\rangle=0\\) 이다.\n\n\n\n\n(증명). Induction 으로 증명한다. \\(k=1\\) 에 대해 \\(\\boldsymbol{r}^{(1)} = \\boldsymbol{b}-\\boldsymbol{Ax}^{(1)}\\) 이며 \\(\\boldsymbol{x}^{(1)} =  \\boldsymbol{x}^{(0)} +t_1 \\boldsymbol{v}^{(1)}\\) 이다. \\(t_1 = \\dfrac{\\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(0)}, \\boldsymbol{v}^{(1)}\\rangle}{\\langle \\boldsymbol{Av}^{(1)},\\boldsymbol{v}^{(1)}\\rangle}\\) 이므로,\n\\[\n\\begin{aligned}\n\\langle \\boldsymbol{r}^{(1)},\\, \\boldsymbol{v}^{(1)} \\rangle &=  \\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(1)},\\, \\boldsymbol{v}^{(1)} \\rangle =\\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(0)},\\, \\boldsymbol{v}^{(1)}\\rangle - t_1 \\langle \\boldsymbol{Av}^{(1)},\\, \\boldsymbol{v}^{(1)}\\rangle \\\\\n&= \\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(0)},\\, \\boldsymbol{v}^{(1)}\\rangle - \\langle \\boldsymbol{b}-\\boldsymbol{Ax}^{(0)},\\, \\boldsymbol{v}^{(1)}\\rangle =\\boldsymbol{0}\n\\end{aligned}\n\\]\n이다. 이제 \\(k\\) 보다 작거나 같은 정수에 대해 성립함을 가정한다.\n\\[\n\\boldsymbol{r}^{(k+1)} =  \\boldsymbol{b} - \\boldsymbol{Ax}^{(k+1)} = \\boldsymbol{b} - \\boldsymbol{Ax}^{(k)} - t_{k+1}\\boldsymbol{Av}^{(k+1)} = \\boldsymbol{r}^{(k)} - t_{k+1}\\boldsymbol{Av}^{(k+1)}\n\\]\n이므로, \\(j=1,\\ldots,\\,k+1\\) 에 대해\n\\[\n\\langle \\boldsymbol{r}^{(k+1)},\\, \\boldsymbol{v}^{(j)}\\rangle = \\langle \\boldsymbol{r}^{(k)},\\, \\boldsymbol{v}^{(j)}\\rangle - t_{k+1} \\langle \\boldsymbol{Av}^{(k+1)},\\, \\boldsymbol{v}^{(j)}\\rangle\n\\]\n이다. \\(j\\le k\\) 라면 \\(\\langle \\boldsymbol{r}^{(k)},\\, \\boldsymbol{v}^{(j)}\\rangle\\) 은 induction 의 가정에 의해 \\(0\\) 이며 \\(\\langle \\boldsymbol{Av}^{(k+1)},\\, \\boldsymbol{v}^{(j)}\\rangle\\) 은 \\(\\boldsymbol{A}\\)-직교조건에 의해 \\(0\\) 이다. \\(j=k+1\\) 이라면 \\(t_k\\) 와 \\(\\boldsymbol{r}^{(k)}\\) 의 정의에 의해 \\(0\\) 이다. 따라서 명제가 성립한다. \\(\\square\\)\n\n\n\n\n정리 7 임의의 \\(\\boldsymbol{x}^{(0)}\\) 에 대해 \\(\\boldsymbol{v}^{(1)}=\\boldsymbol{r}^{(0)}= \\boldsymbol{b}-\\boldsymbol{Ax}^{(0)}\\) 로 잡자. 그리고 \\(\\boldsymbol{x}^{(k-1)}\\) 와 \\(\\boldsymbol{v}^{(k)}\\) 이 주어졌을 때,\n\\[\n\\begin{aligned}\n\\boldsymbol{x}^{(k)} &= \\boldsymbol{x}^{(k-1)} + t_{k} \\boldsymbol{v}^{(k)}, \\\\\n\\boldsymbol{v}^{(k+1)} &= \\boldsymbol{r}^{(k)} + s_{k}\\boldsymbol{v}^{(k)},\\qquad s_{k}= -\\dfrac{\\langle \\boldsymbol{v}^{(k)},\\, \\boldsymbol{Ar}^{(k)}\\rangle}{\\langle \\boldsymbol{v}^{(k)},\\, \\boldsymbol{Av}^{(k)}\\rangle}\n\\end{aligned}\n\\]\n이라고 하면 \\(j=1,\\ldots,\\,k-1\\) 에 대해 \\(\\langle \\boldsymbol{v}^{(j)},\\, \\boldsymbol{Av}^{(k)}\\rangle = 0\\) 이며 \\(\\langle \\boldsymbol{r}^{(j)},\\, \\boldsymbol{r}^{(k)}\\rangle = 0\\) 이다.\n\n\n\n\n(증명). Induction 으로 증명한다. 우선 \\(k=2\\) 일 때 증명한다. \\[\n\\begin{aligned}\n\\langle \\boldsymbol{v}^{(1)},\\, \\boldsymbol{Av}^{(2)}\\rangle &= \\langle \\boldsymbol{v}^{(1)},\\, \\boldsymbol{A}(\\boldsymbol{r}^{(1)}+s_{2}\\boldsymbol{v}^{(1)})\\rangle \\\\\n&= \\langle \\boldsymbol{v}^{(1)},\\, \\boldsymbol{Ar}^{(1)}\\rangle + s_2 \\langle \\boldsymbol{v}^{(1)},\\, \\boldsymbol{\\boldsymbol{v}^{(1)}}\\rangle = 0\n\\end{aligned}\n\\]\n이다. 이제 \\(k\\) 보다 작거나 같은 정수에 대해 성립함을 가정한다.\n\\[\n\\begin{aligned}\n\\langle \\boldsymbol{v}^{(j)},\\, \\boldsymbol{Av}^{(k+1)}\\rangle &=\\langle \\boldsymbol{v}^{(j)},\\, \\boldsymbol{Ar}^{(k)} \\rangle + s_k \\langle \\boldsymbol{v}^{(j)},\\, \\boldsymbol{Av}^{(k)}\\rangle\n\\end{aligned}\n\\]\n\\(j=k\\) 이면 \\(s_k\\) 의 정의에 의해 \\(0\\) 이 된다. \\(j&lt;k\\) 이면 induction 의 가정에 의해 \\(\\langle \\boldsymbol{v}^{(j)},\\, \\boldsymbol{Av}^{(k)}\\rangle =0\\) 이므로\n\\[\n\\begin{aligned}\n\\langle \\boldsymbol{v}^{(j)},\\, \\boldsymbol{Av}^{(k+1)}\\rangle &=\\langle \\boldsymbol{v}^{(j)},\\, \\boldsymbol{Ar}^{(k)} \\rangle = \\\n\\end{aligned}\n\\]",
    "crumbs": [
      "수치해석 II",
      "선형방정식과 반복법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/07_finding_root.html",
    "href": "src/numerical_analysis_using_julia/07_finding_root.html",
    "title": "일변수 방정식의 해",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]\n우리는 수학을 배운 이후 \\(2x-6=0\\) 와 같은 방정식의 해를 구하는 것을 배워 왔다. 여기서는 일변수 방정식, 그중에서도 실함수에 대한 실수해를 구하는 방법을 알아본다.\n많은 경우 우리는 해가 존재하는지 아닌지 여부를 쉽게 알 수 있다. 예를 들어 3차방정식의 경우는 최소한 하나의 해가 존재하며 복잡하긴 하지만 우리는 그 계수만을 가지고 근을 알 수 있는 공식이 존재한다는 것을 안다. 그러나 5차방정식 혹은 그 이상의 홀수차 방정식의 경우는 그 계수만으로 해를 알 수 있는 근의공식이 존재하지 않지만 최소한 하나의 해가 존재한다는 것을 알고 있다. 이 장에서 다루는 수치해석적인 방법을 통해 어떤 구간 안에서 해가 하나 이상 존재하는 것을 미리 알고 있을 때, 그 구간에서의 해를 하나 구할 수 있다.\n또 하나 이 장에서 새롭개 배우는 것은 소위 반복법 (iteration) 이다. 지금까지는 이미 존재하는 함수나 데이터를 통해 우리가 구하고자 하는 값을 직접적으로 구했다. 그러나 많은 경우 직접적으로 이 값을 구할 수 없으며, 이런 경우에는 반복적인 계산을 통해 우리가 구하고자 값에 가까운 값을 구할 수 있다. 이 때 우리가 이 반복적인 실행을 통해 원하는 값을 얻었는지를 판단할 수 있는 근거가 필요한데 이 값들을 허용범위(tolerence) 라고 한다. 예를 들어 우리가 방정식 \\(f(x)=0\\) 의 어떤 해가 \\((-1.0\\times 10^{-8}, \\, 1.0\\times 10^{-8})\\) 구간에 존재한다는 것을 알았다고 하자. 이 경우 해를 \\(x=0\\) 으로 잡으면 최대 오차는 \\(1.0 \\times 10^{-8}\\) 인데 어떤 경우는 부족하지만 어떤 경우는 충분히 만족스러울 수 있다. 만약 이정도의 오차가 충분히 만족스럽다면 허용범위를 구간의 범위가 \\(2.0 \\times 10^{-8}\\) 보다 작다고 정하면된다. 문제에 따라 다양한 허용 범위를 조합하여 사용 할 수도 있다.",
    "crumbs": [
      "수치해석 I",
      "일변수 방정식의 해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/07_finding_root.html#이분법-bisection-method",
    "href": "src/numerical_analysis_using_julia/07_finding_root.html#이분법-bisection-method",
    "title": "일변수 방정식의 해",
    "section": "1 이분법 (Bisection Method)",
    "text": "1 이분법 (Bisection Method)\n\\(f:[a,\\,b]\\to \\mathbb{R}\\) 인 함수가 연속이고 \\(f(a)f(b)&lt;0\\) 라면 중간값 정리 에 의해 \\(f(c)=0\\) 인 \\(c\\in(a,\\,b)\\) 가 존재한다. 이제 \\(d_1 = (a+b)/2\\) 에 대해 \\(f(d_1)=0\\) 이면 해를 찾았고 \\(f(a)f(d_1)&gt;0\\) 이면 해는 \\((d_1,\\,b)\\) 에 존재하며 \\(f(a)f(d_1)&lt;0\\) 이면 해는 \\((a,\\,d_1)\\) 에 존재한다. 여기에 대해 이것을 \\(k\\) 회 반복하면 우리는 그 너비가 \\(\\dfrac{(b-a)}{2^k}\\) 가 되는 해가 존재하는 구간을 항상 찾을 수 있다. 이 과정을 요약하면 아래 그림과 같다.\n\n\n\n이분법\n\n\n이 과정을 프로그래밍 입장에서 다시 정리해보자. 일단 처음 주어지는 함수 \\(f(x)\\) 와 \\(a,\\,b\\) 에 대해 \\(a&lt;b\\) 로 주어졌다고 하자. 그리고 우리는 실제의 근 \\(x_0\\) 와 우리가 구한 근의 차이를 어떤 작은 양수 \\(\\delta\\) 보다 작게 하고자 한다. 이 \\(\\delta\\) 가 허용범위이다.\n\n\\(a_0=a,\\,b_0=b,\\,c_0=\\dfrac{a+b}{2}\\) 로 놓는다. 만약 \\(f(a_0) \\cdot f(c_0)&lt;0\\) 이면 역시 중간값 정리에 의해 \\((a_0,\\, c_0)\\) 구간에 해가 존재함을 알 수 있다. 그렇다면 \\(a_1=a_0,\\, b_1=c_0\\) 으로 놓는다. 반대로 \\(f(a_0)\\cdot f(c_0)&gt;0\\) 이면 \\(f(c_0)\\cdot f(b_0)&lt;0\\) 이므로 \\((c_0,\\,b_0)\\) 구간에 해가 존재한다. 이때는 \\(a_1=c_0,\\, b_1=b_0=b\\) 로 놓는다. \\(f(c_0)=0\\) 이면 \\(c\\) 이 \\(f(x)=0\\) 의 해이므로 더이상 진행하지 않는다.\n\\(a_n&lt;b_n\\) 에 대해 \\(f(a_n)\\cdot f(b_n)&lt;0\\) 이라 하자. \\(|b_n-a_n|&lt;2\\delta\\) 이면 실제의 근 \\(x_0\\) 와 \\(c_n =\\dfrac{a_n+b_n}{2}\\) 의 차이는 \\(\\delta\\) 보다 작기 때문에 \\(c_n\\) 이 우리가 구하고자 하는 수치적인, 그리고 근사적인 근이 된다. \\(|b_n-a_n|\\ge 2\\delta\\) 이면, \\(c_n =\\dfrac{a_n+b_n}{2}\\) 이라 놓는다. \\(f(c_n)=0\\) 이면 우리는 하나의 해를 구했으므로 종료한다. \\(f(a_n)\\cdot f(c_n)&lt;0\\) 이면 \\((a_n,\\,c_n)\\) 구간에서 하나의 해가 존재하므로 \\(a_{n+1}=a_n,\\, b_{n+1}=b_n\\) 으로 놓고 이 과정을 계속 한다. \\(f(a_n)\\cdot f(c_n)&gt;0\\) 이면 \\(f(b_n) \\cdot f(c_n)&lt;0\\) 이므로 \\(a_{n+1}=c_n,\\, b_{n+1}=b_n\\) 으로 놓는다.\n\n\n이제 이것을 줄리아로 구현해 보자. 실제 프로그래밍 상에서는 \\(a_n\\) 이나 \\(b_n\\) 계산하지 않고, \\(a,\\,b\\) 의 변수값을 계속 바꾸도록 한다. \\(a_n\\) 과 \\(b_n\\) 의 히스토리는 대부분 중요하지 않다.\n\n\n이분법에 대한 julia 구현\n이분법을 rootfinding_bisection 함수로 구현해보자. 우선 인자로 해를 구하고자 하는 함수(f) 와, 양 끝 구간(a, b), 그리고 허용범위(xtol)가 필요하다. 이분법은 잠시 뒤에 보이겠지만 허용범위가 양수일 경우 유한번의 반복으로 허용범위 내의 근사적 해를 구한다. 실제 계산에서는 필요하지 않을 수도 있지만 여기서는 몇번의 반복으로 해를 구했는지 확인하기 위해 bisection 함수가 근사적 해와 반복 횟수를 반환하도록 한다.\nfunction rootfinding_bisection(\n    f::Function,                    # 함수\n    a::Real,                        # 구간값 1\n    b::Real,                        # 구간값 2\n    xtol::Real = 1.0e-8,            # 해의 오차의 허용 범위\n    etol::Real = 1.0e-10)           # 해의 함수값의 허용 범위\n    Niter = 0\n    a, b = minmax(a, b)\n    f(a)*f(b) &lt;= 0 || error(\"f(a)*f(b) should be negative\") \n    c = (a+b)/2\n    while ((b-a) &gt; 2*xtol) || (abs(f(c))&lt;etol)\n        Niter +=1\n        \n        if f(c) == 0.0\n            break\n        elseif f(a)*f(c) &lt; 0 \n            a, b = a, c\n        else \n            a, b = c, b \n        end\n        c = (a+b)/2\n    end\n    return c, Niter\nend\n이제 이분법을 이용하여 \\(\\sqrt{2}\\) 의 근사값을 구할 수 있다. \\(\\sqrt{2}\\) 는 \\(f(x)=x^2-2\\) 의 근이므로\nrootfinding_bisection(x -&gt; x^2-2, 0.0, 4.0)\n라 하면, sqrt2 의 값은 다음과 같다.\n(1.4142135605216026, 28)\n\n\n\n\n\n\n\n노트\n\n\n\n앞의 코드에서 f(a)*f(b) &lt;= 0 || error(\"f(a)*f(b) should be negative\") 는 소위 단락 계산 (short-circit evaluation) 을 사용한 표현이다. 표현식 a && b 에서, 하위 표현식 b는 오직 a 가 true 일때만 실행된다. 반면 표현식 a || b 에서, 하위 표현식 b 는 오직 a 가 false 로 계산될 때만 계산을 받는다. 왜냐 하면, a 가 false 이면, b 의 값에 관계없이 a && b 는 무조건 false 가 되고, a 가 true 이면, b 의 값에 관계없이 a && b 는 무조건 true 가 되기 때문이다. 따라서 f(a)*f(b) 가 0 보다 큰 경우에만 에러를 발생시킨다. 이것은\nif f(a)*f(b) &lt;= 0\n    error(\"f(a)*f(b) )\n와 같은 표현이다.",
    "crumbs": [
      "수치해석 I",
      "일변수 방정식의 해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/07_finding_root.html#고정점-반복법-fixed-point-iteration",
    "href": "src/numerical_analysis_using_julia/07_finding_root.html#고정점-반복법-fixed-point-iteration",
    "title": "일변수 방정식의 해",
    "section": "2 고정점 반복법 (Fixed Point Iteration)",
    "text": "2 고정점 반복법 (Fixed Point Iteration)\n함수 \\(f(x)\\) 에 대해 \\(f(p)=p\\) 를 만족하는 점 \\(p\\) 를 \\(f\\) 의 고정점 (fixed point) 라 한다. 고정점 자체가 중요한 경우도 많으며, 방정식의 해를 찾는것과 밀접하게 연관되어 있다. \\(g(x) = f(x)-x\\) 라 놓으면 \\(f(x)\\) 의 고정점을 찾는 것은 \\(g(x)\\) 의 해를 찾는 문제와 같은 문제라는 것을 쉽게 이해 할 수 있을 것이다. 역으로, 함수 \\(g(x)=0\\) 의 해를 찾는 문제는 \\(f(x) = g(x)  + x\\) 의 고정점을 찾는 문제와 동일한 문제이다.\n고정점 반복법이 가능한 이유는 아래의 고정점 정리 때문이다.\n\n\n\n정리 1 (고정점 정리) \\(f\\) 가 \\([a,\\,b]\\) 구간에서 연속이며 \\(f([a,\\,b])\\subset [a,\\,b]\\) 이면,\n  (\\(1\\)) \\(f(p)=p\\) 를 만족하는 고정점 \\(p\\) 가 \\([a,\\,b]\\) 구간에 존재한다.\n  (\\(2\\)) 모든 \\(x,\\,y \\in [a,\\,b]\\) 에 대해 \\(|f(x)-f(y) |\\le \\lambda |x-y|\\) 를 만족하는 \\(\\lambda\\in\\mathbb{R}\\) 이 존재하며, \\(0&lt;\\lambda&lt;1\\) 이면 고정점은 유일하다.\n  (\\(3\\)) (\\(2\\))의 조건을 만족시킬 때, 임의의 \\(p_1\\in [a,\\,b]\\) 와 \\(p_{n+1}=f(p_n)\\) 으로 정의되는 수열 \\(\\langle p_n\\rangle\\) 은 수렴한다.\n\n\n\n\n(증명). (\\(1\\)) \\(f(a)=a\\) 이거나 \\(f(b)=b\\) 이면 자명하므로 \\(f(a)\\ne a,\\, f(b) \\ne b\\) 인 경우만 생각한다. \\(g(x)=f(x)-x\\) 라 하면 가정에 의해 \\(g(a)\\ne 0,\\, g(b) \\ne 0\\) 이다. 모든 \\(x\\in [a,\\,b]\\) 에 대해 \\(a&lt;f(x)&lt;b\\) 이므로 \\(g(a)=f(a)-a&gt;0\\) 이며 \\(g(b)=f(b)-b&lt;0\\) 이다. \\(g(x)\\) 역시 \\([a,\\,b]\\) 에서 연속이므로 중간값정리에 의해 \\(g(p)=0\\) 인 \\(p\\in(a,\\,b)\\) 가 존재해야 한다. 즉 \\(f(p)=p\\) 를 만족하는 점 \\(p\\in [a,\\,b]\\) 가 존재한다.\n(\\(2\\)) 고정점이 두개 이상 존재한다고 가정하고 두개의 임의의 고정점을 \\(p,\\,q\\) 라 하자. 그렇다면, \\[\n|p-q | = |f(p)-f(q)| \\le \\lambda |p-q|&lt; |p-q|\n\\]\n이므로 모순이다. 따라서 고정점은 유일하다.\n(\\(3\\)) \\(p\\) 를 유일한 고정점이라고 하자.\n\\[\n|p-p_{n+1}| = |f(p) - f(p_n)| \\le \\lambda |p-p_n| \\le \\lambda^{n-1}|p-p_1|\n\\]\n이므로 \\(\\displaystyle \\lim_{n \\to \\infty} p_n = p\\) 이다. \\(\\square\\)\n\n\n\n\n따름정리 1 \\([a,\\,b]\\) 구간에서 미분가능한 함수 \\(f(x)\\) 가 \\(f([a,\\,b])\\subset [a,\\,b]\\) 이며, 어떤 양수 \\(0&lt;\\gamma&lt;1\\) 에 대해 \\(x\\in [a,\\,b] \\implies |f'(x)|&lt;\\gamma\\) 이면, \\([a,\\,b]\\) 구간에서 유일한 고정점을 가진다.\n\n\n\n\n(증명). 고정점 정리의 (\\(2\\)) 를 생각하자. \\([a,\\,b]\\) 구간에서 \\(f\\) 가 미분가능이므로 중간값 정리에 의해 \\(\\dfrac{f(x)-f(y)}{x-y}=f'(c)\\) 인 \\(c\\in [a,\\,b]\\) 가 존재하며, \\(|f'(x)|&lt;\\gamma&lt;1\\) 이므로 \\[\n|f(x)-f(y)|=|f'(c)||x-y|&lt;\\gamma |x-y|\n\\]\n이다. 따라서 \\(f(x)\\) 는 \\([a,\\,b]\\) 구간에서 유일한 고정점을 가진다. \\(\\square\\)\n\n\n\n예제 1 함수 \\(f(x) = x^2-2\\) 를 생각하자. \\(f(0)=-2\\) 이며 \\(f(2)=2\\) 이므로 \\([0,\\,2]\\) 구간에서 해가 반드시 존재한다.\n\n\n예제 2 이제 고정점 정리를 이용하여 \\(f(x)=x^3 − 2x^2 − 1 = 0\\) 의 해를 \\([1,\\,3]\\) 구간에서 구해보자. \\(g(x) = x-f(x)\\) 로 놓으면, \\(g(1) = 3,\\, g(3)=-5\\) 이므로 고정점 정리의 가정 \\(g([a,\\,b]) \\subset [a,\\,b]\\) 에 위배된다. \\(p\\) 가 \\(f(x)\\) 의 해이면 \\(p^3 = 2p^2+1\\) 이므로 \\(p\\) 는 \\(h(x) = (2x^2+1)^{1/3}\\) 의 해이다. \\(h(1) \\approx 1.442,\\, h(3) \\approx 2.668\\) 이므로 \\(h(x)\\) 는 고정점 정리의 제 1 조건을 만족한다. 또한\n\\[\nh'(x) = \\dfrac{4x}{3(2x^2+1)^{2/3}}\n\\]\n이며, \\(x\\in [1, 3]\\) 이므로 \\(x&gt;0\\) 이다. 따라서 \\([1,\\,3]\\) 구간에서,\n\\[\n|h'(x) | = \\dfrac{4|x|}{|3(2x^2+1)^{2/3}|} \\le \\dfrac{4|x|}{|3(2x^2+1)^{1/2}|} \\le \\dfrac{4|x|}{|3(2x^2)^{1/2}|}  &lt; \\dfrac{4}{3\\sqrt{2}} &lt;0.943&lt; 1\n\\]\n이므로 \\(h(x)\\) 는 \\([1,\\,3]\\) 구간에서 유일한 고정점을 가진다.\n\nfunction fixedpoint(f::Function, xi::T, xtol::T=1.0e-10, MaxIter::Int64 = 100_000)::T where T&lt;:AbstractFloat\n    Niter = 0\n    for i in 1:MaxIter\n        c = f(xi)\n        if abs(c-xi) &lt; xtol\n            return c\n        else\n            xi = c\n            Niter += 1\n        end\n    end\n    error(\"최대 반복 횟수 $MaxIter 에 도달하였으나 답을 찾지 못함.\")\nend\n\nh(x) = (2x^2+1)^(1/3)\nfixedpoint(h, 2.0)\n여기서 얻은 결과는 2.2055694302615003 이다.",
    "crumbs": [
      "수치해석 I",
      "일변수 방정식의 해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/07_finding_root.html#sec-NA1_newton_method",
    "href": "src/numerical_analysis_using_julia/07_finding_root.html#sec-NA1_newton_method",
    "title": "일변수 방정식의 해",
    "section": "3 뉴턴 방법 (Newton-Method)",
    "text": "3 뉴턴 방법 (Newton-Method)\n\n\n\n뉴턴 방법\n\n\n함수 \\(f(x)\\) 가 미분가능하다고 하자. 그리고 \\((a,\\,b)\\) 구간에서 \\(f(x)=0\\) 의 해가 존재하며, 이 구간에서 도함수 \\(f'(x)\\) 가 \\(0\\) 보다 항상 크다고 하자. 해는 정확히 모르지만 대략 \\(p_0 \\in (a,\\,b)\\) 근처임을 안다고 하자. 테일러 정리에 의해 \\(f(x)\\) 는 \\(p_0\\) 근처에서 대략 \\[\nf(x) \\approx f(p_0) + f'(p_0)(x-p_0)\n\\]\n이다. 이제 \\(f_0 (x) =  f(p_0) + f'(p_0)(x-p_0)\\) 라 놓고 \\(f_0(x) = 0\\) 의 해를 구하여 \\(p_1\\) 이라 하면,\n\\[\np_1 = p_0 - \\dfrac{f(p_0)}{f'(p_0)}\n\\]\n가 된다. 이제 \\(f(x)\\) 를 \\(p_1\\) 에서의 1차 다항식으로 전개한 \\(f_1(x)\\) 는\n\\[\nf_1(x) = f(p_1) + f'(p_1)(x-p_1)\n\\]\n이 되며 이 식의 해를 \\(p_2\\) 라 놓는다. 즉 \\(p_2 = p_1 - \\dfrac{f(p_1)}{f'(p_1)}\\) 이다. 이 과정을 반복하여 해를 얻는 방법을 뉴턴 방법(Newton method) 혹은 뉴턴-랩슨 방법(Newton-Raphson) method 라고 한다.\n\n앞서 배운 이분법과는 달리 뉴턴방법은 시작점이 해와 충분히 가깝지 않다면 해를 발견하지 못할 수도 있다. 뉴턴 방법 역시 조건이 만족할 때 까지 반복문을 수행하는데 이 반복이 끝나지 않을 수 있다.\n\n\n무한루프\n수치해석은 기본적으로 for ~ end 문이나 while ~ end 같은 반복문이 아주 많이 쓰인다. 이 가운데 while (condition) ~ end 구문은 조건이 참인지 아닌지 여부만을 따져 수행을 반복하기 때문에 잘못하면 수행이 끝나지 않을 수 있다. 예를 들어,\nv = 1\nwhile v&lt;10\n    v=v*1\nend\n같은 코드는 종료되지 않는다. 이분법의 경우는 상관 없지만 어떤 알고리즘을 루프로 구현할 경우, 다양한 원인으로 인해 무한루프에 빠질 수 있다. 프로그래밍에 익숙한 사용자라면 일정 시간이 흘러도 결과가 나오지 않으면 무한루프에 빠졌다는 것을 쉽게 알 수 있을 수도 있다. 그러나 함수값 하나 얻는 데 시간이 오래 결리는 경우는, 수행시간이 오래 지났더라도 이것이 정상적인 실행중인지, 무한루프에 빠졌는지 불확실할 경우도 있다. 그래서 이런 경우는 보통 최대 반복 횟수를 정해놓고 이 횟수에 도달하면 에러를 발생시키는 등으로 정상적인 결과를 얻지 못했다는 것을 알려 주는것이 좋다.\n\n\n\n뉴턴 방법의 구현\n\n\\(f(x)=0\\) 을 만족하는 해가 대략 \\(p_0\\) 근처임을 안다.\n\\(p_{n+1} = p_{n}- \\dfrac{f(p_{n})}{f'(p_{n})}\\) 을 계속 반복\n\n이를 구현한 코드는 아래와 같다. 앞서 언급했듯이 뉴턴 방법은 분할법과는 달리 항상 해를 발견하는 것을 보장하지 않는다. 따라서 무한 루프에 빠지는 것을 방지하기 위해 최대 반복 횟수(MaxIter) 를 함수의 인자로 입력한다. \\(p_n\\) 에서의 미분값이 \\(0\\) 일 경우 \\(p_{n+1}\\) 을 구할 수 없으며, \\(|f'(p_n)|\\) 이 매우 작을 경우 원하는 범위를 벗어나기 때문에 미분값의 절대값의 최소값을 지정하여 그 절대값보다 작을 경우는 에러를 발생시키도록 한다. 이분법의 경우는 구간을 좁혀가는 방법이기 때문에 구간의 범위를 허용범위로 정했지만 뉴턴 방법에서는 \\(|f(p_n)|\\) 값이 특정 값 \\(\\delta&gt;0\\) 보다 작을 때 \\(x_n\\) 을 근사적 해로 간주하며 \\(\\delta\\) 를 허용범위로 한다. 다음 함수에서는 이 허용범위를 etol 인자로 전달한다.\nfunction rootfinding_newton(\n    f::Function,                    # 함수\n    df::Function,                   # 도함수\n    p::Real,                        # 시작값\n    MaxIter::Int64=100_000,         # 최대 반복 횟수\n    etol::Real = 1.0e-8,            # 해의 함수값의 허용 범위\n    dfmin::Real = 1.0e-6)           # 미분값의 절대값의 허용되는 최소값\n    \n    Niter = 0\n    for i in 1:MaxIter\n        if abs(f(p)) &lt; etol\n            break\n        elseif abs(df(p)) &lt; dfmin \n            error(\"df ≈ 0.0\")\n        end\n        p = p - f(p)/df(p)\n        Niter += 1       \n        \n        if abs(f(p)) &lt; etol \n            return (p, Niter)\n        end\n    end\n    error(\"최대 반복 횟수 $MaxIter 에 도달하였으나 답을 찾지 못함\")\nend\n\n\nf2(x) = x*(x-1.)*(x-2.)+5\ndf2(x) = 3*x^2-6*x+2\nrootfinding_newton(f2, df2, 1.2)\n\\(f_2(x) = x(x-1)(x-2)+5\\) 에 대한 해를 구하였으며, 시작점 \\(x_i = 1.2\\) 로 부터 추산했을 때 그 결과는 다음과 같다.\n(-0.9041608591349207, 24)\n만약 \\(0.0\\) 부터시작한다면, 즉 newton_method(f2, df2, 0.0) 를 실행한다면,\n(-0.9041608591349207, 7)\n이 된다.\n\n\n\n뉴턴 방법의 단점과 수렴\n이분법은 두 시작점에서의 함수값의 곱이 음수이면 무조건 하나의 해를 원하는 오차 내에서 찾을 수 있다. 그러나, Newton method 는 해를 찾을 수 없는 경우도 생기는데 가장 대표적인 경우는 위의 과정중에 어떤 \\(x_k\\) 에 에서 \\(f'(x_k)=0\\) 인 경우이다. 또한 시작점이 해와 충분히 가깝지 않을 경우 발산 할 수 있다. 우리는 뉴턴 방법을 통해 무조건 해를 갖는 조건을 보일 수 있다.\n\n\n정리 2 (뉴턴 방법의 수렴정리) \\(f\\in C^2_{[a,\\,b]}\\) 인 실함수 \\(f\\) 에 대해 \\(f(p)=0\\) 이며 \\(f'(p)\\ne 0\\) 이라 하자. 그렇다면 어떤 \\(\\delta&gt;0\\) 이 존재하여 \\(p_1\\in [p-\\delta,\\, p+\\delta]\\) 이라면 뉴턴 방법을 통해 얻은 수열 \\(\\langle p_n\\rangle\\) 은 \\(p\\) 로 수렴한다.\n\n\n\n\n(증명). 뉴턴 방법에 의해 \\(p_n\\) 이 정해졌을 때 \\(p_{n+1}\\) 은 직선 \\(f'(p_n) (x-p_n) + f(p_n) = 0\\) 의 해이다. 즉,\n\\[\np_{n+1}= p_n - \\dfrac{f(p_n)}{f'(p_{n})}\n\\]\n이다. \\(g(x) = x-\\dfrac{f(x)}{f'(x)}\\) 라 하자. \\(f(p)=0\\) 이므로 \\(g(p)=p\\) 이다. \\(f \\in C_2{[a,b]}\\) 이고 \\(p\\in p=(a,\\,b)\\) 이며 \\(f'(p) \\ne 0\\) 이므로 어떤 \\(p\\) 의 근방에 대해 미분값이 \\(0\\) 이 아니다. 즉 어떤 \\(\\delta_1&gt;0\\) 에 대해 \\(t\\in I_1=[p-\\delta_1,\\, p+\\delta_1]\\) 이면 \\(f'(t)\\ne 0\\) 이다. \\(g(x)\\) 는 \\(C^1_{I_1}\\) 이므로 도함수가 존재한다.\n\\[\ng'(x) = 1-\\dfrac{(f'(x))^2-f(x) f''(x)}{(f'(x))^2} = \\dfrac{f(x)f''(x)}{(f'(x))^2}\n\\]\n이며 \\(g'(p)= 0\\) 이다. \\(g\\) 의 도함수가 연속이며 \\(g'(p)=0\\) 이므로 어떤 \\(k\\in (0,\\,1)\\) 에 대해 \\(|g'(x)|\\le k\\) 인 구간 \\(I_2=[p-\\delta_2,\\, p+\\delta_2]\\) 가 존재한다. \\(\\delta = \\min(\\delta_1,\\delta_2)\\) 라 하면 구간 \\(I=[p-\\delta,p+\\delta]\\) 에 대해 \\(t\\in I\\) 이면 \\(|g'(t)|\\le k,\\, 0&lt;k&lt;1\\) 이며 \\(f'(t)\\ne 0\\) 이다.\n평균값 정리에 의해 \\(t\\in I\\) 이면 \\(g(t)-g(p) = g'(\\xi)(t-p)\\) 인 \\(\\xi\\in I\\) 가 존재한다. \\(0&lt;|g'(\\xi)|&lt;1\\) 이므로,\n\\[\n|g(t)-p| = |g(t)-g(p)| &lt; |g'(\\xi)||t-p| \\le k|t-p| &lt; |t-p|\n\\]\n\\(|t-p|&lt;\\delta\\) 이므로 \\(|g(t)-p|&lt;\\delta\\) 이다. 따라서 \\(g\\) 는 \\([p-\\delta,\\, p+\\delta]\\) 에서 \\([p-\\delta, p+\\delta]\\) 로의 함수이다. 고정점 정리(정리 1) 에 의해 \\(p_{n+1}=g(p_n)\\) 으로 정의된 수열은 \\(p\\) 로 수렴한다. \\(\\square\\)",
    "crumbs": [
      "수치해석 I",
      "일변수 방정식의 해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/07_finding_root.html#할선법-secant-method",
    "href": "src/numerical_analysis_using_julia/07_finding_root.html#할선법-secant-method",
    "title": "일변수 방정식의 해",
    "section": "4 할선법 (Secant method)",
    "text": "4 할선법 (Secant method)\n\n\n\n할선법\n\n\n뉴턴 방법에서는 함수 \\(f(x)\\) 뿐만 아니라 도함수 \\(f'(x)\\) 도 필요하다. 그러나 도함수 \\(f'(x)\\) 가 매우 복잡하거나 하여, 도함수를 직접적으로 인자로 주기 힘든 경우가 생길 수 있다. 이 때 시작점이 아닌 두 \\(x\\) 좌표 \\(p_0,\\,p_1\\) 을 함수의 인자로 주고, 도함수가 아닌 수치적으로 계산한 미분의 근사값 \\(\\dfrac{f(p_n)-f(p_{n-1})}{p_n -p_{n-1}}\\) 을 사용하는 것을 할선법이라 한다. 즉 주어진 \\(p_{n-1},\\, p_n\\) 에 대해 \\(p_{n+1}\\) 을 다음과 같이 얻는다.\n\\[\np_{n+1} = p_n - \\dfrac{f(p_n)}{g(p_n)} \\,\\qquad \\textrm{where } g(p_n)=\\dfrac{f(p_n)-f(p_{n-1})}{p_n -p_{n-1}}.\n\\]\nfunction rootfinding_secant(\n    f::Function,                # 함수\n    p0::Real,                   # 시작값 1\n    p1::Real,                   # 시작값 2\n    MaxIter::Int64 = 100_000,   # 최대 반복 횟수\n    etol::Real = 1.0e-8,        # 해의 함수값의 허용 범위\n    dfmin::Real = 1.0e-6)       # 도함수의 근사값의 절대값에 허용되는 최소값\n    \n    Niter = 0\n    \n    for i in 1:MaxIter\n        gx = (f(p1)-f(p0))/(p1-p0)\n        if abs(f(p1)) &lt; etol \n            return p1, Niter\n        elseif abs(gx)&lt;dfmin\n            error(\"df ≈ 0.0\")\n        end\n        p0, p1 = p1, p1 - f(p1)/gx\n        Niter += 1\n    end\n    error(\"최대 반복 횟수 $MaxIter 에 도달하였으나 답을 찾지 못함.\")\nend\n이것에 대해 뉴턴법과 비교해보았다.\nf2(x) = x*(x-1.)*(x-2.)+5\ndf2(x) = 3*x^2-6*x+2\nrootfinding_newton(f2, df2, 0.0)\nrootfinding_secant(f2, 0.0, 0.1)\n를 수행하면\n(-0.9041608591349207, 7)\n(-0.9041608591355101, 10)\n의 결과를 얻었다. 뉴턴 방법은 7회, 할선법은 10회의 반복으로 원하는 범위의 해를 얻었다.",
    "crumbs": [
      "수치해석 I",
      "일변수 방정식의 해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/07_finding_root.html#regula-falci",
    "href": "src/numerical_analysis_using_julia/07_finding_root.html#regula-falci",
    "title": "일변수 방정식의 해",
    "section": "5 Regula Falci",
    "text": "5 Regula Falci\n이분법과 할선법을 결합시킨 방법이다. 우선 구간 \\([a,\\,b]\\) 에서 연속인 함수 \\(f(x)\\) 의 해를 구하고자 하며 \\(f(a)\\cdot f(b)&lt;0\\) 이라 하자. 이분법에서는 \\(c=\\dfrac{a+b}{2}\\) 에서의 \\(f(c)\\) 값에 대해 \\(f(a)\\cdot f(c)&lt;0\\) 인지 여부에 따라 다음 구간이 이전 구간의 반으로 줄었다면 Regula false 에서는\n\\[\nc = b-f(b) \\cdot \\dfrac{b-a}{f(b)-f(a)}\n\\]\n에 대한 \\(f(c)\\) 를 생각한다. 이 방법은 도함수가 필요 없으며, 많은 경우 이분법보다 빨리 해를 찾는다는 장점이 있다.\nfunction rootfinding_regula_falci(\n    f::Function,                    # 함수\n    a::Real,                        # 구간값 1\n    b::Real,                        # 구간값 2\n    MaxIter::Int64 = 100_000,       # 최대 반복 회수\n    xtol::Real = 1.0e-8,            # 해의 오차의 허용 범위\n    etol::Real = 1.0e-8,            # 해의 함수값의 허용 범위\n    dfmin::Real = 1.0e-6)           # 도함수의 근사값의 절대값에 허용되는 최소값\n\n    a, b = minmax(a, b)\n    @assert f(a)*f(b) &lt; 0\n\n    Niter = 0\n\n    for i in 1:MaxIter\n        Niter +=1\n        gx =  (f(b)-f(a))/(b-a)\n        c = b-f(b)/gx\n        if (abs(b-a)&lt;xtol) || (abs(f(c))&lt;etol)\n            return c, Niter\n        elseif abs(gx) &lt; dfmin \n            error(\"df ≈ 0.0\")\n        end\n        \n        \n        if f(b)*f(c) &lt; 0 \n            a, b = b, c\n        else \n            a, b = a, c\n        end\n    end\n    error(\"최대 반복 횟수 $MaxIter 에 도달하였으나 답을 찾지 못함.\")\nend\n\n\n네가지 방법의 비교\n\\(f(x)=x-\\cos(x)\\) 에 대한 해를 구해보자. \\(f'(x) = 1+\\sin(x)\\) 이므로 \\(f(x)\\) 는 단조증가함수이다. \\(f(0)=-1&lt;0\\) 이며 \\(f(\\pi/2) = \\pi/2&gt;0\\) 이므로 우리는 \\((0,\\, \\pi/2)\\) 구간에서 단 하나의 해가 존재한다는 것을 알 수 있다. 우리는 앞서 네가지 방법에서 해를 발견할 경우 해와 반복 수행 햇수를 반환하도록 코딩하였다.\nf(x) = x-cos(x)\ndf(x) = 1+sin(x)\nprintln(rootfinding_bisection(f, 0, π/2))\nprintln(rootfinding_newton(f, df, 0.0))\nprintln(rootfinding_secant(f, 0, π/2))\nprintln(rootfinding_regula_falci(f, 0, π/2))\n그 결과는 다음과 같다.\n(0.7390851321023699, 27)\n(0.739085133385284, 4)\n(0.739085133034638, 5)\n(0.7390851292482057, 9)\n이분법이 가장 많은 반복을 수행하였으며 뉴턴법이 가장 적은 반복을 수행했다.",
    "crumbs": [
      "수치해석 I",
      "일변수 방정식의 해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/07_finding_root.html#오차-해석",
    "href": "src/numerical_analysis_using_julia/07_finding_root.html#오차-해석",
    "title": "일변수 방정식의 해",
    "section": "6 오차 해석",
    "text": "6 오차 해석\n\n수학적 기초\n\n\n\n\n\n\n\n정의 1 (수렴 속도 (order of convergence)) 수열 \\(\\langle p_n \\rangle\\) 이 \\(p\\) 로 수렴하며, 모든 \\(n\\) 에 대해 \\(p_n \\ne p_0\\) 라고 하자. 이 때\n\\[\n\\lim_{n \\to \\infty} \\dfrac{| p_{n+1}-p|}{|p_n-p|^\\alpha} = \\mu\n\\]\n를 만족하는 양수 \\(\\mu\\) 와 \\(\\alpha\\ge 1\\) 가 존재한다면 \\(\\alpha\\) 를 수열 \\(\\langle p_n \\rangle\\) 의 수렴 속도 라고 하고 \\(\\mu\\) 를 근사적 오차 상수(asymptotic error constant) 라고 한다. 특히 \\(\\alpha=1\\) 인 수열은 \\(\\mu&lt;1\\) 일 때에만 수렴하며 이 경우 선형 수렴 수열 이라고 한다. \\(\\alpha=2\\) 인 수열은 이차 수렴 수열 이라고 한다.\n\n\n\n\n\n\n\n정리 3 실함수 \\(g\\in C[a,\\,b]\\) 가 \\(g([a,\\,b]) \\subset [a,\\, b]\\) 라 하자. 정리 1 에 의해 고정점 \\(p\\in [a,\\,b]\\) 가 존재한다. 또한 어떤 양수 \\(\\lambda &lt;1\\) 가 존재하여 모든 \\(x\\in (a,\\,b)\\) 에 대해 \\(|g'(x)|&lt;\\lambda\\) 라 하자. \\(g'(p)\\ne 0\\) 라면 \\(p_0 \\ne p\\) 에 대해 수열 \\(\\langle p_n \\rangle\\) 을\n\\[\np_{n+1} = g(p_{n}),\\qquad n=0,\\,1,\\,2,\\ldots\n\\]\n로 정의하자. 이 때 이 수열은 \\([a,\\,b]\\) 의 고정점 \\(p\\) 로 수렴하는 선형 수렴 수열이다.\n\n\n\n\n(증명). 평균값 정리에 의해 각각의 \\(n=1,\\,2,\\ldots\\) 에 대해\n\\[\n\\dfrac{p_{n+1}-p}{p_{n}-p} = \\dfrac{g(p_n)-g(p)}{p_n-p} \\le g'(\\xi_n)\n\\]\n를 만족하는 \\(\\xi_n\\) 이 \\(p_n\\) 과 \\(p\\) 사이에 존재한다. 즉 \\(|p_{n+1}-p| \\le |g'(\\xi_n)(p_n-p)|\\) 이다. \\(\\langle p_n \\rangle\\) 이 \\(p\\) 로 수렴하므로 \\(\\langle \\xi_n\\rangle\\) 도 \\(p\\) 로 수렴한다. \\(g'\\) 이 \\((a,\\,b)\\) 구간에서 연속이므로,\n\\[\n\\lim_{n \\to \\infty} g'(\\xi_n) = g'(p) \\ne 0\n\\]\n이며, 따라서,\n\\[\n\\lim_{n \\to \\infty} \\dfrac{|p_{n+1}-p|}{|p_n-p|} = |g'(p)|\n\\]\n이다. \\(g'(p)\\ne 0\\) 이므로 \\(\\langle p_n \\rangle\\) 은 선형 수렴 수열이다. \\(\\square\\)\n\n– to be filled –",
    "crumbs": [
      "수치해석 I",
      "일변수 방정식의 해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06I_Bezier.html",
    "href": "src/numerical_analysis_using_julia/06I_Bezier.html",
    "title": "Interlude : 베지에 곡선(Bézier curve)",
    "section": "",
    "text": "두 점 \\(P_1,\\,P_2\\) 사이에 하나 이상의 제어점(control point) 를 두어 \\(P_1,\\,P_2\\) 를 잇는 매끄러운 곡선을 만드는 방법이다.\n\n\n\n시작점과 끝점 \\(P_1,\\,P_2\\), 그리고 하나의 제어점 \\(C\\) 가 주어졌다고 하자. \\(t\\in [0,\\,1]\\) 에 대해 \\(Q_1\\) 을 \\(P_1\\) 과 \\(C\\) 의 \\(t\\) 내분점, \\(Q_2\\) 를 \\(C\\) 과 \\(P_2\\) 의 \\(t\\) 내분점이라고 하자. 즉 \\(t\\in [0,\\,1]\\) 에 대해 \\(Q_1 = (1-t)P_1 + tC\\), \\(Q_2=(1-t)C + tP_2\\) 이다. 이 때 \\(P_1,\\,P_2,\\,C\\) 에 의해 정해지는 이차 베지어 곡선 \\(X(P_1, P_2, C, t)\\) 는\n\\[\nX_Q(P_1, P_2, C, t) = (1-t)Q_1 + tQ_2 = (1-t)^2P_1+2t(1-t)C + t^2P_2\n\\]\n로 주어진다.\n\n\n코드\n\\usetikzlibrary{calc}\n\\tikzset{\n  quadratic/.style={\n    to path={\n      (\\tikztostart) .. controls\n      ($#1!1/3!(\\tikztostart)$) and ($#1!1/3!(\\tikztotarget)$)\n      .. (\\tikztotarget)\n    }\n  }\n}\n\\begin{tikzpicture}[scale=1.0]\n\\filldraw [black] (-2, 2) circle (2pt) node[anchor=south] {$P_1$};;\n\\filldraw [black] (2, 2) circle (2pt) node[anchor=south] {$P_2$};\n\\filldraw [blue] (0,0) circle (2pt)node[anchor=north, black] {$C$};\n\\draw[blue,very thick] (-2, 2) to[quadratic={(0, 0)}] (2, 2);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n그림 1: 이차 베지에 곡선\n\n\n\n\n\n\n\n\n\n\n위의 그림은 tikz 로 그렸다. tikz 는 제어점을 하나만 줄 때에도 아래의 사차 베지에 곡선으로 그리기 때문에 베지에 곡선을 그리는 명령어를 사용 하지 않았다.\n\n\n\n\n\n\n\n시작점 \\(P_1\\), 끝점 \\(P_2\\), 제어점 \\(C_1\\), \\(C_2\\) 가 주어졌을 때 \\(t\\in [0, 1]\\) 에 대해 \\(P_1,\\,C_2\\) 를 양 끝점으로 하고 \\(C_1\\) 을 제어점으로 하는 이차 베지에 곡선 \\(X_Q(P_1, C_2, C_1, t)\\) 를 생각 할 수 있다. 또한 \\(C_1,\\, P_2\\) 를 양 끝점으로 하고 \\(C_2\\) 를 제어점으로 하는 이차 베지에 곡선 \\(X_Q(P_1, C_2, C_1, t)\\) 를 생각 할 수 있다. 사차 베지에 곡선 \\(X_C(P_1, P_2, C_1, C_2, t)\\) 는 \\(X_Q(P_1, C_2, C_1, t)\\) 와 \\(X_Q(P_1, C_2, C_1, t)\\) 의 \\(t\\) 내분점으로 정의된다. 즉,\n\\[\nX_C(P_1, P_2, C_1, C_2, t) = (1-t) X_Q(P_1, C_2, C_1, t)  + tX_Q(P_1, C_2, C_1, t)\n\\]\n이다.\n\n\n코드\n\\usetikzlibrary{calc}\n\\tikzset{\n  quadratic/.style={\n    to path={\n      (\\tikztostart) .. controls\n      ($#1!1/3!(\\tikztostart)$) and ($#1!1/3!(\\tikztotarget)$)\n      .. (\\tikztotarget)\n    }\n  }\n}\n\\begin{tikzpicture}[scale=1.0]\n\\filldraw [black] (-2, 2) circle (2pt) node[anchor=south] {$P_1$};;\n\\filldraw [black] (2, 2) circle (2pt) node[anchor=south] {$P_2$};\n\\filldraw [blue] (0,0) circle (2pt)node[anchor=north, black] {$C$};\n\\filldraw [magenta] (-1,0) circle (2pt)node[anchor=north, black] {$C_1$};\n\\filldraw [magenta] (1,0) circle (2pt)node[anchor=north, black] {$C_2$};\n\\draw[blue,very thick] (-2, 2) to[quadratic={(0, 0)}] (2, 2);\n\\draw[magenta, very thick] (-2,2) .. controls (-1,0) and (1,0) .. (2,2);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n그림 2: 이차 베지에 곡선\n\n\n\n\n\n\n\n\n이차와 사차 베지어 곡선을 다음과 같이 구현하였다. 아래의 코드는 NAJ.jl 에 포함되어 있다.\nstruct Bezier{T} \n    p1::Vector{T}\n    p2::Vector{T}\n    c1::Vector{T}\n    c2::Union{Vector{T}, Nothing}\n\n    function Bezier(\n        p1::AbstractVector{T1}, \n        p2::AbstractVector{T2}, \n        c1::AbstractVector{T3}, \n        c2::Union{AbstractVector{T4}, Nothing}=nothing) where {T1&lt;:Real, T2&lt;:Real, T3&lt;:Real, T4&lt;:Real}\n        \n        @assert length(p1) == length(p2) == length(c1)\n        \n        if c2 === nothing\n            TT = promote_type(eltype(p1), eltype(p2), eltype(c1))\n            return new{TT}(Vector(p1), Vector(p2), Vector(c1), nothing)\n        else \n            @assert length(p1) == length(c2)\n            TT = promote_type(eltype(p1), eltype(p2), eltype(c1), eltype(c2))\n            return new{TT}(Vector(p1), Vector(p2), Vector(c1), Vector(c2))\n        end\n        \n    end\nend\n\nfunction (b::Bezier)(t)\n    if b.c2 === nothing\n        return _bezier1(b.p1, b.p2, b.c1, t)\n    else \n        return _bezier2(b.p1, b.p2, b.c1, b.c2, t)\n    end\nend\n\nfunction _bezier1(p1, p2, c1, t)\n    \n    return (1-t)^2 .* p1 .+ (2*(1-t)*t) .*c1 .+ (t^2) .* p2\nend\n\nfunction _bezier2(p1, p2, c1, c2, t)\n   x1 = _bezier1(p1, c2, c1, t)\n   x2 = _bezier1(c1, p2, c2, t)\n   return (1-t).* x1 .+ t .* x2\nend\n\n만약 NAJ.jl 을 설치했다면,\nusing NAJ, Plots\n\nc = Bezier([-2, 2], [2, 2], [0, 0])\nd = Bezier([-2, 2], [2, 2], [-1, 0], [1, 0])\nc2 = Bezier([-2, 2], [2, 2], [0, 0], [0, 0])\n\nt =-0.0:0.02:1.0\ns = permutedims(hcat((c.(t))...))\ns2 = permutedims(hcat((d.(t))...))\ns3 = permutedims(hcat((c2.(t))...))\n\nscatter([-2, 2, 0], [2, 2, 0], aspect_ratio = :equal, dpi=300)\nplot!(s[:,1], s[:, 2], label = \"Quadratic\")\nplot!(s2[:,1], s2[:, 2], label = \"Cubic\")\nplot!(s3[:,1], s3[:, 2], label = \"Quadratic2\") \n를 통해 다음 그림을 얻을 수 있을 것이다.\n\n\n\nBezier Curve",
    "crumbs": [
      "수치해석 I",
      "Interlude : 베지에 곡선(Bézier curve)"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06I_Bezier.html#이차-베지에-곡선",
    "href": "src/numerical_analysis_using_julia/06I_Bezier.html#이차-베지에-곡선",
    "title": "Interlude : 베지에 곡선(Bézier curve)",
    "section": "",
    "text": "시작점과 끝점 \\(P_1,\\,P_2\\), 그리고 하나의 제어점 \\(C\\) 가 주어졌다고 하자. \\(t\\in [0,\\,1]\\) 에 대해 \\(Q_1\\) 을 \\(P_1\\) 과 \\(C\\) 의 \\(t\\) 내분점, \\(Q_2\\) 를 \\(C\\) 과 \\(P_2\\) 의 \\(t\\) 내분점이라고 하자. 즉 \\(t\\in [0,\\,1]\\) 에 대해 \\(Q_1 = (1-t)P_1 + tC\\), \\(Q_2=(1-t)C + tP_2\\) 이다. 이 때 \\(P_1,\\,P_2,\\,C\\) 에 의해 정해지는 이차 베지어 곡선 \\(X(P_1, P_2, C, t)\\) 는\n\\[\nX_Q(P_1, P_2, C, t) = (1-t)Q_1 + tQ_2 = (1-t)^2P_1+2t(1-t)C + t^2P_2\n\\]\n로 주어진다.\n\n\n코드\n\\usetikzlibrary{calc}\n\\tikzset{\n  quadratic/.style={\n    to path={\n      (\\tikztostart) .. controls\n      ($#1!1/3!(\\tikztostart)$) and ($#1!1/3!(\\tikztotarget)$)\n      .. (\\tikztotarget)\n    }\n  }\n}\n\\begin{tikzpicture}[scale=1.0]\n\\filldraw [black] (-2, 2) circle (2pt) node[anchor=south] {$P_1$};;\n\\filldraw [black] (2, 2) circle (2pt) node[anchor=south] {$P_2$};\n\\filldraw [blue] (0,0) circle (2pt)node[anchor=north, black] {$C$};\n\\draw[blue,very thick] (-2, 2) to[quadratic={(0, 0)}] (2, 2);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n그림 1: 이차 베지에 곡선\n\n\n\n\n\n\n\n\n\n\n위의 그림은 tikz 로 그렸다. tikz 는 제어점을 하나만 줄 때에도 아래의 사차 베지에 곡선으로 그리기 때문에 베지에 곡선을 그리는 명령어를 사용 하지 않았다.",
    "crumbs": [
      "수치해석 I",
      "Interlude : 베지에 곡선(Bézier curve)"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06I_Bezier.html#사차-베지에-곡선",
    "href": "src/numerical_analysis_using_julia/06I_Bezier.html#사차-베지에-곡선",
    "title": "Interlude : 베지에 곡선(Bézier curve)",
    "section": "",
    "text": "시작점 \\(P_1\\), 끝점 \\(P_2\\), 제어점 \\(C_1\\), \\(C_2\\) 가 주어졌을 때 \\(t\\in [0, 1]\\) 에 대해 \\(P_1,\\,C_2\\) 를 양 끝점으로 하고 \\(C_1\\) 을 제어점으로 하는 이차 베지에 곡선 \\(X_Q(P_1, C_2, C_1, t)\\) 를 생각 할 수 있다. 또한 \\(C_1,\\, P_2\\) 를 양 끝점으로 하고 \\(C_2\\) 를 제어점으로 하는 이차 베지에 곡선 \\(X_Q(P_1, C_2, C_1, t)\\) 를 생각 할 수 있다. 사차 베지에 곡선 \\(X_C(P_1, P_2, C_1, C_2, t)\\) 는 \\(X_Q(P_1, C_2, C_1, t)\\) 와 \\(X_Q(P_1, C_2, C_1, t)\\) 의 \\(t\\) 내분점으로 정의된다. 즉,\n\\[\nX_C(P_1, P_2, C_1, C_2, t) = (1-t) X_Q(P_1, C_2, C_1, t)  + tX_Q(P_1, C_2, C_1, t)\n\\]\n이다.\n\n\n코드\n\\usetikzlibrary{calc}\n\\tikzset{\n  quadratic/.style={\n    to path={\n      (\\tikztostart) .. controls\n      ($#1!1/3!(\\tikztostart)$) and ($#1!1/3!(\\tikztotarget)$)\n      .. (\\tikztotarget)\n    }\n  }\n}\n\\begin{tikzpicture}[scale=1.0]\n\\filldraw [black] (-2, 2) circle (2pt) node[anchor=south] {$P_1$};;\n\\filldraw [black] (2, 2) circle (2pt) node[anchor=south] {$P_2$};\n\\filldraw [blue] (0,0) circle (2pt)node[anchor=north, black] {$C$};\n\\filldraw [magenta] (-1,0) circle (2pt)node[anchor=north, black] {$C_1$};\n\\filldraw [magenta] (1,0) circle (2pt)node[anchor=north, black] {$C_2$};\n\\draw[blue,very thick] (-2, 2) to[quadratic={(0, 0)}] (2, 2);\n\\draw[magenta, very thick] (-2,2) .. controls (-1,0) and (1,0) .. (2,2);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n그림 2: 이차 베지에 곡선",
    "crumbs": [
      "수치해석 I",
      "Interlude : 베지에 곡선(Bézier curve)"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06I_Bezier.html#julia-코드",
    "href": "src/numerical_analysis_using_julia/06I_Bezier.html#julia-코드",
    "title": "Interlude : 베지에 곡선(Bézier curve)",
    "section": "",
    "text": "이차와 사차 베지어 곡선을 다음과 같이 구현하였다. 아래의 코드는 NAJ.jl 에 포함되어 있다.\nstruct Bezier{T} \n    p1::Vector{T}\n    p2::Vector{T}\n    c1::Vector{T}\n    c2::Union{Vector{T}, Nothing}\n\n    function Bezier(\n        p1::AbstractVector{T1}, \n        p2::AbstractVector{T2}, \n        c1::AbstractVector{T3}, \n        c2::Union{AbstractVector{T4}, Nothing}=nothing) where {T1&lt;:Real, T2&lt;:Real, T3&lt;:Real, T4&lt;:Real}\n        \n        @assert length(p1) == length(p2) == length(c1)\n        \n        if c2 === nothing\n            TT = promote_type(eltype(p1), eltype(p2), eltype(c1))\n            return new{TT}(Vector(p1), Vector(p2), Vector(c1), nothing)\n        else \n            @assert length(p1) == length(c2)\n            TT = promote_type(eltype(p1), eltype(p2), eltype(c1), eltype(c2))\n            return new{TT}(Vector(p1), Vector(p2), Vector(c1), Vector(c2))\n        end\n        \n    end\nend\n\nfunction (b::Bezier)(t)\n    if b.c2 === nothing\n        return _bezier1(b.p1, b.p2, b.c1, t)\n    else \n        return _bezier2(b.p1, b.p2, b.c1, b.c2, t)\n    end\nend\n\nfunction _bezier1(p1, p2, c1, t)\n    \n    return (1-t)^2 .* p1 .+ (2*(1-t)*t) .*c1 .+ (t^2) .* p2\nend\n\nfunction _bezier2(p1, p2, c1, c2, t)\n   x1 = _bezier1(p1, c2, c1, t)\n   x2 = _bezier1(c1, p2, c2, t)\n   return (1-t).* x1 .+ t .* x2\nend\n\n만약 NAJ.jl 을 설치했다면,\nusing NAJ, Plots\n\nc = Bezier([-2, 2], [2, 2], [0, 0])\nd = Bezier([-2, 2], [2, 2], [-1, 0], [1, 0])\nc2 = Bezier([-2, 2], [2, 2], [0, 0], [0, 0])\n\nt =-0.0:0.02:1.0\ns = permutedims(hcat((c.(t))...))\ns2 = permutedims(hcat((d.(t))...))\ns3 = permutedims(hcat((c2.(t))...))\n\nscatter([-2, 2, 0], [2, 2, 0], aspect_ratio = :equal, dpi=300)\nplot!(s[:,1], s[:, 2], label = \"Quadratic\")\nplot!(s2[:,1], s2[:, 2], label = \"Cubic\")\nplot!(s3[:,1], s3[:, 2], label = \"Quadratic2\") \n를 통해 다음 그림을 얻을 수 있을 것이다.\n\n\n\nBezier Curve",
    "crumbs": [
      "수치해석 I",
      "Interlude : 베지에 곡선(Bézier curve)"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/05I_interpolation_struct.html",
    "href": "src/numerical_analysis_using_julia/05I_interpolation_struct.html",
    "title": "Interlude : 보간법의 구현",
    "section": "",
    "text": "Interlude : 보간법의 구현\n우리는 앞의 다항식의 구현에서 복합타입을 함수처럼 사용하였다. 보간법의 경우 원래의 데이터와 보간 방식을 입력하면 어떠한 값에 대해서도 보간된 값을 계산 할 수 있다. 우리는 이것을 역시 복합 타입으로 구현하고자 한다.\n복합 타입의 정의는 다음과 같다.\nmutable struct Interpolator1D{T}\n    xp :: Vector{T}\n    yp :: Vector{T}\n    kind :: Symbol\n    bc :: Union{Vector{T}, Nothing}\n    coeffs ::Union{Vector{T}, Nothing}\n\n    function Interpolator1D(\n        xp::AbstractVector{T}, \n        yp::Vector{S}, \n        kind::Symbol, \n        bc::Union{Nothing, Vector}=nothing, \n        ) where {T &lt;: Real, S&lt;: Real}\n        @assert kind ∈ (:nearest, :linear, :cubic, :cubic_spline_naturalbc, :cubic_spline_clampedbc)\n        @assert length(xp) == length(yp)\n        \n        N = promote_type(T, S)\n        xp = convert.(N, xp)\n        yp = convert.(N, yp)\n\n        if kind ∈ (:cubic_spline_naturalbc, :cubic_spline_clampedbc)\n            coeffs = get_cubic_spline_coefficients(xp, yp, kind, bc)\n        else \n            bc = nothing\n            coeffs = nothing\n        end\n        new{N}(Vector(xp), Vector(yp), kind, bc, coeffs)\n    end\nend\nInterpolator1D 자료형의 xp 와 yp 는 보간법에 사용될 원래의 데이터이며, kind 는 보간 방법으로 :nearest(최근접 보간법), :linear (선형 보간법), :cubic (삼차함수 보간법), :cubic_spline_naturalbc (cubic spline with natural boundary condition), :cubic_spline_clampedbc (cubic spline with clamped boundary condition) 을 의미한다. bc 는 경계조건을 의미하며, cubic spline 방법이 아닐 경우 필요가 없으므로, 벡터 혹은 nothing 의 Union 이다.\nCubinc Spline 보간법의 경우 경계조건을 계산하는 함수 get_cubic_spline_coefficients 를 다음과 같이 구현하였다.\n\nfunction get_cubic_spline_coefficients(xp, yp, bc_kind = :cubic_spline_naturalbc, bc=nothing)\n    @assert length(xp) == length(yp)\n    @assert bc_kind ∈ (:cubic_spline_clampedbc, :cubic_spline_naturalbc)\n    @assert (bc === nothing) || (length(bc) == 2)\n    T = eltype(xp)\n    N = length(xp)\n    M = spzeros(T, (4*N-4, 4*N-4))\n    Y = zeros(T, (4*N-4, 1))\n    for i in 1:N-2 \n        M[4*(i-1)+1, (4*(i-1)+1):4*i] = [one(T) xp[i] (xp[i])^2 (xp[i])^3]\n        M[4*(i-1)+2, 4*(i-1)+1:4*i] = [one(T) xp[i+1] (xp[i+1])^2 (xp[i+1])^3]\n        M[4*(i-1)+3, 4*(i-1)+2:4*(i+1)] = [1 2*xp[i+1] 3*(xp[i+1])^2 0 -1 -2*xp[i+1] -3*(xp[i+1])^2]\n        M[4*(i-1)+4, 4*(i-1)+3:4*(i+1)] = [2 6*(xp[i+1]) 0 0 -2 -6*(xp[i+1])]\n        \n        Y[4*(i-1)+1] = yp[i]\n        Y[4*(i-1)+2] = yp[i+1]\n        \n        if i == N-2\n            println(4*N-4, \", \", 4*(i-1)+4)\n        end\n    end\n    \n    M[end-3, end-3:end] = [one(T) xp[end-1] (xp[end-1])^2 (xp[end-1])^3] \n    M[end-2, end-3:end] = [one(T) xp[end] (xp[end])^2 (xp[end])^3] \n    \n    if bc_kind == :cubic_spline_naturalbc\n        \n        M[end-1, 3:4] = [2 6*xp[1]]\n        M[end, end-1:end] = [2 6*xp[end]]\n        Y[end-3] = yp[end-1]\n        Y[end-2]=  yp[end]\n    else\n        M[end-3, end-3:end] = [one(T) xp[end-1] (xp[end-1])^2 (xp[end-1])^3] \n        M[end-2, end-3:end] = [one(T) xp[end] (xp[end])^2 (xp[end])^3] \n        M[end-1, 2:4] = [1 2*xp[1] 3(xp[1])^2]\n        M[end, end-2:end] = [1 2*xp[end] 3(xp[end])^2]\n        Y[end-3] = yp[end-1]\n        Y[end-2]=  yp[end]\n        Y[end-1] = bc[1]\n        Y[end] = bc[2]\n    end\n\n    return (M\\Y)[:,1]\n\nend",
    "crumbs": [
      "수치해석 I",
      "Interlude : 보간법의 구현"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/04I_polynomial.html",
    "href": "src/numerical_analysis_using_julia/04I_polynomial.html",
    "title": "Interlude : 일변수 다항식의 Julia 구현",
    "section": "",
    "text": "수치해석에서는 다항식을 아주 많이 사용한다. 2023년 9월 현재 Julia 언어의 생태계에서는 Polynomials.jl 이 다항식을 다루는 패키지로 널리 사용되고 있지만 여기에서는 수치해석과 julia 언어에 대한 연습으로 다항식을 표현하는 복합 타입(struct type) 을 만들어보자. 그 이름은 일단 SimplePolynomial 로 하도록 하자. 이것은 앞서 소개했던 NAJ.jl 에 포함되어 있다.\nJulia 는 객체 지향 언어가 아니기 때문에 클래스와 클래스의 상속이라는 개념이 존재하지 않으며 대부분 복합 타입이 그 역할을 대신한다. 복합 타입에 대해서는 복합 타입 과 공식 매뉴얼의 Types 를 참고하라. 복합 타입의 가장 큰 장점중의 하나는 타입 변수를 함수처럼 사용 할 수 있다는 것이다. 만약 우리가 계수로 SimplePolynomial 복합 타입을 만들었다면, 이 복합 타입 변수를 함수처럼 사용 할 수 있다. 여기서는 다항식에 대한 복합타입을 정의한 후 이 복합타입을 함수로서 사용하도록 하겠다.\n다항식 함수값의 계산을 위해 julia 는 evalpoly 함수를 제공한다. evalpoly(1.0, [2.0, 3.0]) 은 다항식 \\(p(x)=2.0+3.0x\\) 에 대해 p(1.0) 을 계산해준다. 자세한 것은 evalpoly 에 대한 자체 설명을 확인하라. 일변수 다항식을 구현하는 이유는 단순히 다항식의 함수값을 계산하는 것 이외에 다항식의 덧셈, 뺄셈, 곱셈, 미분, 적분과 기타 앞으로 배울 다항식에 관련된 여러 기능이 필요해서이다. evalpoly 는 이것까지 제공하지는 않는다.\n\n수학적인 의미에서의 다항식은\n\\[\np(x) = \\sum_{k=0}^n a_k x^k\n\\]\n형태의 함수이며 \\(a_k\\) 를 계수(coefficient) 라고 하며 \\(a_k\\ne 0\\) 인 가장 큰 \\(k\\) 값을 다항식 \\(p(x)\\) 의 차수(degree) 라고 한다. 선형대수학적인 입장에서 \\(n\\) 차 이하인 실계수 다항식의 집합 \\(\\mathbb{R}_n[x]\\) 나 복소수 계수 다항식의 집합 \\(\\mathbb{C}_n[x]\\) 는 \\(n+1\\) 차원 벡터공간이며 \\(\\{1,\\,x,\\,x^2,\\ldots,\\,x^n\\}\\) 는 그 기저가 될 수 있다. 다항식 공간의 기저는 다항식이 정의되는 구간과 내적이 정의에 따라 다양한 기저로 정의될 수 있다. 예를 들어 \\([-1,\\,1]\\) 구간에서 정의되며 내적이\n\\[\n\\langle f,\\,g \\rangle = \\int_{-1}^1 f(x)\\, g(x)\\, dx\n\\]\n로 주어졌을 때 르장드르 다항식 \\(P_k(x),\\, k=0,\\,1,\\ldots\\) 은 재귀적으로 아래와 같이 정의된다.\n\\[\n\\begin{aligned}\nP_0(x) &= 1, \\\\\nP_1(x) & = x, \\\\\nP_{k+1}(x) &= \\dfrac{2k+1}{k+1}xP_k(x) - \\dfrac{k}{k+1}P_{k-1}(x).\n\\end{aligned}\n\\tag{1}\\]\n이 때 \\(\\{P_1(x),\\ldots,P_k(x)\\}\\) 는 \\(\\mathbb{R}_k[x]\\) 의 직교기저가 된다. 마찬가지로 아래의 NAJ.jl 에서 제공하는 다항식은 다음과 같다.\n\n\n\n표 1: NAJ.jl 에 포함된 다항식의 기저함수\n\n\n\n\n\n기저 함수 이름\nNAJ.jl 에서의 타입\n다항식\n\n\n\n\n\\(x^k\\)\nSimplePolynomial\n\\(x^k\\)\n\n\n르장드르 다항식\nLegendrePolynoial\n\\(P_k(x)\\)\n\n\n체비세프 다항식\nChevyshevPolynomial\n\\(T_k(x)\\)\n\n\n에르미트 다항식\nHermitePolynomial\n\\(H_k(x)\\)\n\n\n라게르 다항식\nLaguerrePolynomial\n\\(L_k(x)\\)\n\n\n\n\n\n\n\n\n\n일단 다음의 조건을 만족하도록 뼈대를 만들자.\n\n계수를 입력하면 그에 맞는 다항식 복합타입 객체가 생성되도록 한다. 예를 들어 SimplePolynomial([1, 2, 3]) 은 \\(1+2x+3x^3\\) 에 해당하는 다항식 객체를 생성하며 LegendrePolynomial([1, 2, 3]) 은 \\(1P_0(x) + 2 P_1 (x) + 3 P_2 (x)\\) 의 다항식 객체를 생성한다.\n계수의 타입을 타입 매개변수로 받는 복합타입으로 한다.\n복합타입을 함수처럼 사용할 수 있도록 한다.\nBase.show 함수가 이 다항식 객체를 보기 좋게 출력 할 수 있도록 한다.\n\n위의 다항식 객체들은 immutable 한 복합타입으로 구현되었다. 다항식 객체의 계수를 정하여 생성 할 수 있는데 LegendrePolynomial([1, 2, 3]) 는 정수 타입의 계수를 갖는 객체가 생성되며 LegendrePolynomial{Float64}([1, 2, 3]) 는 계수가 Float64 로 변형되어 저장된다. 물론 정수 타입의 계수라도 일반적인 실함수 처럼 사용 할 수 있다. 다음의 예를 보라.\nIn [2]: using NAJ\n\nIn [3]: p = LegendrePolynomial([1,2,3])\nOut[3]: Legendre Polynomial{Int64}(1 P_0(x) + 2 P_1(x) + 3 P_2(x))\n\n\nIn [4]: p(0.2)\nOut[4]: 0.07999999999999985\n\n\nIn [5]: p.([-1.0, 0.0, 1.0])\nOut[5]: 3-element Vector{Float64}:\n  2.0\n -0.5\n  6.0\n\n이 다항식은 특정 기저에 대한 다항식 표현이므로 추상 타입으로 AbstractBasisPolynomial 을 만들었으며 위의 다항식들은 모두 이 AbstractBasisPolynomial 타입의 하위타입이다. 다항식에 대한 정보를 알 수 있는 몇몇 함수를 만들었는데 이 함수들 위의 다항식 타입에 대한 공통이므로 다중 디스패치를 이용하여 AbstractBasisPolynomial 을 인자로 받는 함수로 작성하였다. 일반적인 베열은 length 함수로 원소의 갯수를 알 수 있는데, 다항식에서는 계수의 갯수-1 이 차수가 된다. Base.length 함수에 대한 디스패치를 작성하여 이 함수를 이용하자. 보통 다항식의 차수를 order 혹은 degree 라고 하는데 여기서는 계수의 길이에서 1을 뺀 값과 같다.\nBase.length(p::AbstractBasisPolynomial) = length(p.coeffs)\n\norder(p::AbstractBasisPolynomial) = length(p)-1\ndegree(p::AbstractBasisPolynomial) = order(p)\n\n\n\n\n일반적인 다항식을 구현한 SimplePolynomial 에 대해 알아보자.\nJulia 에서는 많은 타입에 zero 와 one 함수가 구현되어 있다. zero(p) 는 p 타입의 덧셈에한 항등원을 반환하며, one(p) 는 p 와 같은 타입의 곱셈에 대한 항등원을 반환한다. 다항식의 zero 와 one 을 다음과 같이 구현한다.\nfunction Base.zero(a::P) where P&lt;:SimplePolynomial\n    return SimplePolynomial([zero(eltype(a.coeffs)), ])\nend\n\nfunction Base.one(a::P) where P&lt;:SimplePolynomial\n    return SimplePolynomial([one(eltype(a.coeffs)), ])\nend\n\n\n\n이제 SimpllePolynomial 객체 사이에 혹은 SimplePolynomial 객체와 상수간의 +, -, *, / 연산과 단항연산(-) 을 정의하자. 단항 연산 - 는 -3 과 같이 덧셈에 대한 역원으로 변경시키는 연산을 말한다. 다항식과 연산되는 것은 다항식, 혹은 기본 수(number) 타입의 변수로 한다. 다만 나누기 / 의 경우 일단은 다항식/상수 만을 정의한다. 각각의 연산은 Base.:+, Base.:-, Base.:*, Base.:/ 연산자를 다중 디스패치 함으로서 구현한다.\nfunction Base.:-(b::P) where {P&lt;: SimplePolynomial}\n    coeffs = -b.coeffs\n    return SimplePolynomial(coeffs)\nend\n\nfunction Base.:+(a::T, b::SimplePolynomial{P}) where {T &lt;: Number, P &lt;: Number} \n    rT = promote_type(T, P)\n    coeffs = rT.(b.coeffs)\n    coeffs[1] += a\n    return SimplePolynomial(coeffs)\nend\n\nfunction Base.:+(b::SimplePolynomial{P}, a::T) where {P &lt;: Number, T &lt;: Number} \n    return a+b\nend\n\nfunction Base.:+(a::SimplePolynomial{P1}, b::SimplePolynomial{P2}) where {P1 &lt;: Number, P2 &lt;: Number} \n    rT = promote_type(P1, P2)\n    if length(b) &gt; length(a)\n        coeffs = zeros(rT, length(b))\n        coeffs[1:length(a)] = a.coeffs[:]\n        coeffs += b.coeffs\n    else \n        coeffs = zeros(rT, length(a))\n        coeffs[1:length(b)] = b.coeffs[:]\n        coeffs += a.coeffs\n    end\n    return SimplePolynomial(coeffs)\nend\n\nfunction Base.:-(a::SimplePolynomial{P1}, b::SimplePolynomial{P2}) where {P1 &lt;: Number, P2 &lt;: Number} \n    return a + (-b)\nend\n\nfunction Base.:-(b::SimplePolynomial{P}, a::T) where {P &lt;: Number, T &lt;: Number} \n    return b+(-a)\nend\n\nfunction Base.:-(a::T, b::SimplePolynomial{P}) where {T &lt;: Number, P &lt;: Number} \n    return a+(-b)\nend\n\nfunction Base.:*(a::T, b::SimplePolynomial{P}) where {T &lt;: Number, P &lt;: Number} \n    return SimplePolynomial(b.coeffs*a)\nend\n\nfunction Base.:*(b::SimplePolynomial{P}, a::T) where {P &lt;: Number, T &lt;: Number} \n    return SimplePolynomial(b.coeffs*a)\nend\n\nfunction Base.:*(a::SimplePolynomial{P1}, b::SimplePolynomial{P2}) where {P1 &lt;: Number, P2 &lt;:Number} \n    rT = promote_type(P1, P2)\n    ord1, ord2 = degree(a), degree(b)\n    ord = ord1*ord2\n    coef = zeros(rT, ord+2)\n    \n    for i in 0:ord1, j in 0:ord2\n        @inbounds coef[i+j+1] += a.coeffs[i+1]*b.coeffs[j+1]\n    end\n    return SimplePolynomial(coef)\nend\n\nfunction Base.:/(b::SimplePolynomial{P}, a::T) where {P &lt;: Number, T &lt;: Number} \n    return SimplePolynomial(b.coeffs/a)\nend\n\n최고차항의 계수가 1인 다항식을 monic 이라고 한다. 이에 대해서도 구현한다. monic 함수는 다항식의 최고차항의 계수가 a 일 때 전체 계수를 a 로 나누어주어 최고차 항의 계수를 1 로 만든다.\nfunction monic(p::P) where P&lt;:SimplePolynomial\n    return p/p.coeffs[end]\nend\n\n\n\n\n다항식 \\(p(x) = a_0 + a_1 x + \\cdots + a_n x^n\\) 일 때,\n\\[\n\\begin{aligned}\n\\dfrac{d}{dx}p(x) &= a_1 + 2a_2 x + \\cdots + na_n x^{n-1},\\\\\n\\int_x p(t)\\, dt &=a_0 x + \\dfrac{a_1}{2}x^2 + \\cdots + \\dfrac{a_n}{n+1}x^{n+1} + \\text{const.}\n\\end{aligned}\n\\]\n임을 안다. 이것을 이용하여 다항식을 미분하는 함수 derivative 와 정적분 함수 integrate 함수를 구현하였다. integrate 함수의 경우, 다항식만 입력되면 상수항이 0 인 부정적분, 다항식과 수(number) 하나가 입력되면 이 수가 상수항인 부정적분을 반환하며, 다항식과 수 두개가 입력되면 첫번째 수에서 두번째 수 까지의 정적분값을 반환하도록 하였다.\nfunction derivative(p::SimplePolynomial)\n    if length(p) &lt; 2 \n        return SimplePolynomial([one(eltype(p.coeffs)), ])\n    else\n        coeffs = p.coeffs[2:end] .* (1:(length(p)-1))\n        return SimplePolynomial(coeffs)\n    end\nend\n\nfunction integrate(p::SimplePolynomial, a::Union{Nothing, Number}=nothing, b::Union{Nothing, Number}=nothing) \n\n    if eltype(p.coeffs) &lt;: Integer\n        coeffs = zeros(Float64, length(p)+1)\n    else \n        coeffs = zeros(eltype(p.coeffs), length(p)+1)\n    end\n    \n    for i in 1:length(p.coeffs)\n        coeffs[i+1] = p.coeffs[i]/(i)\n    end\n    \n    if a === nothing && b === nothing # 상수항이 0 인 부정적분\n        coeffs[1] = zero(eltype(coeffs))\n        return SimplePolynomial(coeffs)\n    elseif a === nothing || b === nothing # 상수항이 a 혹은 b 로 주어진 부정적분\n        coeffs[1] = a\n        return SimplePolynomial(coeffs)\n    else # a 에서 b 구간 까지의 정적분\n        return evalpoly(b, coeffs) - evalpoly(a, coeffs)\n    end\nend\n\n\n\n\nmonic 다항식 \\(p(x)\\) 의 전체 근이 \\(x_1,\\,x_2,\\ldots,\\,x_n\\) 라고 하면\n\\[\np(x) = c \\prod_{i=1}^n (x-x_i)\n\\]\n이다. 우리는 이미 다항식의 곱셈을 구현했으므로 이를 쉽게 구현 할 수 있다.\nfunction polynomial_from_roots(xp::AbstractVector{T}) where T&lt;:Number \n    return prod([SimplePolynomial([-x0, 1]) for x0 in xp])\nend\n그렇다면,\npp3 = polynomial_from_roots([1.0, 1.0, 2.0])\n에 대해 다음의 출력이 나온다.\nSimplePolynomial(1.0 x^3 - 4.0 x^2 + 5.0 x^1 - 2.0)\n\\((x-1)^2(x-2)= x^3-4x^2+5x-2\\) 이므로 정확한 결과가 나왔다. 이 다항식 복합타입은 앞으로도 계속 개선되며 사용 될 것이다.",
    "crumbs": [
      "수치해석 I",
      "Interlude : 일변수 다항식의 Julia 구현"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/04I_polynomial.html#뼈대",
    "href": "src/numerical_analysis_using_julia/04I_polynomial.html#뼈대",
    "title": "Interlude : 일변수 다항식의 Julia 구현",
    "section": "",
    "text": "일단 다음의 조건을 만족하도록 뼈대를 만들자.\n\n계수를 입력하면 그에 맞는 다항식 복합타입 객체가 생성되도록 한다. 예를 들어 SimplePolynomial([1, 2, 3]) 은 \\(1+2x+3x^3\\) 에 해당하는 다항식 객체를 생성하며 LegendrePolynomial([1, 2, 3]) 은 \\(1P_0(x) + 2 P_1 (x) + 3 P_2 (x)\\) 의 다항식 객체를 생성한다.\n계수의 타입을 타입 매개변수로 받는 복합타입으로 한다.\n복합타입을 함수처럼 사용할 수 있도록 한다.\nBase.show 함수가 이 다항식 객체를 보기 좋게 출력 할 수 있도록 한다.\n\n위의 다항식 객체들은 immutable 한 복합타입으로 구현되었다. 다항식 객체의 계수를 정하여 생성 할 수 있는데 LegendrePolynomial([1, 2, 3]) 는 정수 타입의 계수를 갖는 객체가 생성되며 LegendrePolynomial{Float64}([1, 2, 3]) 는 계수가 Float64 로 변형되어 저장된다. 물론 정수 타입의 계수라도 일반적인 실함수 처럼 사용 할 수 있다. 다음의 예를 보라.\nIn [2]: using NAJ\n\nIn [3]: p = LegendrePolynomial([1,2,3])\nOut[3]: Legendre Polynomial{Int64}(1 P_0(x) + 2 P_1(x) + 3 P_2(x))\n\n\nIn [4]: p(0.2)\nOut[4]: 0.07999999999999985\n\n\nIn [5]: p.([-1.0, 0.0, 1.0])\nOut[5]: 3-element Vector{Float64}:\n  2.0\n -0.5\n  6.0\n\n이 다항식은 특정 기저에 대한 다항식 표현이므로 추상 타입으로 AbstractBasisPolynomial 을 만들었으며 위의 다항식들은 모두 이 AbstractBasisPolynomial 타입의 하위타입이다. 다항식에 대한 정보를 알 수 있는 몇몇 함수를 만들었는데 이 함수들 위의 다항식 타입에 대한 공통이므로 다중 디스패치를 이용하여 AbstractBasisPolynomial 을 인자로 받는 함수로 작성하였다. 일반적인 베열은 length 함수로 원소의 갯수를 알 수 있는데, 다항식에서는 계수의 갯수-1 이 차수가 된다. Base.length 함수에 대한 디스패치를 작성하여 이 함수를 이용하자. 보통 다항식의 차수를 order 혹은 degree 라고 하는데 여기서는 계수의 길이에서 1을 뺀 값과 같다.\nBase.length(p::AbstractBasisPolynomial) = length(p.coeffs)\n\norder(p::AbstractBasisPolynomial) = length(p)-1\ndegree(p::AbstractBasisPolynomial) = order(p)",
    "crumbs": [
      "수치해석 I",
      "Interlude : 일변수 다항식의 Julia 구현"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/04I_polynomial.html#simplepolynomial",
    "href": "src/numerical_analysis_using_julia/04I_polynomial.html#simplepolynomial",
    "title": "Interlude : 일변수 다항식의 Julia 구현",
    "section": "",
    "text": "일반적인 다항식을 구현한 SimplePolynomial 에 대해 알아보자.\nJulia 에서는 많은 타입에 zero 와 one 함수가 구현되어 있다. zero(p) 는 p 타입의 덧셈에한 항등원을 반환하며, one(p) 는 p 와 같은 타입의 곱셈에 대한 항등원을 반환한다. 다항식의 zero 와 one 을 다음과 같이 구현한다.\nfunction Base.zero(a::P) where P&lt;:SimplePolynomial\n    return SimplePolynomial([zero(eltype(a.coeffs)), ])\nend\n\nfunction Base.one(a::P) where P&lt;:SimplePolynomial\n    return SimplePolynomial([one(eltype(a.coeffs)), ])\nend\n\n\n\n이제 SimpllePolynomial 객체 사이에 혹은 SimplePolynomial 객체와 상수간의 +, -, *, / 연산과 단항연산(-) 을 정의하자. 단항 연산 - 는 -3 과 같이 덧셈에 대한 역원으로 변경시키는 연산을 말한다. 다항식과 연산되는 것은 다항식, 혹은 기본 수(number) 타입의 변수로 한다. 다만 나누기 / 의 경우 일단은 다항식/상수 만을 정의한다. 각각의 연산은 Base.:+, Base.:-, Base.:*, Base.:/ 연산자를 다중 디스패치 함으로서 구현한다.\nfunction Base.:-(b::P) where {P&lt;: SimplePolynomial}\n    coeffs = -b.coeffs\n    return SimplePolynomial(coeffs)\nend\n\nfunction Base.:+(a::T, b::SimplePolynomial{P}) where {T &lt;: Number, P &lt;: Number} \n    rT = promote_type(T, P)\n    coeffs = rT.(b.coeffs)\n    coeffs[1] += a\n    return SimplePolynomial(coeffs)\nend\n\nfunction Base.:+(b::SimplePolynomial{P}, a::T) where {P &lt;: Number, T &lt;: Number} \n    return a+b\nend\n\nfunction Base.:+(a::SimplePolynomial{P1}, b::SimplePolynomial{P2}) where {P1 &lt;: Number, P2 &lt;: Number} \n    rT = promote_type(P1, P2)\n    if length(b) &gt; length(a)\n        coeffs = zeros(rT, length(b))\n        coeffs[1:length(a)] = a.coeffs[:]\n        coeffs += b.coeffs\n    else \n        coeffs = zeros(rT, length(a))\n        coeffs[1:length(b)] = b.coeffs[:]\n        coeffs += a.coeffs\n    end\n    return SimplePolynomial(coeffs)\nend\n\nfunction Base.:-(a::SimplePolynomial{P1}, b::SimplePolynomial{P2}) where {P1 &lt;: Number, P2 &lt;: Number} \n    return a + (-b)\nend\n\nfunction Base.:-(b::SimplePolynomial{P}, a::T) where {P &lt;: Number, T &lt;: Number} \n    return b+(-a)\nend\n\nfunction Base.:-(a::T, b::SimplePolynomial{P}) where {T &lt;: Number, P &lt;: Number} \n    return a+(-b)\nend\n\nfunction Base.:*(a::T, b::SimplePolynomial{P}) where {T &lt;: Number, P &lt;: Number} \n    return SimplePolynomial(b.coeffs*a)\nend\n\nfunction Base.:*(b::SimplePolynomial{P}, a::T) where {P &lt;: Number, T &lt;: Number} \n    return SimplePolynomial(b.coeffs*a)\nend\n\nfunction Base.:*(a::SimplePolynomial{P1}, b::SimplePolynomial{P2}) where {P1 &lt;: Number, P2 &lt;:Number} \n    rT = promote_type(P1, P2)\n    ord1, ord2 = degree(a), degree(b)\n    ord = ord1*ord2\n    coef = zeros(rT, ord+2)\n    \n    for i in 0:ord1, j in 0:ord2\n        @inbounds coef[i+j+1] += a.coeffs[i+1]*b.coeffs[j+1]\n    end\n    return SimplePolynomial(coef)\nend\n\nfunction Base.:/(b::SimplePolynomial{P}, a::T) where {P &lt;: Number, T &lt;: Number} \n    return SimplePolynomial(b.coeffs/a)\nend\n\n최고차항의 계수가 1인 다항식을 monic 이라고 한다. 이에 대해서도 구현한다. monic 함수는 다항식의 최고차항의 계수가 a 일 때 전체 계수를 a 로 나누어주어 최고차 항의 계수를 1 로 만든다.\nfunction monic(p::P) where P&lt;:SimplePolynomial\n    return p/p.coeffs[end]\nend\n\n\n\n\n다항식 \\(p(x) = a_0 + a_1 x + \\cdots + a_n x^n\\) 일 때,\n\\[\n\\begin{aligned}\n\\dfrac{d}{dx}p(x) &= a_1 + 2a_2 x + \\cdots + na_n x^{n-1},\\\\\n\\int_x p(t)\\, dt &=a_0 x + \\dfrac{a_1}{2}x^2 + \\cdots + \\dfrac{a_n}{n+1}x^{n+1} + \\text{const.}\n\\end{aligned}\n\\]\n임을 안다. 이것을 이용하여 다항식을 미분하는 함수 derivative 와 정적분 함수 integrate 함수를 구현하였다. integrate 함수의 경우, 다항식만 입력되면 상수항이 0 인 부정적분, 다항식과 수(number) 하나가 입력되면 이 수가 상수항인 부정적분을 반환하며, 다항식과 수 두개가 입력되면 첫번째 수에서 두번째 수 까지의 정적분값을 반환하도록 하였다.\nfunction derivative(p::SimplePolynomial)\n    if length(p) &lt; 2 \n        return SimplePolynomial([one(eltype(p.coeffs)), ])\n    else\n        coeffs = p.coeffs[2:end] .* (1:(length(p)-1))\n        return SimplePolynomial(coeffs)\n    end\nend\n\nfunction integrate(p::SimplePolynomial, a::Union{Nothing, Number}=nothing, b::Union{Nothing, Number}=nothing) \n\n    if eltype(p.coeffs) &lt;: Integer\n        coeffs = zeros(Float64, length(p)+1)\n    else \n        coeffs = zeros(eltype(p.coeffs), length(p)+1)\n    end\n    \n    for i in 1:length(p.coeffs)\n        coeffs[i+1] = p.coeffs[i]/(i)\n    end\n    \n    if a === nothing && b === nothing # 상수항이 0 인 부정적분\n        coeffs[1] = zero(eltype(coeffs))\n        return SimplePolynomial(coeffs)\n    elseif a === nothing || b === nothing # 상수항이 a 혹은 b 로 주어진 부정적분\n        coeffs[1] = a\n        return SimplePolynomial(coeffs)\n    else # a 에서 b 구간 까지의 정적분\n        return evalpoly(b, coeffs) - evalpoly(a, coeffs)\n    end\nend\n\n\n\n\nmonic 다항식 \\(p(x)\\) 의 전체 근이 \\(x_1,\\,x_2,\\ldots,\\,x_n\\) 라고 하면\n\\[\np(x) = c \\prod_{i=1}^n (x-x_i)\n\\]\n이다. 우리는 이미 다항식의 곱셈을 구현했으므로 이를 쉽게 구현 할 수 있다.\nfunction polynomial_from_roots(xp::AbstractVector{T}) where T&lt;:Number \n    return prod([SimplePolynomial([-x0, 1]) for x0 in xp])\nend\n그렇다면,\npp3 = polynomial_from_roots([1.0, 1.0, 2.0])\n에 대해 다음의 출력이 나온다.\nSimplePolynomial(1.0 x^3 - 4.0 x^2 + 5.0 x^1 - 2.0)\n\\((x-1)^2(x-2)= x^3-4x^2+5x-2\\) 이므로 정확한 결과가 나왔다. 이 다항식 복합타입은 앞으로도 계속 개선되며 사용 될 것이다.",
    "crumbs": [
      "수치해석 I",
      "Interlude : 일변수 다항식의 Julia 구현"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/02_notations_and_propositions.html",
    "href": "src/numerical_analysis_using_julia/02_notations_and_propositions.html",
    "title": "수학에 관련된 표기법과 명제들",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]\n수치해석을 위해서는 기본적으로 미적분학/해석학과 선형대수학에 관련된 지식이 필요하다. 이 장은 문서에 걸쳐 사용될 표기법과 명제들을 확인하기 위한 것이다. 수학적 명제들은 학부 1-2 학년 수준의 미적분학과 선형대수학에 포함되는 내용이며 증명은 제시하지 않는다.",
    "crumbs": [
      "수치해석 I",
      "수학에 관련된 표기법과 명제들"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-basic_notations",
    "href": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-basic_notations",
    "title": "수학에 관련된 표기법과 명제들",
    "section": "1 기본적인 개념과 표기법",
    "text": "1 기본적인 개념과 표기법\n\n\n\n\n\n\n\n정의 1 (수) 정수의 집합을 \\(\\mathbb{Z}\\) 라고 한다. 양의 정수의 집합을 \\(\\mathbb{Z}_+\\) 라고 하고 음의 정수의 집합을 \\(\\mathbb{Z}_-\\) 라고 한다. 유리수의 집합을 \\(\\mathbb{Q}\\) 라고 하고 실수의 집합을 \\(\\mathbb{R}\\), 복소수의 집합을 \\(\\mathbb{C}\\) 라고 한다. 실수의 집합과 복소수의 집합은 체(field) 라고 불리우는 사칙연산이 잘 정의되는 집합이다. 체는 \\(\\mathbb{F}\\) 로 표기한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 2 (함수) 집합 \\(X\\) 에서 \\(Y\\) 로의 함수 \\(f\\) 는 \\(f:X\\to Y\\) 라고 표기한다. \\(X\\) 에서 \\(Y\\) 로의 어떤 함수를 의미할때는 \\(X \\mapsto Y\\) 라고 표기한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 3 (크로네커 델타 함수) 정수 \\(i, j\\) 에 대해\n\\[\n\\delta _{ij} = \\left\\{\\begin{array}{ll} 1 \\qquad &; \\text{if } i = j, \\\\ 0 &; \\text{if }i \\ne j \\end{array} \\right.\n\\]\n로 정의된 함수 \\(\\delta_{ij}\\) 를 크로네커 델타 함수라고 한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 4 (유클리드 공간의 표기) \\(n\\) 차원 유클리드 공간 \\(\\mathbb{R}^n = \\{(x_1,\\ldots,\\,x_n): x_i\\in \\mathbb{R},\\,i=1,\\ldots,\\,\\}\\) 을 \\((a_1,\\ldots,\\,a_n)\\) 형식으로 표현 하기도 하고 열벡터 형식, 즉 \\(\\begin{bmatrix} a_1 &\\ldots &a_n\\end{bmatrix}^T\\) 형식으로 로 표기하기로 하자. 즉 유클리드 공간상의 점으로서 사용될 때 두 표기는 동등하다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 5 (행렬) 실수 성분을 갖는 \\(m \\times n\\) 행렬 전체의 집합을 \\(\\mathbb{R}^{m \\times n}\\) 이라고 한다. 복소수 성분을 갖는 \\(m \\times n\\) 행렬 전체의 집합을 \\(\\mathbb{C}^{m \\times n}\\) 라고 하며, 실수/복소수 여부가 일단 고정되기만 하면 상관 없는 경우 \\(\\mathbb{F}^{m \\times n}\\) 라고 표기한다. 행렬 \\(\\boldsymbol{A}\\) 의 \\(i\\) 행 \\(j\\) 열 성분은 \\((\\boldsymbol{A})_{ij}\\) 혹은 \\(A_{ij}\\) 와 같이 표기한다.\n\\(n\\times 1\\) 행렬의 집합을 특히 \\(\\mathbb{F}^n\\) 라고 한다. 실수/복소수 여부가 정해져 있을 경우 \\(\\mathbb{F}\\) 는 \\(\\mathbb{R}\\) 이나 \\(\\mathbb{C}\\) 가 된다. \\(\\boldsymbol{v}\\in \\mathbb{F}^n\\) 의 \\(j\\) 행 성분은 \\(v_j\\) 로 표기한다. \\(\\boldsymbol{A}\\in \\mathbb{F}^{m \\times n}\\) 에 대해 \\(B_{ij} = A_{ji}\\) 인 행렬 \\(\\boldsymbol{B}\\in \\mathbb{F}^{n \\times n}\\) 를 \\(\\boldsymbol{A}\\) 의 전치 행렬 (transposed matrix) 이라고 하며 \\(\\boldsymbol{A}^T\\) 라고 쓴다. \\(\\boldsymbol{A}=\\boldsymbol{A}^T\\) 일 때 \\(\\boldsymbol{A}\\) 를 대칭 행렬(symmetric matrix) 이라고 한다. \\(\\boldsymbol{A}\\) 가 복소행렬일 때 \\(C_{ij} = \\overline{A_{ji}}\\) 인 행렬 \\(\\boldsymbol{C}\\) 를 수반 행렬 (adjoint matrix) 라고 하며 \\(\\boldsymbol{A}^\\ast\\) 라고 쓴다. \\(\\boldsymbol{A}=\\boldsymbol{A}^T\\) 이면 대칭 행렬 이라고 하고 \\(\\boldsymbol{A}=\\boldsymbol{A}^\\ast\\) 이면 에르미트 행렬(Hermitian matrix) 이라고 한다.\n\n\n\n\n\n이미 사용했지만 행렬은 \\(\\boldsymbol{A}\\) 와 같이 기울어진 굵은 글씨체 대문자로 표현한다. 열벡터, 즉 벡터의 행렬표현의 경우 \\(\\boldsymbol{v}\\) 와 같이 기울어진 굵은 글씨체의 소문자로 표현한다. 집합의 경우는 \\(X, A\\) 와 같이 대문자를, 집합의 원소의 경우는 소문자를 사용한다. 학문적인 관례에 따르거나, 혹은 특별한 필요가 있을 경우 언급과 함께 이 원칙에 어긋날 수 있다.",
    "crumbs": [
      "수치해석 I",
      "수학에 관련된 표기법과 명제들"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-summary_of_linear_algebra",
    "href": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-summary_of_linear_algebra",
    "title": "수학에 관련된 표기법과 명제들",
    "section": "2 행렬대수학 정리",
    "text": "2 행렬대수학 정리\n\n\n\n\n\n\n\n정의 6 (벡터공간과 벡터) 어떤 집합 \\(V\\) 와 체 \\(\\mathbb{F}\\) 에 대해 \\(V\\) 의 원소 사이의 덧셈과 \\(\\mathbb{F}\\) 와 \\(V\\) 사이의 곱셈이 정의되어 있으며, 다음이 성립하면 \\(V\\) 를 \\(\\mathbb{F}\\) 위에서의 벡터공간 혹은 \\(\\mathbb{F}\\)-벡터공간 이라 한다.\n\n\\(u,\\,v \\in V ,\\, a\\in \\mathbb{F} \\implies u+v = v+u \\in V\\), \\(av\\in V\\),\n\\(u,\\,v,\\,w \\in V,\\, a,\\, b\\in \\mathbb{F} \\implies (u + v) + w = u+(v+w)\\), \\((ab)v = a(bv)\\),\n\\(\\exists 0_V\\in V\\, \\forall v\\in V,\\, v + 0_V=0_V+v = v\\),\n\\(\\forall v\\in V \\;\\exists w\\in V\\) s.t. \\(v + w = 0_V\\),\n\\(\\forall v \\in V \\implies 1v =v\\),\n\\(u,\\,v \\in V,\\, a,\\, b\\in \\mathbb{F} \\implies a(u+v)=au + av,\\, (a+b)v = av+bv\\).\n\n벡터공간 \\(V\\) 가 정의되었을 때 벡터공간의 원소를 벡터(vector) 라 한다. \\(\\mathbb{R}\\) 위에서의 벡터 공간을 실벡터공간(real vector space) 혹은 \\(\\mathbb{R}\\)-벡터공간, \\(\\mathbb{C}\\) 위에서의 벡터 공간을 복소벡터공간(complex vector space) 혹은 \\(\\mathbb{C}\\)-벡터공간 이라 한다. \\(0_V\\) 는 영벡터라고 불린다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 7 (선형 결합, 선형 독립, 기저) \\(\\mathbb{F}\\)-벡터공간 \\(V\\) 와 \\(v_1,\\ldots,\\,v_m\\in V\\) 에 대해\n\\[\nv = a_1v_1 + \\cdots + a_mv_m ,\\qquad a_1,\\ldots,\\,a_m\\in \\mathbb{F}\n\\tag{1}\\]\n일 때 \\(v\\) 는 \\(\\{v_1,\\ldots,\\,v_m\\}\\) 의 선형 결합이라고 한다. \\(\\text{span}\\, (v_1,\\ldots,\\,v_m)\\) 은 \\(\\{v_1,\\ldots,\\,v_m\\}\\) 의 선형결합인 모든 벡터의 집합이다. 즉,\n\\[\n\\text{span}\\, (v_1,\\ldots,\\,v_m) =\\{a_1v_1 + \\cdots + a_m v_m: a_1,\\ldots,\\,a_m\\in \\mathbb{F}\\}\n\\tag{2}\\]\n이다. \\(\\{v_1,\\ldots,\\,v_m\\}\\) 에 대해\n\\[\na_1v_1 + \\cdots + a_m v_m = 0_V \\implies a_1= \\cdots =a_m=0\n\\]\n일 때 \\(\\{v_1,\\ldots,\\,v_m\\}\\) 이 선형 독립 이라고 한다. 선형 독립이 아닌 벡터의 집합을 선형 종속 이라고 한다.\n\\(\\{v_1,\\ldots,\\,v_m\\}\\) 이 선형 독립이며 \\(\\text{span}\\, (v_1,\\ldots,\\,v_m) = V\\) 일 때 \\(\\{v_1,\\ldots,\\,v_m\\}\\) 를 \\(V\\) 의 기저(basis) 라고 한다. 유한개의 벡터로 이루어진 기저를 갖는 벡터공간을 유한차원 벡터공간이라고 한다. 유한차원 벡터공간이 아닌 벡터공간을 무한차원 벡터공간이라고 한다.\n\n\n\n\n\n\n명제 1 벡터공간의 기저는 항상 찾을 수 있다. 또한 벡터공간의 기저는 유일하지 않다. 그러나 유한차원 벡터공간의 기저에 포함되는 벡터의 갯수는 정해져 있으며 이 갯수를 벡터공간의 차원(dimension) 이라고 한다. 벡터공간 \\(V\\) 의 차원은 \\(\\dim (V)\\) 로 표기한다. \\(\\dim(\\mathbb{F}^n) = n\\) 이다.\n\n\n\n명제 2 벡터공간 \\(V\\) 의 기저 \\(\\mathcal{B}_V = \\{v_1,\\ldots,\\,v_m\\}\\) 에 대해 벡터 \\(v\\in V\\) 를 \\(\\mathcal{B}_V\\) 의 선형결합으로 표현하는 방법은 유일하다.\n\n\n\n\n\n\n\n\n\n정의 8 \\(U\\) 가 벡터공간 \\(V\\) 의 부분집합이며 벡터공간일 때 \\(U\\) 를 \\(V\\) 의 부분공간이라고 한다.\n\n\n\n\n\n\n명제 3 \\(U\\) 가 유한차원 벡터공간 \\(V\\) 의 부분부분공간이면 \\(U\\) 도 유한차원 벡터공간이며 \\(\\dim (U) \\le \\dim (V)\\) 이다. \\(\\dim (U)= \\dim (V)\\) 이면 \\(U=V\\) 이다.\n\n\n\n\n\n\n\n\n\n정의 9 \\(\\mathbb{F}\\)-벡터공간 \\(U,\\,V\\) 에 대해 함수 \\(T:U \\to F\\) 가 다음을 만족하면 선형 사상(linear map) 이라고 한다.\n\\[\n\\forall u_1,\\,u_2\\in U,\\, \\forall c\\in \\mathbb{F}, \\qquad T(u_1+cu_2) =  T(u_1)+cT(u_2).\n\\]\n\\(U \\mapsto V\\) 선형 사상의 집합을 \\(\\mathcal{L}(U, V)\\) 라고 표기한다. 자기 자신으로의 선형사상을 선형 연산자 (linear operator) 라고 하고 \\(\\mathcal{L}(U)\\) 처럼 표현한다.\n\n\n\n\n\n\n명제 4 (선형연산자의 합과 스칼라곱) \\(\\mathbb{F}\\)-벡터공간 \\(U,\\,V\\) 에서의 선형사상의 집합 \\(\\mathcal{L}(U, V)\\) 에 더하기와 스칼라곱 연산을 다음과 같이 정의한다. \\(T,\\,S\\in \\mathcal{L}(U, V)\\) 이고 \\(c\\in \\mathbb{F}\\) 일 때 \\(u\\in U\\) 에 대해\n\\[\n\\begin{aligned}\n(T+S)(u) &= T(u)+S(u), \\\\\n(cT)(u) & =  c(T(u)).\n\\end{aligned}\n\\]\n이렇게 정의된 연산에 대해 \\(\\mathcal{L}(U, V)\\) 는 벡터공간이다.\n\n\n\n명제 5 \\(S\\in \\mathcal{L}(U, V)\\), \\(T\\in \\mathcal{L}(V, W)\\) 일 때 선형사상의 합성 \\(T \\circ S\\) 도 선형사상이다.\n\n\n\n명제 6 \\(T\\in \\mathcal{L}(U, V)\\) 는 \\(U\\) 의 어떤 기저 \\(\\{u_1,\\ldots,\\,u_m\\}\\) 에 대해 \\(T(u_1)=v_1, \\ldots,\\,T(u_m)=v_m\\) 을 만족하는 선형사상은 유일하다.\n\n\n\n명제 7 \\(T\\in \\mathcal{L}(U, V)\\) 일 때 \\(n=\\dim (U), m = \\dim (V)\\) 라고 하자. \\(\\{u_1,\\ldots,\\,u_n\\}\\) 이 \\(U\\) 의 기저이고 \\(\\{v_1,\\ldots,\\,v_m\\}\\) 이 \\(V\\) 의 기저일 때 선형사상 \\(T\\) 는 \\(u_i\\) 에 대해 항상\n\\[\nT (u_i) =  A_{1i}v_1 + \\cdots + A_{mj}v_m  = \\sum_{j=1}^m A_{ji}v_j, \\qquad i=1,\\ldots, n\n\\tag{3}\\]\n꼴이다.\n\n\n\n\n\n\n\n\n\n정의 10 (벡터와 선형 사상의 행렬 표현) \\(T\\in \\mathcal{L}(U, V)\\) 이고 \\(\\mathcal{B}_U = \\{u_1,\\ldots,\\,u_m\\}\\) 과 \\(\\mathcal{B}_V = \\{v_1,\\ldots,\\,v_n\\}\\) 이 각각 \\(U\\) 와 \\(V\\) 의 기저라고 하자. \\(u = a_1 u_1 + a_m u_m\\) 일 때 \\(u\\) 를 \\(m \\times 1\\) 행렬로 \\(\\begin{bmatrix} a_1 & \\cdots & a_m\\end{bmatrix}^T\\) 와 같이 \\(m \\times 1\\) 행렬로 표현하는 것을 \\(u\\) 의 \\(\\mathcal{B}_U\\) 기저에 대한 행렬 표현이라고 하며 \\([u]_{\\mathcal{B}_U}\\) 로 표기한다. 역시 \\(v= b_1 v_1 + \\cdots + b_n v_n\\) 일 때 \\([v]_{\\mathcal{B}_V} = \\begin{bmatrix}b_1 & \\cdots & b_n\\end{bmatrix}^T\\) 이다.\n\\(T\\) 는 \\(\\mathcal{B}_U,\\,\\mathcal{B}_V\\) 에 대해 식 3 와 같이 정의되므로 행렬 형태로 쓸 수 있으며 이것을 \\([T]_{\\mathcal{B}_U, \\mathcal{B}_V}\\) 라고 표기한다. 즉 \\(([T]_{\\mathcal{B}_U, \\mathcal{B}_V})_{ij} = A_{ij}\\) 이다.\n\n\n\n\n\n\n명제 8 정해진 기저에 대해 벡터와 선형사상의 행렬표현은 유일하다.\n\n\n\n명제 9 정의 10 의 정의, 표기법과 일반적인 행렬의 합과 스칼라곱, 행렬간 곱 연산에 대해\n\\[\nT(u) =v \\iff [T]_{\\mathcal{B}_U, \\mathcal{B}_V} [u]_{\\mathcal{B}_U} = [v]_{\\mathcal{B}_V}\n\\]\n가 성립한다. \\(T,\\,S\\in \\mathcal{L}(U, V)\\), \\(u_1,\\,u_2\\in U\\), \\(c\\in \\mathbb{F}\\) 에 대해\n\\[\n\\begin{aligned}\nT(u_1+cu_2) = v &\\iff [T]_{\\mathcal{B}_U,\\mathcal{B}_V}[u_1]_{\\mathcal{B}_U} + c[T]_{\\mathcal{B}_U,\\mathcal{B}_V}[u_2]_{\\mathcal{B}_U} = [v]_{\\mathcal{B}_V}, \\\\\n(T+cS)(u) = v &\\iff ([T]_{\\mathcal{B}_U,\\mathcal{B}_V}+c[S]_{\\mathcal{B}_U,\\mathcal{B}_V})[u]_{\\mathcal{B}_U} = [v]_{\\mathcal{B}_V}, \\\\\n\\end{aligned}\n\\]\n가 성립한다. \\(T\\in \\mathcal{L}(U, V),\\, S\\in \\mathcal{L}(V, W)\\) 이고 \\(\\mathcal{B}_U,\\, \\mathcal{B}_V,\\, \\mathcal{B}_W\\) 가 각각 \\(U, V, W\\) 의 기저 일 때,\n\\[\n(S \\circ T)(u) = w \\iff [S]_{\\mathcal{B}_V, \\mathcal{B}_W} [T]_{\\mathcal{B}_U, \\mathcal{B}_V} [u]_{\\mathcal{B}_U} = [w]_{\\mathcal{B}_W}\n\\]\n가 성립한다.\n\n\n\n명제 10 두 벡터공간 사이에 전단사 함수가 존재할 때 두 벡터공간이 동형 (isomorphic) 이라고 하며, 두 벡터공간 사이의 전단사 함수를 동형 사상(isomorphism) 이라고 한다. 두 벡터공간 \\(U,\\,V\\) 가 동형일 때 \\(U\\cong V\\) 로 표현한다.\n(\\(1\\)) 유한차원 벡터공간 \\(U,\\,V\\) 에 대해 다음이 성립한다.\n\\[\nU \\cong V \\iff \\dim(U) = \\dim (V).\n\\]\n(\\(2\\)) \\(n\\) 차원 \\(\\mathbb{F}\\)-벡터공간과 \\(\\mathbb{F}^n\\) 는 동형이다.\n(\\(3\\)) \\(n\\) 차원 \\(\\mathbb{F}\\)-벡터공간 \\(U\\) 와 \\(m\\) 차원 \\(\\mathbb{F}\\)-벡터공간 \\(V\\) 사이의 선형사상의 집합 \\(\\mathcal{L}(U, V)\\) 와 \\(\\mathbb{F}^{m \\times n}\\) 는 동형이다.\n\n\n\n명제 11 벡터와 선형사상은 그 행렬 표현과 본질적으로 같다.\n\n\n\n\n\n\n\n\n\n정의 11 \\(\\boldsymbol{A}\\in \\mathbb{F}^{m \\times n}\\) 에 대해 \\(\\ker (\\boldsymbol{A})\\), \\(\\text{im}\\, (\\boldsymbol{A})\\), \\(\\text{rank}\\, (\\boldsymbol{A})\\), \\(\\text{nuliity}(\\boldsymbol{A})\\) 는 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\n\\ker (\\boldsymbol{A}) &= \\{\\boldsymbol{v}\\in \\mathbb{R}^n : \\boldsymbol{Av}=\\boldsymbol{0} \\}, \\\\[0.3em]\n\\text{im}\\, (\\boldsymbol{A}) &= \\{\\boldsymbol{Av}: \\boldsymbol{v}\\in \\mathbb{F}^n\\}, \\\\[0.3em]\n\\text{rank}\\, (\\boldsymbol{A}) &= \\dim (\\text{im}\\, (\\boldsymbol{A})), \\\\[0.3em]\n\\text{nullity}\\, (\\boldsymbol{A}) & = \\dim (\\ker (\\boldsymbol{A})).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n명제 12 \\(\\boldsymbol{A}\\in \\mathbb{F}^{m \\times n}\\) 에 대해 \\(\\ker (\\boldsymbol{A})\\) 는 \\(\\mathbb{F}^n\\) 의 부분공간이며 \\(\\text{im}\\, (\\boldsymbol{A})\\) 는 \\(\\mathbb{F}^{m}\\) 의 부분공간이다. 또한 다음이 성립한다.\n\\[\n\\dim (\\mathbb{F}^m) = \\text{rank}\\, (\\boldsymbol{A}) + \\text{nullity}\\, (\\boldsymbol{A}).\n\\]\n\n\n\n\n\n\n\n\n\n정의 12 (행렬식) 행렬식 \\(\\det :\\mathbb{F}^{n \\times n} \\to \\mathbb{F}\\) 은 다음과 같이 정의된 함수이다. \n\\[\n\\det(\\boldsymbol{A}) = \\sum_{\\sigma \\in S_n} \\text{sgn}(\\sigma)\\left(\\prod_{i=1}^n A_{i\\sigma(i)}\\right)\n\\tag{4}\\]\n\n\n\n\n식 4 에 관한 사항들은 선형대수학 책을 참고하라\n\n명제 13 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 와 \\(c\\in \\mathbb{F}\\) 에 대해 다음이 성립한다.\n  (\\(1\\)) \\(\\boldsymbol{A}\\) 의 서로 다른 두 행이나 두 열의 위치를 바꾼 행렬을 \\(\\boldsymbol{A}_1\\) 이라 할 때 \\(\\det(\\boldsymbol{A}_1) = -\\det (\\boldsymbol{A})\\) 이다.\n  (\\(2\\)) \\(\\boldsymbol{A}\\) 의 한 행 혹은 한 열에 상수 \\(c\\) 가 곱해진 행렬을 \\(\\boldsymbol{A}_2\\) 라 할 때 \\(\\det(\\boldsymbol{A}_2) = c\\det(\\boldsymbol{A})\\) 이다.\n  (\\(3\\)) \\(\\boldsymbol{A}\\) 의 한 행이 다른 행의 상수곱이거나 한 열이 다른 열의 상수곱이면 \\(\\det (\\boldsymbol{A}) = 0\\) 이다.\n  (\\(4\\)) \\(\\det(\\boldsymbol{A}^T) = \\det(\\boldsymbol{A})\\) 이다.\n  (\\(5\\)) \\(\\det(\\boldsymbol{AB}) = \\det(\\boldsymbol{A}) \\det(\\boldsymbol{B})\\) 이다.\n\n\n\n\n\n\n\n\n\n정의 13 (항등행렬과 역행렬) \\(n \\times n\\) 행렬을 정사각 행렬(square matrix) 이라고 한다. \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 가 \\(A_{ii}=1\\), \\((i=1,\\ldots,\\,n)\\) 이며 나머지 성분이 \\(0\\) 일 때 항등 행렬 (Identity matrix) 라고 하며 \\(\\boldsymbol{I}_n\\) 으로 표기한다. 행렬의 크기를 구체적으로 표기할 필요가 없을 때는 \\(\\boldsymbol{I}\\) 라고 표기한다.\n\\(\\boldsymbol{X}\\in \\mathbb{F}^{n\\times n}\\) 에 대해 어떤 행렬 \\(\\boldsymbol{Y}\\in \\mathbb{F}^{n\\times n}\\) 가 존재하여 \\(\\boldsymbol{XY}=\\boldsymbol{YX}=\\boldsymbol{I}_n\\) 일 때 \\(\\boldsymbol{Y}\\) 를 \\(\\boldsymbol{X}\\) 의 역행렬 (inverse matrix) 라고 하며 행렬 \\(\\boldsymbol{A}\\) 의 역행렬은 \\(\\boldsymbol{A}^{-1}\\) 로 표기한다. 역행렬이 존재하는 행렬을 가역 행렬 이라고 한다.\n\n\n\n\n\n\n명제 14 \\(\\boldsymbol{A},\\,\\boldsymbol{B}\\in \\mathbb{F}^{n\\times n}\\) 와 \\(c\\in \\mathbb{F}\\) 에 대해 다음이 성립한다.\n  (\\(1\\)) \\((\\boldsymbol{A}^{-1})^{-1} =\\boldsymbol{A}\\),\n  (\\(2\\)) \\((c\\boldsymbol{A})^{-1} = \\dfrac{1}{c} \\boldsymbol{A}^{-1}\\),\n  (\\(3\\)) \\((\\boldsymbol{AB}^{-1}) =\\boldsymbol{B}^{-1}\\boldsymbol{A}^{-1}\\).\n  (\\(4\\)) \\(\\det(\\boldsymbol{A}^{-1}) = \\dfrac{1}{\\det (\\boldsymbol{A})}\\)\n\n\n\n명제 15 정사각 행렬 \\(\\boldsymbol{A}\\) 가 가역행렬일 필요충분조건은 \\(\\det (\\boldsymbol{A}) \\ne 0\\) 이다.\n\n\n\n명제 16 \\(\\boldsymbol{A} \\in \\mathbb{F}^{m\\times n}\\) 과 \\(\\boldsymbol{x}\\in \\mathbb{F}^n\\), \\(\\boldsymbol{y} \\in \\mathbb{F}^m\\), 그리고 가역행렬 \\(\\boldsymbol{S}\\in \\mathbb{F}^{m \\times m}\\) 에 대해 다음이 성립한다.\n\\[\n\\boldsymbol{Ax}=\\boldsymbol{y} \\iff \\boldsymbol{SAx} = \\boldsymbol{Sy}\n\\]\n\n\n\n명제 17 \\(\\{\\boldsymbol{v}_1,\\ldots,\\,\\boldsymbol{v}_n \\}\\) 가 \\(\\mathbb{F}^n\\) 의 기저이며 \\(\\boldsymbol{P}\\in \\mathbb{F}^{n \\times n}\\) 가 가역이면 \\(\\{\\boldsymbol{Pv}_i : i=1,\\ldots,\\,n\\}\\) 도 \\(\\mathbb{F}^n\\) 의 기저이다.\n\n\n\n\n\n\n\n\n\n정의 14 두 정사각 행렬 \\(\\boldsymbol{A},\\, \\boldsymbol{B}\\in  \\mathbb{F}^{n \\times n}\\) 에 대해 어떤 가역 행렬 \\(\\boldsymbol{P}\\in \\mathbb{F}^{n \\times n}\\) 가 존재하여 \\(\\boldsymbol{A}= \\boldsymbol{PBP}^{-1}\\) 일 때 두 행렬을 닮은 행렬 이라고 한다. 행렬 \\(\\boldsymbol{A}\\) 와 가역행렬 \\(\\boldsymbol{P}\\) 에 대해 \\(\\boldsymbol{PAP}^{-1}\\) 로 바꾸는 것을 닮음 변환 (similar transform) 이라고 한다.\n\n\n\n\n\n\n명제 18 닮음 변환은 기저의 변환이다. \\(\\mathcal{B} = \\{u_1,\\ldots,\\,u_n\\}\\), 가 벡터공간 \\(U\\) 의 기저라고 하고 \\(T\\in \\mathcal{L}(U)\\) 에 대해 \\([T]_{\\mathcal{B}} = \\boldsymbol{A}\\) 라고 하자. \\([u_i]_{\\mathcal{B}}= \\hat{\\boldsymbol{e}}_i\\) 이다. \\(U\\) 의 다른 기저 \\(\\mathcal{B}' = \\{u_1', \\ldots,\\, u_n'\\}\\) 에 대해 \\(\\boldsymbol{p}_i = [u'_i]_{\\mathcal{B}}\\) 를 \\(i\\) 번째 열벡터로 갖는 행렬을 \\(\\boldsymbol{P}\\) 라고 하자. 이 때 \\(\\boldsymbol{PAP}^{-1} = [T]_{\\mathcal{B}'}\\) 이다.\n\n\n\n\n\n\n\n\n\n정의 15 (내적과 노름) \\(\\mathbb{F}\\) 벡터공간 \\(V\\) 에서 다음을 만족하는 함수 \\(\\langle \\,, \\,\\rangle : V \\times V \\to \\mathbb{F}\\) 를 내적(inner product) 이라고 한다.\n  (\\(1\\)) \\(\\langle v,v\\rangle \\ge 0\\),\n  (\\(2\\)) \\(\\langle v,v \\rangle = 0 \\iff v=0_V\\),\n  (\\(3\\)) \\(\\langle u + u',v\\rangle = \\langle u, u\\rangle + \\langle u', u\\rangle\\), \\(\\langle u, v + v'\\rangle = \\langle u, v\\rangle + \\langle u, v'\\rangle\\),\n  (\\(5\\)) \\(c\\in \\mathbb{F}\\) 에 대해 \\(\\langle cu,v\\rangle = c\\langle u, v\\rangle\\), \\(\\langle u, cv\\rangle = \\overline{c} \\langle u, v\\rangle\\),\n  (\\(6\\)) \\(\\langle u, v\\rangle = \\overline{\\langle v, u\\rangle}\\). \\(\\mathbb{R}\\)-내적벡터공간인 경우 \\(\\langle u, v\\rangle =\\langle v, u\\rangle\\)\n내적이 부여된 벡터공간을 내적벡터공간 이라고 한다. 두 백터의 내적이 \\(0\\) 일 때 두 벡터는 서로 직교한다(perpendicular, orthogonal) 라고 한다.\n내적벡터 공간에서 거리함수(노름, norm) \\(\\|\\, \\|:V \\to [0, \\infty)\\) 를\n\\[\n\\|v\\|=\\sqrt{\\langle v, v\\rangle}\n\\tag{5}\\]\n로 정의 할 수 있다.\n\n\n\n\n\n\n명제 19 \\(\\mathbb{F}\\)-내적벡터공간의 거리 함수에 대해 다음이 성립한다.\n  (\\(1\\)) \\(\\|v\\|\\ge 0\\),\n  (\\(2\\)) \\(\\|v\\|=0 \\iff v=0_V\\),\n  (\\(3\\)) \\(c \\in \\mathbb{F}\\) 에 대해 \\(\\|cv\\|= |c|\\|v\\|\\),\n  (\\(4\\)) \\(\\|u+v\\| \\le \\|u\\| + \\|v\\|\\).\n\n\n\n명제 20 \\(\\mathbb{F}\\)-내적벡터공간의 내적과 거리함수에 대해 다음이 성립한다.\n  (\\(1\\)) 피타고라스 정리 : \\(\\langle u,v\\rangle = 0 \\iff \\|u+v\\|^2 = \\|u\\|^2 + \\|v\\|^2\\),\n  (\\(2\\)) 코시-슈바르츠 부등식 : \\(| \\langle u,v\\rangle | \\le \\|u\\| \\|v\\|\\),\n  (\\(3\\)) 삼각부등식 : \\(\\|u+ v\\| \\le \\|u\\| + \\|v\\|\\),\n  (\\(4\\)) 평행사변형 공식 : \\(\\|u+v\\|^2 + \\|u-v\\|^2 = 2\\|u\\|^2 + 2\\|v\\|^2\\).\n\n\n\n\n\n\n\n\n\n정의 16 (고유값, 고유벡터, 고유공간) 행렬 \\(\\boldsymbol{A}\\in \\mathbb{F}^{m \\times n}\\) 에 대해 \\(\\boldsymbol{Av}=\\lambda \\boldsymbol{v}\\) 를 만족하는 \\(\\lambda \\in \\mathbb{F}\\) 와 \\(\\boldsymbol{0}\\) 이 아닌 \\(\\boldsymbol{v}\\in \\mathbb{F}^n\\) 가 존재할 때 \\(\\lambda\\) 를 \\(\\boldsymbol{A}\\) 의 고유값 (eigen value) 이라고 하고 \\(\\boldsymbol{v}\\) 를 고유값 \\(\\lambda\\) 에 대한 \\(\\boldsymbol{A}\\) 의 고유 벡터 (eigne vector) 라고 한다. 고유값 \\(\\lambda\\) 를 갖는 고유벡터의 집합과 \\(\\{\\boldsymbol{0}\\}\\) 의 합집합을 고유값 \\(\\lambda\\) 에 대해 \\(\\boldsymbol{A}\\) 의 고유 공간 (eigenspace) 라고 하며, \\(E(\\lambda, \\boldsymbol{A})\\) 라고 표기한다.\n\n\n\n\n\n\n정의 17 (특성다항식) 정사각 행렬 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 에 대해 \\(p_\\boldsymbol{A}(\\lambda) = \\det(\\lambda \\boldsymbol{I}_n -\\boldsymbol{A})\\) 는 \\(\\lambda\\) 에 대한 \\(n\\)-차 다항식으로 행렬 \\(\\boldsymbol{A}\\) 의 특성다항식 (characteristic polynomial) 이라 한다.\n\n\n\n명제 21 \\(\\lambda \\in \\mathbb{F}\\) 가 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n\\times n}\\) 의 고유값이기 위한 필요충분조건은 \\(p_{\\boldsymbol{A}}(\\lambda) =  0\\) 인 것이다.\n\n\n\n명제 22 복소수체에서 정의된 정사각 행렬 \\(\\boldsymbol{A}\\in \\mathbb{C}^{n\\times n}\\) 에 대한 고유값이 항상 존재한다.\n\n\n\n명제 23 \\(\\mathbb{F}\\)-내적 벡터 공간에서 서로 다른 고유값을 갖는 고유벡터들의 집합은 선형독립이다.\n\n\n\n\n\n\n\n\n\n정의 18 (정규직교기저와 표준기저) \\(\\mathcal{B}=\\{v_1,\\ldots,\\,v_n\\}\\) 이 내적벡터공간 \\(V\\) 의 기저이며 \\(\\langle v_i,\\,v_j\\rangle = \\delta_{ij}\\) 일 때 \\(\\mathcal{B}\\) 를 \\(V\\) 의 정규직교기저 (orthonormal basis) 라고 한다. 벡터공간 \\(\\mathbb{F}^n\\) 에 대해 \\(\\hat{\\boldsymbol{e}}_i \\in \\mathbb{F}^n\\) 를 \\((\\hat{\\boldsymbol{e}}_i)_j = \\delta_{ij}\\) 로 정의하자. 이 때 \\(\\{\\hat{\\boldsymbol{e}}_1, \\ldots, \\hat{\\boldsymbol{e}}_n\\}\\) 는 \\(\\mathbb{F}^n\\) 의 정규직교기저이며 특히 표준 기저 (standard basis) 라고 한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 19 (유클리드 공간) \\(\\mathbb{R}^n\\) 벡터공간에 대해 내적이\n\\[\n\\langle \\boldsymbol{x},\\, \\boldsymbol{y} \\rangle = \\sum_{i=1}^n x_i y_i = \\boldsymbol{x}^T\\boldsymbol{y}\n\\]\n로 정의되었을 때 이 \\(\\mathbb{R}^n\\) 을 유클리드 공간 이라고 한다.\n\n\\(\\boldsymbol{e}_i \\in \\mathbb{R}^n\\) 을 \\(i\\) 번째 성분만 \\(1\\) 이며 나머지 성분은 \\(0\\) 인 유클리드 공간의 벡터라고 하자. 이 때 \\(\\mathcal{B}_S = \\{\\boldsymbol{e}_1,\\ldots,\\,\\boldsymbol{e}_n\\}\\) 을 유클리드 공간의 표준기저 (standard basis) 라고 한다.\n\n\n\n\n\n\n명제 24 \\(\\mathbb{F}\\)-벡터공간 \\(V\\) 의 정규직교기저 \\(\\mathcal{B}_V = \\{v_1,\\ldots,\\,v_n\\}\\) 에 대해 \\(v=\\sum_{i} a_i v_i\\) 라면 \\(a_i = \\langle v,\\,v_i\\rangle\\) 이다. 즉,\n\\[\nv = \\sum_{i} \\langle v,\\, v_i\\rangle v_i\n\\]\n이다.\n\n\n\n명제 25 유한차원 내적벡터공간에서는 기저로부터 항상 정규직교기저를 얻을 수 있다. 그중 한 가지 방법이 그람-슈미트(Gram-Schmidt) 방법이다\n\n\n\n명제 26 (슈어 정리 (Schur’s Theorem)) 유한차원 \\(\\mathbb{C}\\)-내적벡터공간 \\(V\\) 에서 정의된 연산자 \\(T\\in \\mathcal{L}(V)\\) 는 어떤 정규직교기저에서 상삼각 행렬로 표현된다.\n\n\n\n명제 27 \\(\\mathbb{R}\\)-내적공간 \\(V\\) 의 정규직교 기저 \\(\\mathcal{B}_V\\) 에 대한 벡터 \\(u, v\\) 의 행렬표현 \\(\\boldsymbol{u} = [u]_{\\mathcal{B}_V},\\, \\boldsymbol{v} = [v]_{\\mathcal{B}_V}\\) 에 대해 \\(\\langle u, v\\rangle = \\boldsymbol{v}^T\\boldsymbol{u}\\) 이다. \\(V\\) 가 \\(\\mathbb{C}\\)-내적공간일 경우 \\(\\langle u, v\\rangle = \\boldsymbol{v}^\\ast\\boldsymbol{u}\\) 이다.\n\n\n\n명제 28 \\(\\boldsymbol{u},\\,\\boldsymbol{v}\\in \\mathbb{F}^n\\), \\(\\boldsymbol{A} \\in \\mathbb{F}^{n \\times n}\\) 와 \\(\\boldsymbol{A}\\) 의 수반행렬 \\(\\boldsymbol{A}^\\ast\\) 에 대해 다음이 성립한다.\n\\[\n\\langle \\boldsymbol{Au}, \\boldsymbol{v}\\rangle = \\langle \\boldsymbol{u}, \\boldsymbol{A}^\\ast \\boldsymbol{v}\\rangle\n\\]\n\n\n\n\n\n\n\n\n\n정의 20 \\(\\boldsymbol{S}\\in \\mathbb{F}^{n\\times n}\\) 가 모든 \\(\\boldsymbol{v}\\in \\mathbb{F}^n\\) 에 대해 \\(\\|\\boldsymbol{Sv}\\| = \\|\\boldsymbol{v}\\|\\) 이면 \\(\\boldsymbol{S}\\) 를 Isometry 라고 한다. \\(\\mathbb{F} =\\mathbb{R}\\) 일 때 직교 행렬(orthogonal matrix) 이라고 하며 \\(\\mathbb{F}=\\mathbb{C}\\) 일 때 유니타리 행렬(unitary matrix) 라고 한다. 직교 행렬에 의한 닮음변환을 직교 변환 (orthogonal transformation) 이라고 하며, 유니타리 행렬에 의한 닮음변환을 유니타리 변환 (unitary transformation) 이라고 한다.\n\n\n\n\n\n\n명제 29 \\(\\boldsymbol{S}\\) 가 Isometry 일 때 다음이 성립한다.\n  (\\(1\\)) \\(\\{\\boldsymbol{v}_1,\\ldots,\\boldsymbol{v}_n\\}\\) 이 정규직교기저일 때 \\(\\{\\boldsymbol{Sv}_1,\\ldots,\\boldsymbol{Sv}_n\\}\\) 도 정규직교기저이다.\n  (\\(2\\)) \\(\\boldsymbol{S}^{-1}= \\boldsymbol{S}^\\ast\\) 이다. 따라서 \\(\\boldsymbol{O}\\) 가 직교행렬이면 \\(\\boldsymbol{O}^{-1}=\\boldsymbol{O}\\) 이다.\n  (\\(3\\)) 두 isometry 의 곱은 isometry 이다.\n\n\n\n\n\n\n\n\n\n정의 21 정사각 행렬 \\(\\boldsymbol{A} \\in \\mathbb{F}^{n \\times n}\\) 가 \\(\\boldsymbol{AA}^\\ast = \\boldsymbol{A}^\\ast\\boldsymbol{A}\\) 일 때 이 행렬을 정규 행렬 (normal matrix) 라고 한다.\n\n\n\n\n\n\n명제 30 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 가 정규행렬일 때 다음이 성립한다.\n  (\\(1\\)) \\(\\boldsymbol{A}\\) 가 정규행렬일 필요충분조건은 모든 \\(\\boldsymbol{v}\\in \\mathbb{F}^n\\) 에 대해 \\(\\|\\boldsymbol{Av}\\|=\\|\\boldsymbol{A}^\\ast\\boldsymbol{v}\\|\\) 이다.\n  (\\(2\\)) \\(\\boldsymbol{v}\\) 가 공유값 \\(\\lambda\\) 를 갖는 \\(\\boldsymbol{A}\\) 의 고유벡터이면 고유값 \\(\\overline{\\lambda}\\) 를 갖는 \\(\\boldsymbol{A}^\\ast\\) 의 고유벡터이다.\n  (\\(3\\)) 서로 다른 고유값을 갖는 \\(\\boldsymbol{A}\\) 의 고유벡터는 직교한다.\n  (\\(4\\)) \\(\\boldsymbol{A}\\) 가 복소행렬일 때 상삼각행렬 꼴은 대각행렬이다.\\(^\\ast\\)\\(^\\ast\\) 슈어 정리 에 의해 항상 상삼각 행렬 꼴로 표현할 수 있음을 안다.\n\n\n\n명제 31 (복소수체에서 정의된 행렬의 스펙트럼 정리) \\(\\boldsymbol{A}\\in \\mathbb{C}^{n \\times n}\\) 일 때 \\(\\boldsymbol{A}^\\ast = \\boldsymbol{A}\\) 인 행렬을 에르미트 행렬이라고 한다. 에르미트 행렬은 정규행렬이다. 에르미트 행렬에 대해 다음이 성립한다.\n  (\\(1\\)) 에르미트 행렬의 고유값은 실수이다.\n  (\\(2\\)) 서로 다른 고유값을 갖는 에르미트 행렬의 고유벡터는 직교한다.\n  (\\(3\\)) 에르미트 행렬의 고유벡터로 정규직교기저를 구성 할 수 있다.\n  (\\(4\\)) 에르미트 행렬은 유니타리 변환에 의해 대각행렬이 된다. 이 대각행렬의 대각성분이 고유값이다.\n\n\n\n명제 32 (실수체에서 정의된 행렬의 스펙트럼 정리) \\(\\boldsymbol{A}\\in \\mathbb{R}^{n \\times n}\\) 일 때 \\(\\boldsymbol{A}^T= \\boldsymbol{A}\\) 인 행렬을 대칭 행렬이라고 한다. 대칭행렬은 정규행렬이다. 대칭행렬에 대해 다음이 성립한다.\n  (\\(1\\)) 서로 다른 고유값을 갖는 대칭행렬의 고유벡터는 직교한다.\n  (\\(3\\)) 대칭행렬의 고유벡터로 정규직교기저를 구성 할 수 있다.\n  (\\(4\\)) 직교행렬은 직교 변환에 의해 대각행렬이 된다. 이 대각행렬의 대각성분이 고유값이다.",
    "crumbs": [
      "수치해석 I",
      "수학에 관련된 표기법과 명제들"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-summary_of_one_variable_calculus",
    "href": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-summary_of_one_variable_calculus",
    "title": "수학에 관련된 표기법과 명제들",
    "section": "3 일변수 미적분학 정리",
    "text": "3 일변수 미적분학 정리\n\n\n\n\n\n\n\n\n정의 22 \\(a&lt;b\\) 일 때 아래와 같이 정의된 \\((a, b)\\) 와 \\([a, b]\\) 를 각각 열린 구간 (open interval), 닫힌 구간 (closed interval) 이라 한다.\n\\[\n\\begin{aligned}\n(a, b) & = \\{x\\in \\mathbb{R} : a&lt;x&lt;b\\}, \\\\\n[a, b] & = \\{x \\in \\mathbb{R} : a \\le x \\le b\\}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n정의 23 실수의 집합 \\(X\\) 에 대해 어떤 \\(M\\in \\mathbb{R}\\) 이 존재하여 \\(x\\in X \\implies x \\le M\\) 일 때 \\(X\\) 는 위로 유계 라고 한다. 어떤 \\(m\\in \\mathbb{R}\\) 이 존재하여 \\(x\\in X \\implies x \\ge m\\) 일 때 \\(X\\) 는 아래로 유계 라고 한다. \\(X\\) 가 위로도 아래로도 유계이면 유계 라고 한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 24 집합 \\(A\\) 에 대해 양의 정수 \\(\\mathbb{Z}_+\\) 에서 \\(A\\) 로의 함수를 수열 (sequence) 라고 한다. \\(a:\\mathbb{Z}_+ \\to A\\) 일 때 \\(a(n)\\) 을 관례적으로 \\(a_n\\) 이라고 쓰며 \\(\\langle a_n \\rangle\\) 은 \\((a_1,\\,a_2,\\ldots,)\\) 를 의미한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 25 (수열의 극한) 수열 \\(\\langle a_n\\rangle = (a_1,\\,a_2,\\ldots)\\) 이 \\(a\\) 로 수렴한다는 것은 임의의 실수 \\(\\epsilon&gt;0\\) 에 대해 어떤 자연수 \\(N\\) 이 존재하여\n\\[\nn&gt;N \\implies |a_n -a |&lt;\\epsilon\n\\]\n을 만족하는 것이며, \\((a_n)\\) 이 \\(a\\) 로 수렴 할 때\n\\[\n\\lim_{n\\to \\infty} a_n = a\n\\]\n라 표기한다. 수열이 수렴하지 않을 때는 발산한다라고 한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 26 (함수의 극한과 연속성) 함수 \\(f:X\\subset \\mathbb{R} \\to \\mathbb{R}\\) 을 생각하자. 함수 \\(f\\) 가 \\(x_0 \\in X\\) 에서 \\(L\\) 로 수렴한다는 것은 임의의 \\(\\epsilon&gt;0\\) 에 대해 어떤 \\(\\delta&gt;0\\) 이 존재하여\n\\[\n|x-x_0 |&lt; \\delta \\implies |f(x)-L|&lt; \\epsilon\n\\]\n인 것이다. 이 때,\n\\[\n\\lim_{x \\to x_0} f(x) = L\n\\]\n이라 쓴다. 만약\n\\[\n\\lim_{x \\to x_0} f(x) = f(x_0)\n\\]\n이면 \\(f\\) 는 \\(x_0\\) 에서 연속이라고 하며, \\(f\\) 가 모든 \\(x\\in X\\) 에 대해 연속일 때 \\(f\\) 를 연속함수라 한다. 어떤 구간 \\(I\\) 에서 연속인 함수의 집합을 \\(C_I\\) 라고 쓴다. 특별히 닫힌 구간 \\([a,\\,b]\\) 에서 정의된 함수 \\(f\\) 에 대해 \\(f\\) 가 \\((a,\\,b)\\) 에서 연속이며 \\(\\lim_{x\\to a+} f(x)=f(a)\\) 이고 \\(\\lim_{x \\to b-} f(x) = f(b)\\) 이면 \\(f\\) 는 \\([a,\\,b]\\) 에서 연속이라고 한다.\n\n\n\n\n\n\n명제 33 (최대최소정리) 함수 \\(f\\) 가 닫힌 구간 \\([a,\\,b]\\) 에서 연속이라면 \\(\\{f(x):x\\in [a,\\,b]\\}\\) 는 최소값과 최대값을 가진다.\n\n\n\n\n\n\n\n\n\n정의 27 (함수의 미분) 함수 \\(f:X\\subset \\mathbb{R} \\to \\mathbb{R}\\) 에 대해 \\(x_0\\) 에서 미분가능하다는 것은 극한\n\\[\n\\lim_{h \\to 0}\\dfrac{f(a+h)-f(a)}{h}\n\\]\n이 존재한다는 것이며 위의 극한값을 \\(f'(a)\\) 라고 쓴다. 함수 \\(f\\) 가 모든 \\(x\\in X\\) 에 대해 미분 가능 할 때 \\(f\\) 를 미분가능함수라고 한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 28 (도함수) 함수 \\(f:X \\subset \\mathbb{R} \\to \\mathbb{R}\\) 이 미분 가능 할 때 \\(f'(x)\\) 는 \\(X\\) 에서 정의된 함수이며 이를 \\(f\\) 의 도함수 혹은 1계 도함수라고 한다. \\(f'(x)\\) 가 미분가능한 함수 일 때 \\(f'(x)\\) 의 도함수를 \\(f''(x)\\) 혹은 \\(f^{(2)}(x)\\) 라고 쓰며 \\(f\\) 의 2계 도함수라고 한다. 같은 방법으로 \\(f\\) 함수에 대한 \\(n\\) 계 도함수를 정의할 수 있으며 \\(f^{(n)}(x)\\) 라고 쓴다. 여러 계수의 도함수를 나열할 경우 원래의 함수를 \\(f^{(0)}\\), 1 계 도함수를 \\(f^{(1)}\\) 로 쓴다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 29 (\\(C^n_I\\)) 구간 \\(I\\) 에서 \\(n\\) 번 미분 가능하며, 그 \\(n\\) 계 도함수가 연속인 함수의 집합을 \\(C^n_I\\) 라고 한다. 구간이 실수 전체일 경우는 \\(C^n\\) 이라고 한다. \\(e^x\\) 함수의 경우와 같이 실수 전체 영역에서 무한번 미분 가능한 함수의 집합을 \\(C^{\\infty}\\) 라고 한다.\n\n\n\n\n\n\n명제 34 (롤의 정리 (Roll’s theorem)) \\(f\\in C[a,\\,b]\\) 이고 \\(f\\) 가 \\((a,\\,b)\\) 구간에서 미분가능하다고 하자. \\(f(a)=f(b)\\) 이면 \\(f'(c)=0\\) 을 만족하는 \\(c\\in (a,\\,b)\\) 가 존재한다.\n\n\n\n명제 35 (일반화된 롤의 정리) \\(f\\in C^{n-2}[a,\\,b]\\) 이고 \\(f\\) 가 \\((a,\\,b)\\) 구간에서 \\(n-1\\) 번 미분 가능하며 \\(n\\) 개의 서로 다른 \\(x_1,\\ldots,\\,x_n\\) 에서 \\(f(x_i)=0\\) 일 때, \\(f^{(n-1)}(\\xi) = 0\\) 인 \\(\\xi\\in (a,\\,b)\\) 가 존재한다.\n\n\n\n명제 36 (사잇값 정리 (Intermediate value theorem)) \\([a,\\,b]\\) 를 포함하는 구간에서 연속인 함수 \\(f:X\\to \\mathbb{R}\\) 에 대해 \\(d\\) 가 \\(f(a)\\) 와 \\(f(b)\\) 사이의 값이면, 즉 \\(f(a)&lt; d &lt; f(b)\\) 이거나 \\(f(b)&lt;d &lt;f(a)\\) 라면 \\[\nf(c) = d\n\\]\n인 \\(c\\) 가 \\((a,\\,b)\\) 안에 항상 존재한다.\n\n\n\n명제 37 (따름정리 : 여러 점의 경우) \\([a,\\,b]\\) 를 포함하는 구간에서 연속인 함수 \\(f:X \\to \\mathbb{R}\\) 에 대해 \\(\\{x_1,\\ldots,\\,x_n\\}\\subset [a,\\,b]\\) 일 때\n\\[\nf(c) = \\dfrac{f(x_1)+ \\cdots + f(x_n)}{n}\n\\]\n을 만족하는 \\(c\\in [a,\\,b]\\) 가 항상 존재한다.\n\n\n\n명제 38 (평균값 정리 (Mean value theorem)) 함수 \\(f:X \\subset\\mathbb{R} \\to \\mathbb{R}\\) 이 \\([a,\\,b]\\) 를 포함하는 구간에서 에서 연속이고, \\((a,\\,b)\\) 에서 미분 가능하면 \\[\n\\dfrac{f(b)-f(a)}{b-a}=f'(c)\n\\]\n를 만족하는 \\(c\\in (a,\\,b)\\) 가 존재한다.\n\n\n\n명제 39 (일변수 함수에 대한 테일러 정리) \\(a\\) 를 포함하는 열린구간 \\(I\\) 에서 정의된 함수 \\(f:I \\to \\mathbb{R}\\) 가 \\(n+1\\) 번 미분 가능한 함수일 때 임의의 \\(x\\in I\\) 에 대하여\n\\[\nf(x) = \\sum_{k=0}^{n} \\dfrac{f^{(k)}(a)}{k!} (x-a)^k + \\dfrac{f^{(n+1)}(\\xi)}{(n+1)!}(x-a)^{n+1}\n\\]\n을 만족하는 \\(\\xi \\in I\\) 가 존재한다. 이 때 \\(\\displaystyle \\sum_{k=0}^{n} \\dfrac{f^{(k)}}{k!} (x-a)^k\\) 를 \\(n\\) 차 테일러 다항식이라고 하며, \\(\\displaystyle \\dfrac{f^{(n+1)}(\\xi)}{(n+1)!}(x-a)^{n+1}\\) 를 나머지라고 한다.\n\n\n\n\n\n\n\n\n\n정의 30 구간 \\([a,\\,b]\\) 에서 정의된 함수 \\(f\\) 를 생각하자. 수열 \\(\\langle x_n\\rangle\\) 가 \\(x_1=a&lt;x_2&lt;\\cdots &lt;x_n=b\\) 를 만족할 때,\n\\[\n\\begin{aligned}\n&\\lim_{n \\to \\infty} \\sum_{k=2}^n \\min(\\{f(x_{i-1}),\\, f(x_{i})\\}) (x_{i}-x_{i-1})  \\\\\n&\\qquad \\qquad \\qquad = \\lim_{n \\to \\infty} \\sum_{k=2}^n \\max(\\{f(x_{i-1}),\\, f(x_{i})\\}) (x_{i}-x_{i-1})\n\\end{aligned}\n\\]\n라면 함수 \\(f\\) 는 \\([a,\\,b]\\) 구간에서 리만 적분 가능하다고 하며 위의 값을\n\\[\n\\int_{a}^b f \\, dx\n\\] 라고 쓴다.\n\n\n\n\n\n\n명제 40 \\([a,\\,b]\\) 구간에서 연속인 함수는 리만 적분 가능하다.\n\n\n\n명제 41 (미적분학의 제 1 기본 정리) \\(f:[a,b] \\to \\mathbb{R}\\)가 리만 적분 가능한 함수일 때, 함수 \\(g : [a,\\,b] \\to \\mathbb{R}\\) 을 다음과 같이 정의하자.\n\\[\ng(x) = \\int_a^x f(t)\\, dt.\n\\]\n이 때, \\(g(x)\\) 는 \\((a,\\,b)\\) 에서 미분 가능한 함수이며 원래 주어진 함수 \\(f(x)\\) 와는 다음의 관계를 만족힌다.\n\\[\n\\dfrac{d}{dx}g(x) = f(x).\n\\]\n이 경우 \\(g(x)\\) 를 \\(f(x)\\) 의 부정 적분(indefinite integral) 이라 한다.\n\n\n\n명제 42 \\(f[a,\\,b] \\to \\mathbb{R}\\) 이 리만적분 가능한 함수이며, \\(a'\\in [a,\\,b]\\) 라 하자. 이 때\n\\[\ng(x) = \\int_a^x f(t)\\, dt,\\qquad h(x) = \\int_{a'}^x f(t)\\, dt\n\\]\n라고 하면,\n\\[\ng(x)-h(x) = \\text{const}\n\\]\n이다.\n\n\n\n명제 43 (미적분학의 제 2 기본 정리) \\([a,\\,b]\\) 구간에서 적분가능한 함수 \\(f(x)\\) 의 부정적분이 \\(F(x)\\) 일 때 다음이 성립한다.\n\\[\n\\int_a^b f(x)\\, dx = F(b)-F(a).\n\\]\n\n\n\n명제 44 (적분에 대한 평균값 정리 (Mean value theorem for integrals)) 함수 \\(f:X \\subset\\mathbb{R} \\to \\mathbb{R}\\) 이 \\([a,\\,b]\\) 를 포함하는 구간에서 에서 연속이면 \\[\nf(c) = \\dfrac{1}{b-a}\\int_a^b f(x),\\,dx\n\\]\n를 만족하는 \\(c\\in (a,\\,b)\\) 가 존재한다.\n\n\n\n명제 45 (적분에 대한 가중 평균 정리 (Weighted mean value theorems for integrals)) 함수 \\(f:X \\subset\\mathbb{R} \\to \\mathbb{R}\\), \\(g:Y \\subset\\mathbb{R} \\to \\mathbb{R}\\) 이 \\([a,\\,b]\\) 를 포함하는 구간에서 에서 연속이면\n\\[\n\\int_a^b f(x)\\,g(x)\\, dx = f(c)\\int_a^b g(x)\\, dx\n\\]\n을 만족하는 \\(c\\in (a,\\,b)\\) 가 존재한다.",
    "crumbs": [
      "수치해석 I",
      "수학에 관련된 표기법과 명제들"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-summary_of_multy_variable_calculus",
    "href": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-summary_of_multy_variable_calculus",
    "title": "수학에 관련된 표기법과 명제들",
    "section": "4 다변수 미적분학 정리",
    "text": "4 다변수 미적분학 정리\n\n\n\n\n\n\n\n정의 31 (\\(n\\)-cell) \\(\\mathbb{R}^n\\) 에서\n\\[\n[a_1,\\,b_1]\\times \\cdots \\times [a_n,\\,b_n] = \\{(x_1,\\ldots,\\,x_n)\\in \\mathbb{R}^n : a_i \\le x_i \\le b_i,\\, i=1,\\ldots,n\\}\n\\]\n를 \\(n\\)-cell 이라고 한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 32 (다변수 함수의 연속) \\(\\mathbb{R}^n\\) 에서의 열린 집합 \\(U\\) 에서 정의된 함수 \\(F:U \\to \\mathbb{R}^m\\) 이\n\\[\n\\lim_{\\boldsymbol{h} \\to \\boldsymbol{0}} \\|F(\\boldsymbol{p} + \\boldsymbol{h}) - F(\\boldsymbol{p})\\|  = 0\n\\]\n일 때 \\(F\\) 는 \\(\\boldsymbol{p}\\) 에서 연속 이라고 한다. \\(F\\) 가 모든 \\(\\boldsymbol{p}\\in U\\) 에서 연속일 때 \\(F\\) 를 \\(U\\) 에서 연속이라고 한다.\n\n\n\n\n\n\n\n\n\n\n\n\n정의 33 (다변수 함수의 미분과 자코비 행렬) \\(\\mathbb{R}^n\\) 에서의 열린 집합 \\(U\\) 에서 정의된 함수 \\(F:U \\to \\mathbb{R}^m\\) 이 \\(\\boldsymbol{p}\\in U\\) 에서 어떤 행렬 \\(M\\in \\mathbb{R}^{m \\times n}\\) 에 대해\n\\[\n\\lim_{\\boldsymbol{h} \\to \\boldsymbol{0}} \\dfrac{\\|F(\\boldsymbol{p} + \\boldsymbol{h}) - F(\\boldsymbol{p}) - \\boldsymbol{Mp}\\|}{\\|\\boldsymbol{h}\\|} = \\boldsymbol{0}\n\\]\n를 만족 할 때 \\(F\\) 는 \\(\\boldsymbol{p}\\) 에서 미분가능하다 라고 한다. 이때 \\(\\boldsymbol{M}\\) 을 \\(\\boldsymbol{p}\\) 에서의 \\(F\\) 의 자코비 행렬(Jacobian matrix) 이라고 하며 \\(DF(\\boldsymbol{p}),\\, \\boldsymbol{J}_F(\\boldsymbol{p})\\) 옥은 \\(F'(\\boldsymbol{p})\\) 라고 표기한다. \\(F\\) 가 모든 \\(\\boldsymbol{p}\\in U\\) 에서 미분 가능일 때 \\(F\\) 를 \\(U\\) 에서 미분가능하다 라고 한다.\n\n\n\n\n\n\n명제 46 \\(\\mathbb{R}^n\\) 에서의 열린 집합 \\(U\\) 에서 정의된 함수 \\(F:U \\to \\mathbb{R}^m\\) 에 대해 다음은 동치이다.\n  (\\(1\\)) \\(F\\) 는 \\(\\boldsymbol{p}\\in U\\) 에서 미분가능하다.\n  (\\(2\\)) 어떤 \\(\\boldsymbol{M}\\in \\mathbb{R}^{m \\times n}\\) 과 \\(\\displaystyle \\lim_{\\boldsymbol{h}\\to \\boldsymbol{0}}\\dfrac{R(\\boldsymbol{h})}{\\|\\boldsymbol{h}\\|} = \\boldsymbol{0}\\) 인 어떤 함수 \\(R:U \\to \\mathbb{R}^m\\) 에 대해 \\[\nF(\\boldsymbol{p}+\\boldsymbol{h}) = \\boldsymbol{Mp}+R(\\boldsymbol{v})\n\\]     이다.\n\n\n\n정리 1 \\(F : U\\subset \\mathbb{R}^n \\to \\mathbb{R}^m\\) 가 \\(\\boldsymbol{p}\\in U\\) 에서 미분가능할 때 임의의 \\(\\boldsymbol{u}\\in \\mathbb{R}^n-\\{\\boldsymbol{0}\\}\\) 에 대해 아래의 극한이 존재한다.\n\\[\n\\lim_{t \\to \\infty} \\dfrac{F(\\boldsymbol{p}+t\\boldsymbol{u}) - F(\\boldsymbol{p})}{t}\n\\tag{6}\\]\n\n\n\n\n\n\n\n\n\n정의 34 (방향미분과 편미분) \\(\\boldsymbol{u}\\) 가 단위벡터, 즉 \\(\\|\\boldsymbol{u}\\|=1\\) 일 때 식 6 의 극한을 \\(F\\) 의 \\(\\boldsymbol{p}\\) 에서의 \\(\\boldsymbol{u}\\) 방향으로의 방향미분 (directional derivative) 라고 하고 \\(D_\\boldsymbol{u}F(\\boldsymbol{p})\\) 라고 쓴다. \\(\\boldsymbol{u}\\) 가 어떤 표준기저 \\(\\hat{\\boldsymbol{e}}_i\\) 일 때의 방향미분을 편미분 (partial derivative) 라고 하며 \\(D_{\\hat{\\boldsymbol{e}}_i} F(\\boldsymbol{p})\\) 를 \\(D_i F(\\boldsymbol{p})\\), \\(\\dfrac{\\partial F}{\\partial x_i}(\\boldsymbol{p})\\), 혹은 \\(\\partial_i F(\\boldsymbol{p})\\) 라고 표기한다.\n\n\n\n\n\n\n명제 47 \\(F : U \\subset \\mathbb{R}^n \\to \\mathbb{R}^m\\) 이 \\(\\boldsymbol{p}\\in U\\) 에서 미분가능이면 \\(\\boldsymbol{p}\\) 에서 연속이다.",
    "crumbs": [
      "수치해석 I",
      "수학에 관련된 표기법과 명제들"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-transcription_ruls",
    "href": "src/numerical_analysis_using_julia/02_notations_and_propositions.html#sec-transcription_ruls",
    "title": "수학에 관련된 표기법과 명제들",
    "section": "5 이 책에 사용할 행렬 표기의 규칙들",
    "text": "5 이 책에 사용할 행렬 표기의 규칙들\n수학과 프로그래밍 전체에서 행렬이 많이 사용된다. 여기서 행렬을 표현하는 데 사용한 몇가지 규칙을 나열한다. (J-3) 과 같은 항목은 Julia 의 규칙이며 (M-2) 와 같은 항목은 수학적인 표현에 대한 규칙이다. 두 표현의 번호가 같은 경우는 수학적인 표현에 대한 Julia 구현에 대한 규칙 혹은 설명을 의미한다.\n(J-0) 특별한 언급이 없을 경우 Julia 에서 정수형과 실수형은 각각 Int64, Float64 를 사용한다.\n(J-1) 기본적으로 코드상에서 행렬은 A, B 와 같은 대문자 혹은 A1 과 같은 대문자로 시작하는 변수명을 사용한다. Julia 에서 행렬 A 의 \\(i\\) 번째 행벡터는 A[i,:] 로, \\(j\\) 번째 열벡터는 A[:,j] 로 표현 할 수 있으므로 그대로 사용한다. 행렬 A 의 \\(i\\) 행 \\(j\\) 열 성분은 A[i, j] 이다. 벡터와 스칼라는 x, y, a, b 와 같은 소문자나 x_effective, b2 와 같이 소문자로 시작하는 변수명을 사용한다. x 가 벡터인 경우 \\(i\\) 번째 성분은 x[i] 이다.\n(J-2) 행렬 A 에 대한 전치행렬은 transpose(A), 수반행렬은 A' 나 adjoint(A) 로 얻을 수 있다.\n(J-5) Julia 에서는 Matlab의 eye() 나 numpy 의 np.identity() 혹은 np.eye() 와 같은 \\(n\\times n\\) 항등행렬을 위한 함수가 존재하지 않는다. 대신 LinearAlgebra 모듈의 uniform scaling operator I 를 임의의 크기와 임의의 원소 타입의 항등행렬로 사용할 수 있다. 타입과 크기는 다른 행렬과 계산할하는 등 필요할 때 정해진다.\nIn [1]: I+[2 3;4 2]\nOut[1]: 2×2 Matrix{Int64}:\n 3  3\n 4  3\n\nIn [2]: I-[1 2 3;3 4 5; 6 7 8]\nOut[2]: 3×3 Matrix{Int64}:\n  0  -2  -3\n -3  -3  -5\n -6  -7  -7\n필요할 경우에는 항등행렬을 아래와 같이 type을 직접 선언하여 만들 수 있다.\nMatrix{Int32}(I, 3, 3)      # Int32 타입의 3x3 항등행렬\nMatrix{Float64}(I, 5, 5)    # Float64 타입의 5x5 항등행렬\nMatrix{Bool}(I, 4, 4)       # Bool 타입의 4x4 항등행렬\n\\(\\mathbb{R}^{5 \\times 1}\\) 의 단위행렬 \\(\\boldsymbol{e}_3\\) 는 다음과 같이 얻을 수 있다.\ne_3 = Matrix{Float64}(I, 5, 5)[:,3]\n(M-6) \\(m\\times n\\) 행렬 \\(\\boldsymbol{A}\\) 와 \\(m \\times k\\) 행렬 \\(\\boldsymbol{B}\\) 에 대해(두 행렬의 행의 갯수가 같음에 주의하라)) \\([\\boldsymbol{A}\\; \\boldsymbol{B}]\\) 는 \\(\\boldsymbol{A}\\) 행렬 옆에 \\(\\boldsymbol{B}\\) 행렬을 두는 \\(m \\times (n+k)\\) 행렬을 의미한다. \\(m\\times n\\) 행렬 \\(\\boldsymbol{A}\\) 와 \\(l \\times n\\) 행렬 \\(\\boldsymbol{C}\\) 에 대해 (두 행렬의 열의 갯수가 동일함에 유의하라) \\([\\boldsymbol{A} \\,; \\boldsymbol{C}]\\) 는 \\(\\boldsymbol{A}\\) 행렬 아래에 \\(\\boldsymbol{C}\\) 행렬을 두는 \\((m+l)\\times n\\) 행렬이다. 아마 \\(\\begin{bmatrix} \\boldsymbol{A} \\\\ \\boldsymbol{C} \\end{bmatrix}\\) 와 동일하다고 하면 이해하기 쉬울 것이다. ; 기호는 julia 에서 열바뀜을 나타내므로 julia 와 어느정도 유사성이 있다 하겠다. 여기에 \\(l\\times k\\) 행렬 \\(\\boldsymbol{D}\\) 에 대해 \\(\\begin{bmatrix} \\boldsymbol{A} & \\boldsymbol{B} \\\\ \\boldsymbol{C} & \\boldsymbol{D} \\end{bmatrix}\\) 가 의미하는 것은 여러분이 쉽게 유추 할 수 있을 것이다.\n(J-6) Julia 에서는 다음과 같이 사용 할 수 있다.\nIn [26]: A = [1 3;2 4];B=[3 2; 4 3];[A B]\nOut[26]: 2×4 Matrix{Int64}:\n 1  3  3  2\n 2  4  4  3\n(M-7) \\(m\\times n\\) 행렬 \\(\\boldsymbol{A}\\) 와 \\(m \\times k\\) 행렬 \\(\\boldsymbol{B}\\) 혹은 \\(m\\) 차원 벡터 \\(\\boldsymbol{b}\\) 에 대해 \\([\\boldsymbol{A} \\mid \\boldsymbol{B}]\\) 혹은 \\([\\boldsymbol{A} \\mid \\boldsymbol{b}]\\) 는 행렬에 대한 연산을 동시에 적용하기 위한 개념이다. 연산 \\(\\hat{L}[\\boldsymbol{A} \\mid \\boldsymbol{B}] = [\\hat{L}\\boldsymbol{A} \\mid \\hat{L}\\boldsymbol{B}]\\) 를 의미한다.",
    "crumbs": [
      "수치해석 I",
      "수학에 관련된 표기법과 명제들"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet_and_image.html#행렬-기반-변환",
    "href": "src/image_processing/wavelet_and_image.html#행렬-기반-변환",
    "title": "웨이블릿 변환을 이용한 이미지 처리",
    "section": "1 행렬 기반 변환",
    "text": "1 행렬 기반 변환\n\n1차원 변환\n일차원 데이터 \\(\\boldsymbol{f} = \\begin{bmatrix} f_1 & \\cdots & f_N\\end{bmatrix}^N\\) 를 다음과 같이 변환하여 \\(\\boldsymbol{t}=\\begin{bmatrix} t_1 & \\cdots & t_N\\end{bmatrix}^T\\) 로 변환한다고 하자.\n\\[\nt_u = \\sum_{i=1}^N r(u,\\,i) f_i\n\\]\n1차원 푸리에 변환도 위와 같은 형태의 변환이며, 이 때 \\((u,\\,i)\\) 성분이 \\(r(u,\\,i)\\) 인 행렬 \\(\\boldsymbol{A}\\) 를 생각 할 수 있으며,\n\\[\n\\boldsymbol{t}=\\boldsymbol{Af}\n\\]\n이다. 많은 경우 \\(\\boldsymbol{A}\\) 가 직교 행렬, 즉 \\(\\boldsymbol{AA}^T=\\boldsymbol{I}\\) 인데 이 경우 역변환은 다음과 같다.\n\\[\n\\boldsymbol{f} = \\boldsymbol{A}^{-1}\\boldsymbol{t} = \\boldsymbol{A}^T\\boldsymbol{t}\n\\]\n\n\n\n2차원 변환\n이미지를 이미지로 변환하는 경우 원본 \\(f[i,\\,j]\\) 와 변환된 \\(t[u,\\,v]\\) 사이에 다음의 관계가 성립한다고 하자.\n\\[\n\\begin{aligned}\nt[u,\\,v] = \\sum_{i=1}^N \\sum_{j=1}^N r(u, v, i, j)\\,  f[i, j], \\\\\nf[i,\\,j] = \\sum_{u=1}^N \\sum_{v=1}^N s(u, v, i, j) \\, t[u, v].\n\\end{aligned}\n\\]\n이 경우 \\(s(u, v, i, j)\\) 는 \\(r(u, v, i, j)\\) 의 역변환이다. \\(r(u, v, i, j)\\) 가 두 1차원 변환의 곱으로 다음과 같이 표현될 수 있을 때 \\(r\\) 을 분리가능하다고 한다.\n\\[\nr(u, v, i, j) = r_1(u, i)\\, r_2(v, j)\n\\]\n또한 \\(r_1=r_2\\) 이면 \\(r\\) 을 대칭변환 이라고 한다. \\(t[u,\\,v]=\\boldsymbol{T}\\), \\(r_1[i, u]=\\boldsymbol{A}\\), \\(f[i,\\,j]=\\boldsymbol{F}\\) 라고 표현하면,\n\\[\nt[u,\\,v] = \\sum_{i, j} r_1[u, i] r_1[v, j] f[i, j]\n\\]\n이므로\n\\[\n\\boldsymbol{T}=\\boldsymbol{AFA}^T\n\\]\n이다. \\(\\boldsymbol{A}\\) 가 직교행렬이라면\n\\[\n\\boldsymbol{F} = \\boldsymbol{A}^T\\boldsymbol{TA}\n\\]\n이다.\n\n\n\n\\(\\mathbb{C}^n\\) 에서의 2차원 변환\n\\(\\boldsymbol{s}_k,\\,\\boldsymbol{s}_l\\in \\mathcal{M}_{N}(\\mathbb{C})\\) 의 내적은\n\\[\n\\langle \\boldsymbol{s}_k,\\,\\boldsymbol{s}_l\\rangle = \\overline{\\langle \\boldsymbol{s}_l,\\, \\boldsymbol{s}_k\\rangle} = \\boldsymbol{s}_l^\\ast \\boldsymbol{s}_k\n\\]\n이다. 복소벡터공간에서 거리를 보존하는 변환은 유니타리 변환이며 유니타리 변환에 대한 행렬은 유니타리 행렬로 \\(\\boldsymbol{A}^{-1} = \\boldsymbol{A}^\\ast\\) 인 행렬을 의미한다. 따라서 두 정사각 이미지 \\(\\boldsymbol{T}\\) 와 \\(\\boldsymbol{F}\\) 사이에\n\\[\n\\boldsymbol{T}= \\boldsymbol{AFA}^T\n\\]\n의 관계가 성립하며 \\(\\boldsymbol{A}\\) 가 유니타리라면\n\\[\n\\boldsymbol{F} = \\boldsymbol{A}^{\\ast} \\boldsymbol{TA}^{\\ast T}\n\\]\n가 성립한다.\n\n\n\n이중직교성\n내적벡터공간의 기저 \\(\\{v_1,\\, v_2,\\ldots\\}\\) 의 각각의 \\(v_i\\) 에 대해 어떤 \\(\\widetilde{v}_i\\) 가 존재하여 \\(i\\ne j\\) 이면 \\(\\langle v_i,\\, \\widetilde{v}_j\\rangle = 0\\) 일 때 \\(\\{v_1,\\,v_2,\\ldots\\}\\) 를 이중직교기저 라고 한다. 만약\n\\[\n\\langle v_i,\\, \\widetilde{v}_j\\rangle = \\delta_{ij}\n\\]\n이면 이중정규직교기저 라고 한다. \\(\\widetilde{v}_i = v_i,\\, i=1,\\,2,\\ldots\\) 이면 우리가 아는 정규직교기저이다.\n\n\n\nCorrelation\n이미 Convolution 과 Corellation 에서 정의한 correlation 은 복소함수 공간이나 복소벡터공간에서 다음과 같다.\n\\[\n(f \\otimes g)(x) = \\int_{-\\infty}^\\infty f^\\ast (t)\\, g(t+x)\\, dt\n\\]\n이로부터\n\\[\n(f \\otimes g)(0) = \\int_{-\\infty}^\\infty f^\\ast (t)\\, g(t)\\, dt = \\langle g,\\, f\\rangle = \\overline{\\langle f,\\, g\\rangle}\n\\]\n임을 안다. 즉 두 벡터 사이의",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "웨이블릿 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet_and_image.html#wavelet-변환",
    "href": "src/image_processing/wavelet_and_image.html#wavelet-변환",
    "title": "웨이블릿 변환을 이용한 이미지 처리",
    "section": "2 Wavelet 변환",
    "text": "2 Wavelet 변환\n\nScaling functions\nfather scaling function \\(\\varphi(x)\\) 에 대해\n\\[\n\\varphi_{j,\\,k} (x) = 2^{j/2} \\varphi(2^j x-k)\n\\]\n를 생각하자. 이 함수는 \\(\\varphi(x)\\) 에 대해 \\(x\\) 축으로 \\(2^j\\) 만큼 축소되어 있으며 \\(k\\) 만큼 이동되어 있다. 고정된 \\(j=j_0\\) 에 대해",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "웨이블릿 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/wavelet_transform.html",
    "href": "src/image_processing/wavelet/wavelet_transform.html",
    "title": "웨이블릿 변환",
    "section": "",
    "text": "정의 1 (Wavelet) \\(\\psi\\in L^2(\\mathbb{R})\\) 이 \\(\\hat{\\psi}=\\mathfrak{F}[\\psi]\\) 에 대해 다음을 만족할 때 \\(\\psi\\) 를 웨이블릿(wavelet) 이라고 한다.\n\\[\nC_\\psi := {\\Large \\int}_{-\\infty}^\\infty \\dfrac{\\left| \\hat{\\psi}(\\omega)\\right|^2}{|\\omega|} \\, d\\omega &lt; \\infty.\n\\tag{1}\\]\n\n\n\n\n\n\n명제 1 식 1 을 admissibility condition 이라고 한다. 이 정의로부터 다음을 알 수 있다.\n  (\\(1\\)) \\(\\displaystyle \\lim_{\\omega \\to 0} \\hat{\\psi}(\\omega) = 0\\).\n  (\\(2\\)) \\(\\hat{\\psi}\\) 가 연속함수라면 \\(\\hat{\\psi}(0)=0\\) 이어야 한다.\n  (\\(3\\)) \\(\\displaystyle 0 = \\hat{\\psi}(0) = \\int_{-\\infty}^\\infty \\psi (t)\\, dt\\) 이다. 즉 \\(\\psi\\) 의 평균값은 \\(0\\) 이다.\n  (\\(4\\)) \\(\\displaystyle \\lim_{|\\omega| \\to \\infty }  \\hat{\\psi}(\\omega) = 0\\) 이다.\n\n\n\n\n\n\n\n\n\n정의 2 (웨이블릿의 \\(k\\)-차 모멘트) \\(\\psi\\in L^2(\\mathbb{R})\\) 이 웨이블릿일 때 아래와 같이 정의된 \\(m_k\\) 를 \\(k\\)-차 모멘트 라고 한다.\n\\[\nm_k := \\int_{-\\infty}^\\infty t^k \\psi(t)\\, dt.\n\\tag{2}\\]\n\\(m_0 = m_1 = \\cdots = m_k=0\\) 일 때 \\(\\psi\\) 는 \\(k\\)-vanishing moment 를 가진다고 한다.\n\n\n\n\n\n\n명제 2 정의 2 에서 \\(m_k=0\\) 인 것의 필요충분조건은 다음과 같다.\n\\[\n\\left[\\dfrac{d^k \\hat{\\psi}(\\omega)}{d\\omega^k}\\right]_{\\omega = 0} = 0.\n\\]\n\n\n(증명). 푸리에 변환의 수학적 성질 (\\(7\\)) 로부터\n\\[\n\\left[\\dfrac{d^k \\hat{\\psi}(\\omega)}{d\\omega^k}\\right]_{\\omega = 0} = (-i)^k \\mathfrak{F}\\left[t^k \\psi(t)\\right](\\omega = 0) = (-i)^k \\int_{-\\infty}^\\infty t^n \\psi(t)\\, dt = (-i)^k m_k\n\\]\n이다. \\(\\square\\)\n\n\n웨이블릿은 보통 1차부터 특정 \\(n\\) 차 까지의 모멘트가 \\(0\\) 이다. \\(m_k=0\\) 이 되는 가장 큰 \\(k\\) 값은 웨이블릿의 성질을 파악하는 중요한 값이다. \\(K = \\min \\{ k\\in \\mathbb{Z}_+: m_k \\ne 0\\}\\) 이라고 하자. \\(\\hat{\\psi}\\) 를 \\(\\omega=0\\) 근처에서 테일러 전개 하면\n\\[\n\\hat{\\psi}(\\omega) \\approx (-i)^K \\dfrac{m_K}{K!} \\omega^K + O(\\omega^{k+1})\n\\]\n이다.\n\n\n\n예제 1 (Harr 웨이블릿) Haar 웨이블릿은 헝가리의 수학자 Alfred Haar 에 의해 도입된 최초의 웨이블릿으로 아래와 같이 정의된다.\n\\[\n\\psi_H (t) = \\left\\{\\begin{array}{ll} 1, \\qquad & 0, \\le t &lt; \\dfrac{1}{2} \\\\ -1 & \\dfrac{1}{2} \\le t &lt; 1 \\\\ 0, & \\text{otherwise}.\\end{array}\\right.\n\\tag{3}\\]\n이로부터 다음을 쉽게 알 수 있다.\n\\[\n\\int_{-\\infty}^\\infty \\psi_H (t)\\, dt = 0, \\qquad \\int_{-\\infty}^\\infty |\\psi_H(t)|^2 = 1.\n\\]\n웨이블릿의 푸리에 변환 \\(\\mathfrak{F}[\\psi_H](\\omega)\\) 는 다음과 같다.\n\\[\n\\begin{aligned}\n\\hat{\\psi}_H(\\omega) &= \\int_{0}^{1/2} e^{-i \\omega t} \\, dt - \\int_{1/2}^1 e^{-i\\omega t}\\, dt = i e^{-i\\omega/2} \\dfrac{\\sin^2(\\omega/4)}{\\omega/4}\n\\end{aligned}\n\\tag{4}\\]\nadmissibility condition 을 확인해보자.\n\\[\n{\\Large \\int}_{-\\infty}^\\infty \\dfrac{\\left| \\hat{\\psi}_H(\\omega)\\right|^2}{|\\omega|} \\, d\\omega = \\int_{-\\infty}^\\infty 16|\\omega|^{-3} \\left|\\sin^4 \\left(\\dfrac{\\omega}{4}\\right)\\right|\\, d\\omega \\le \\int_{-\\infty}^\\infty \\dfrac{16}{|\\omega|^3}\\, d\\omega &lt;\\infty\n\\]\n\n\n\n\n\n\n그림 1: Haar 웨이블릿과 그 푸리에 변환\n\n\n\n그림 1 에서 보듯이 Haar wavelet 은 시간 도메인에서 잘 국소화 되어 있지만 진동수 도메인에서는 잘 국소화 되어 있지 않다. 또한 푸리에 변환인 \\(\\hat{\\psi}_H(\\omega)\\) 는 \\(\\omega\\) 가 커짐에 따라 \\(\\omega\\) 에 빈비례하므로 진동수 도메인에서 컴팩트 지지 함수가 아니다. \\(\\psi(t)\\) 가 연속함수가 아니므로 \\(\\omega \\to \\infty\\) 극한에서 충분히 빨리 \\(0\\) 이 되지 않는데 이것으로 인해 Haar 웨이블릿은 실제로 사용하는데 약점이 있지만 일반적인 웨이블릿의 성질을 파악하는데 가장 기초적인 웨이블릿이다.\n\n\n\n\n정리 1 \\(\\psi\\) 가 웨이블릿이고 \\(\\phi\\) 가 유계인 적분가능 함수라면 \\(\\psi \\ast \\phi\\) 는 웨이블릿이다.\n\n\n(증명). 우선 다음을 보자. \\(f,\\,g\\in L^2(\\mathbb{R})\\) 이라면\n\\[\n\\infty &gt; \\int_{-\\infty}^\\infty |f(t)|^2\\, dt\n\\]\n이므로 \\(|f|\\in L^2(\\mathbb{R})\\) 이다. 코시-슈바르츠 부등식에 따라\n\\[\n\\left(\\int_{-\\infty}^\\infty |f(t)g(t)|\\, dt\\right)^2 \\le {\\int_{-\\infty}^\\infty |f(t)|^2\\, dt}{\\int_{-\\infty}^\\infty |g(t')|^2\\, dt'}\n\\]\n이다. 다음의 식에는 이것이 사용된다. 다음을 보자.\n\\[\n\\begin{aligned}\n\\int_{-\\infty}^\\infty \\left|(\\psi \\ast \\phi)(t)\\right|^2\\,  dt &= \\int_{-\\infty}^\\infty \\left|\\int_{-\\infty}^\\infty \\psi(x-t)\\,\\phi(x) \\, dx\\right|^2\\, dt \\\\\n& \\le \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty \\left|\\psi(x-t)\\right|\\,\\left|\\phi(x)\\right| \\, dx\\right]^2\\, dt \\\\\n&= \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty |\\psi(x-t) |\\, |\\phi(x)|^{1/2} |\\phi(x)|^{1/2}\\, dx\\right]^2\\, dt \\\\\n&\\le \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty |\\psi(x-t) |^2\\, |\\phi(x)| \\, dx \\int_{-\\infty}^\\infty |\\phi(y)|\\, dy\\right]\\, dt \\\\\n&=\\int_{-\\infty}^\\infty |\\phi(y)|\\, dy \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty |\\psi(x-t) |^2\\, |\\phi(x)| \\, dx \\right]\\, dt  \\\\\n&= \\left(\\int_{-\\infty}^\\infty |\\phi(y)|\\, dy \\right)^2 \\int_{-\\infty}^\\infty |\\psi(t) |^2\\, dt &lt;\\infty\n\\end{aligned}\n\\]\n따라서 \\(\\psi \\ast \\phi \\in L^2(\\mathbb{R})\\) 이다. \\(\\phi\\) 가 유계인 적분가능함수이므로 \\(\\left|\\hat{\\phi}(\\omega)\\right|&lt;\\le M\\) 인 양수 \\(M\\) 이 존재한다.\n\\[\n\\begin{aligned}\n{\\Large \\int_{-\\infty}^\\infty} \\dfrac{\\left|\\mathfrak{F}[\\psi \\ast \\phi](\\omega)\\right|^2}{|\\omega|}\\, d\\omega &= {\\Large \\int_{-\\infty}^\\infty} \\dfrac{\\left| \\hat{\\psi}(\\omega) \\hat{\\phi}(\\omega) \\right|^2 }{\\omega} \\, d\\omega = {\\Large \\int_{-\\infty}^\\infty} \\dfrac{\\left| \\hat{\\psi}(\\omega) \\right|^2 }{\\omega} \\left| \\hat{\\phi}(\\omega)\\right|^2\\, d\\omega  \\\\\n&\\le {\\Large \\int_{-\\infty}^\\infty} \\dfrac{\\left| \\hat{\\psi}(\\omega) \\right|^2 }{\\omega}  \\sup \\left\\{\\left|\\hat{\\phi}(\\omega)\\right|^2 \\right\\}\\, d\\omega \\le \\infty\n\\end{aligned}\n\\]\n이므로 \\(\\psi \\ast \\phi\\) 도 웨이블릿이다. \\(\\square\\)\n\n\n정리 1 은 일단 하나의 웨이블릿이 존재한다면 이로부터 무수히 많은 웨이블릿을 만들 수 있다는 것을 보장한다. 유계인 적분 가능함수 \\(\\phi\\) 만 찾고 웨이블릿 \\(\\psi\\) 에 대해 \\(\\psi \\ast \\phi\\) 만 해 주면 된다.\n\n\n\n예제 2 \\(\\phi (t)\\) 를 다음과 같이 정의하자.\n\\[\n\\phi(t) = \\left\\{\\begin{array}{ll} 0, \\qquad & t&lt;0, \\\\ 1, & 0 \\le t \\le 1, \\\\ 0 \\ge 1. \\end{array}\\right.\n\\]\nHaar 웨이블릿 \\(\\psi_H\\) 와 \\(\\phi\\) 의 합성곱 \\(\\psi_H \\ast \\phi\\) 는 아래 그림 그림 3 과 같다.\n\n\n\n\n\n\n그림 2: \\(\\psi_H \\ast \\phi (t)\\) 웨이블릿\n\n\n\n\n\n\n\n\n예제 3 \\(\\phi (t) = e^{-t^2}\\) 와의 합성곱은\nHaar 웨이블릿 \\(\\psi_H\\) 와 \\(\\phi\\) 의 합성곱 \\(\\psi_H \\ast \\phi\\) 는 아래 그림 그림 3 과 같다.\n\n\n\n\n\n\n그림 3: \\(\\psi_H \\ast \\phi (t)\\) 웨이블릿\n\n\n\n\n\n\n\n\n\n그림 4: \\(\\mathfrak[F](\\psi_H \\ast \\phi)(t)\\)",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "웨이블릿 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/wavelet_transform.html#정의와-기본적인-성질",
    "href": "src/image_processing/wavelet/wavelet_transform.html#정의와-기본적인-성질",
    "title": "웨이블릿 변환",
    "section": "",
    "text": "정의 1 (Wavelet) \\(\\psi\\in L^2(\\mathbb{R})\\) 이 \\(\\hat{\\psi}=\\mathfrak{F}[\\psi]\\) 에 대해 다음을 만족할 때 \\(\\psi\\) 를 웨이블릿(wavelet) 이라고 한다.\n\\[\nC_\\psi := {\\Large \\int}_{-\\infty}^\\infty \\dfrac{\\left| \\hat{\\psi}(\\omega)\\right|^2}{|\\omega|} \\, d\\omega &lt; \\infty.\n\\tag{1}\\]\n\n\n\n\n\n\n명제 1 식 1 을 admissibility condition 이라고 한다. 이 정의로부터 다음을 알 수 있다.\n  (\\(1\\)) \\(\\displaystyle \\lim_{\\omega \\to 0} \\hat{\\psi}(\\omega) = 0\\).\n  (\\(2\\)) \\(\\hat{\\psi}\\) 가 연속함수라면 \\(\\hat{\\psi}(0)=0\\) 이어야 한다.\n  (\\(3\\)) \\(\\displaystyle 0 = \\hat{\\psi}(0) = \\int_{-\\infty}^\\infty \\psi (t)\\, dt\\) 이다. 즉 \\(\\psi\\) 의 평균값은 \\(0\\) 이다.\n  (\\(4\\)) \\(\\displaystyle \\lim_{|\\omega| \\to \\infty }  \\hat{\\psi}(\\omega) = 0\\) 이다.\n\n\n\n\n\n\n\n\n\n정의 2 (웨이블릿의 \\(k\\)-차 모멘트) \\(\\psi\\in L^2(\\mathbb{R})\\) 이 웨이블릿일 때 아래와 같이 정의된 \\(m_k\\) 를 \\(k\\)-차 모멘트 라고 한다.\n\\[\nm_k := \\int_{-\\infty}^\\infty t^k \\psi(t)\\, dt.\n\\tag{2}\\]\n\\(m_0 = m_1 = \\cdots = m_k=0\\) 일 때 \\(\\psi\\) 는 \\(k\\)-vanishing moment 를 가진다고 한다.\n\n\n\n\n\n\n명제 2 정의 2 에서 \\(m_k=0\\) 인 것의 필요충분조건은 다음과 같다.\n\\[\n\\left[\\dfrac{d^k \\hat{\\psi}(\\omega)}{d\\omega^k}\\right]_{\\omega = 0} = 0.\n\\]\n\n\n(증명). 푸리에 변환의 수학적 성질 (\\(7\\)) 로부터\n\\[\n\\left[\\dfrac{d^k \\hat{\\psi}(\\omega)}{d\\omega^k}\\right]_{\\omega = 0} = (-i)^k \\mathfrak{F}\\left[t^k \\psi(t)\\right](\\omega = 0) = (-i)^k \\int_{-\\infty}^\\infty t^n \\psi(t)\\, dt = (-i)^k m_k\n\\]\n이다. \\(\\square\\)\n\n\n웨이블릿은 보통 1차부터 특정 \\(n\\) 차 까지의 모멘트가 \\(0\\) 이다. \\(m_k=0\\) 이 되는 가장 큰 \\(k\\) 값은 웨이블릿의 성질을 파악하는 중요한 값이다. \\(K = \\min \\{ k\\in \\mathbb{Z}_+: m_k \\ne 0\\}\\) 이라고 하자. \\(\\hat{\\psi}\\) 를 \\(\\omega=0\\) 근처에서 테일러 전개 하면\n\\[\n\\hat{\\psi}(\\omega) \\approx (-i)^K \\dfrac{m_K}{K!} \\omega^K + O(\\omega^{k+1})\n\\]\n이다.\n\n\n\n예제 1 (Harr 웨이블릿) Haar 웨이블릿은 헝가리의 수학자 Alfred Haar 에 의해 도입된 최초의 웨이블릿으로 아래와 같이 정의된다.\n\\[\n\\psi_H (t) = \\left\\{\\begin{array}{ll} 1, \\qquad & 0, \\le t &lt; \\dfrac{1}{2} \\\\ -1 & \\dfrac{1}{2} \\le t &lt; 1 \\\\ 0, & \\text{otherwise}.\\end{array}\\right.\n\\tag{3}\\]\n이로부터 다음을 쉽게 알 수 있다.\n\\[\n\\int_{-\\infty}^\\infty \\psi_H (t)\\, dt = 0, \\qquad \\int_{-\\infty}^\\infty |\\psi_H(t)|^2 = 1.\n\\]\n웨이블릿의 푸리에 변환 \\(\\mathfrak{F}[\\psi_H](\\omega)\\) 는 다음과 같다.\n\\[\n\\begin{aligned}\n\\hat{\\psi}_H(\\omega) &= \\int_{0}^{1/2} e^{-i \\omega t} \\, dt - \\int_{1/2}^1 e^{-i\\omega t}\\, dt = i e^{-i\\omega/2} \\dfrac{\\sin^2(\\omega/4)}{\\omega/4}\n\\end{aligned}\n\\tag{4}\\]\nadmissibility condition 을 확인해보자.\n\\[\n{\\Large \\int}_{-\\infty}^\\infty \\dfrac{\\left| \\hat{\\psi}_H(\\omega)\\right|^2}{|\\omega|} \\, d\\omega = \\int_{-\\infty}^\\infty 16|\\omega|^{-3} \\left|\\sin^4 \\left(\\dfrac{\\omega}{4}\\right)\\right|\\, d\\omega \\le \\int_{-\\infty}^\\infty \\dfrac{16}{|\\omega|^3}\\, d\\omega &lt;\\infty\n\\]\n\n\n\n\n\n\n그림 1: Haar 웨이블릿과 그 푸리에 변환\n\n\n\n그림 1 에서 보듯이 Haar wavelet 은 시간 도메인에서 잘 국소화 되어 있지만 진동수 도메인에서는 잘 국소화 되어 있지 않다. 또한 푸리에 변환인 \\(\\hat{\\psi}_H(\\omega)\\) 는 \\(\\omega\\) 가 커짐에 따라 \\(\\omega\\) 에 빈비례하므로 진동수 도메인에서 컴팩트 지지 함수가 아니다. \\(\\psi(t)\\) 가 연속함수가 아니므로 \\(\\omega \\to \\infty\\) 극한에서 충분히 빨리 \\(0\\) 이 되지 않는데 이것으로 인해 Haar 웨이블릿은 실제로 사용하는데 약점이 있지만 일반적인 웨이블릿의 성질을 파악하는데 가장 기초적인 웨이블릿이다.\n\n\n\n\n정리 1 \\(\\psi\\) 가 웨이블릿이고 \\(\\phi\\) 가 유계인 적분가능 함수라면 \\(\\psi \\ast \\phi\\) 는 웨이블릿이다.\n\n\n(증명). 우선 다음을 보자. \\(f,\\,g\\in L^2(\\mathbb{R})\\) 이라면\n\\[\n\\infty &gt; \\int_{-\\infty}^\\infty |f(t)|^2\\, dt\n\\]\n이므로 \\(|f|\\in L^2(\\mathbb{R})\\) 이다. 코시-슈바르츠 부등식에 따라\n\\[\n\\left(\\int_{-\\infty}^\\infty |f(t)g(t)|\\, dt\\right)^2 \\le {\\int_{-\\infty}^\\infty |f(t)|^2\\, dt}{\\int_{-\\infty}^\\infty |g(t')|^2\\, dt'}\n\\]\n이다. 다음의 식에는 이것이 사용된다. 다음을 보자.\n\\[\n\\begin{aligned}\n\\int_{-\\infty}^\\infty \\left|(\\psi \\ast \\phi)(t)\\right|^2\\,  dt &= \\int_{-\\infty}^\\infty \\left|\\int_{-\\infty}^\\infty \\psi(x-t)\\,\\phi(x) \\, dx\\right|^2\\, dt \\\\\n& \\le \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty \\left|\\psi(x-t)\\right|\\,\\left|\\phi(x)\\right| \\, dx\\right]^2\\, dt \\\\\n&= \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty |\\psi(x-t) |\\, |\\phi(x)|^{1/2} |\\phi(x)|^{1/2}\\, dx\\right]^2\\, dt \\\\\n&\\le \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty |\\psi(x-t) |^2\\, |\\phi(x)| \\, dx \\int_{-\\infty}^\\infty |\\phi(y)|\\, dy\\right]\\, dt \\\\\n&=\\int_{-\\infty}^\\infty |\\phi(y)|\\, dy \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty |\\psi(x-t) |^2\\, |\\phi(x)| \\, dx \\right]\\, dt  \\\\\n&= \\left(\\int_{-\\infty}^\\infty |\\phi(y)|\\, dy \\right)^2 \\int_{-\\infty}^\\infty |\\psi(t) |^2\\, dt &lt;\\infty\n\\end{aligned}\n\\]\n따라서 \\(\\psi \\ast \\phi \\in L^2(\\mathbb{R})\\) 이다. \\(\\phi\\) 가 유계인 적분가능함수이므로 \\(\\left|\\hat{\\phi}(\\omega)\\right|&lt;\\le M\\) 인 양수 \\(M\\) 이 존재한다.\n\\[\n\\begin{aligned}\n{\\Large \\int_{-\\infty}^\\infty} \\dfrac{\\left|\\mathfrak{F}[\\psi \\ast \\phi](\\omega)\\right|^2}{|\\omega|}\\, d\\omega &= {\\Large \\int_{-\\infty}^\\infty} \\dfrac{\\left| \\hat{\\psi}(\\omega) \\hat{\\phi}(\\omega) \\right|^2 }{\\omega} \\, d\\omega = {\\Large \\int_{-\\infty}^\\infty} \\dfrac{\\left| \\hat{\\psi}(\\omega) \\right|^2 }{\\omega} \\left| \\hat{\\phi}(\\omega)\\right|^2\\, d\\omega  \\\\\n&\\le {\\Large \\int_{-\\infty}^\\infty} \\dfrac{\\left| \\hat{\\psi}(\\omega) \\right|^2 }{\\omega}  \\sup \\left\\{\\left|\\hat{\\phi}(\\omega)\\right|^2 \\right\\}\\, d\\omega \\le \\infty\n\\end{aligned}\n\\]\n이므로 \\(\\psi \\ast \\phi\\) 도 웨이블릿이다. \\(\\square\\)\n\n\n정리 1 은 일단 하나의 웨이블릿이 존재한다면 이로부터 무수히 많은 웨이블릿을 만들 수 있다는 것을 보장한다. 유계인 적분 가능함수 \\(\\phi\\) 만 찾고 웨이블릿 \\(\\psi\\) 에 대해 \\(\\psi \\ast \\phi\\) 만 해 주면 된다.\n\n\n\n예제 2 \\(\\phi (t)\\) 를 다음과 같이 정의하자.\n\\[\n\\phi(t) = \\left\\{\\begin{array}{ll} 0, \\qquad & t&lt;0, \\\\ 1, & 0 \\le t \\le 1, \\\\ 0 \\ge 1. \\end{array}\\right.\n\\]\nHaar 웨이블릿 \\(\\psi_H\\) 와 \\(\\phi\\) 의 합성곱 \\(\\psi_H \\ast \\phi\\) 는 아래 그림 그림 3 과 같다.\n\n\n\n\n\n\n그림 2: \\(\\psi_H \\ast \\phi (t)\\) 웨이블릿\n\n\n\n\n\n\n\n\n예제 3 \\(\\phi (t) = e^{-t^2}\\) 와의 합성곱은\nHaar 웨이블릿 \\(\\psi_H\\) 와 \\(\\phi\\) 의 합성곱 \\(\\psi_H \\ast \\phi\\) 는 아래 그림 그림 3 과 같다.\n\n\n\n\n\n\n그림 3: \\(\\psi_H \\ast \\phi (t)\\) 웨이블릿\n\n\n\n\n\n\n\n\n\n그림 4: \\(\\mathfrak[F](\\psi_H \\ast \\phi)(t)\\)",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "웨이블릿 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/wavelet_transform.html#딸-웨이블릿",
    "href": "src/image_processing/wavelet/wavelet_transform.html#딸-웨이블릿",
    "title": "웨이블릿 변환",
    "section": "2 딸 웨이블릿",
    "text": "2 딸 웨이블릿\n\n\n\n\n\n\n\n정의 3 (Mother wavelet) 우리는 기본이 되는 웨이블릿 \\(\\psi(t)\\) 에 대해 아래와 같이 딸 웨이블릿(daughter wavelet) \\(\\psi_{a,\\,b}(t)\\) 를 정의 할 수 있다.\n\\[\n\\psi_{a,b}(t) = \\dfrac{1}{\\sqrt{|a|}} \\psi \\left(\\dfrac{t-b}{a}\\right), \\qquad a,\\,b\\in \\mathbb{R},\\, a\\ne 0.\n\\tag{5}\\]\n여기서 \\(a\\) 를 scaling parameter 라고 하고 \\(b\\) 를 translation parameter 라고 한다. 여기서 \\(\\psi(t) = \\psi_{1,\\,0}(t)\\) 를 mother wavelet 이라고 한다.\n\n\n\n\n\n명제 3 Mother wavelet \\(\\psi_{1, 0}(t)\\) 에 대해 \\(\\hat{\\psi}_{0,1}(\\omega) = \\mathfrak{F}[\\psi_{0, 1}](\\omega)\\) 일 때, 딸 웨이블릿의 푸리에 변환은 다음과 같다.\n\\[\n\\hat{\\psi}_{a,\\,b}(\\omega)= \\mathfrak{F}[\\psi_{a,b}](\\omega) = \\dfrac{a}{\\sqrt{|a|}}e^{i\\omega b} \\hat{\\psi}_{1, 0}(a\\omega).\n\\tag{6}\\]\n\n\n(증명). 푸리에 변환을 직접 해보면 된다.\n\n\n웨이블릿에 대해 \\(a,\\,b\\) 값이 변화하면 어떻게 되는지 아래 그림으로 대략 파악 할 수 있다.\n\n\n\n\n\n\n그림 5: Daughter wavlet\n\n\n\n\n\n\n예제 4 (멕시코 모자 웨이블릿) 멕시코 모자 웨이블릿은 가우시안 함수 \\(e^{-t^2/2}\\) 의 2차 미분에 \\(-1\\) 을 곱한 함수로 정의된다.\n\\[\n\\psi_{1,0}(t) := -\\left(\\dfrac{d^2}{dt^2}e^{-t^2/2}\\right) = (1-t^2)\\,e^{-t^2/2}.\n\\tag{7}\\]\n이에 대한 푸리에 변환은 다음과 같다.\n\\[\n\\hat{\\psi}_{1, 0}(\\omega) = \\mathfrak{F}\\left[-\\dfrac{d^2}{dt^2}e^{-t^2/2}\\right] = \\omega^2 \\mathfrak{F}[e^{-t^2/2}] = \\sqrt{2\\pi}\\omega^2 e^{-\\omega^2/2}\n\\tag{8}\\]\n\n\n\n\n\n\n그림 6: 멕시코 모자 웨이블릿 \\(\\psi_{1, 0}\\) 과 \\(\\hat{\\psi}_{1, 0}\\)\n\n\n\n\n\n\n\n\n\n그림 7: 멕시코 모자 웨이블릿 \\(\\psi_{1, 0}(t)\\), \\(\\psi_{\\frac{3}{2}, -2}(t)\\), \\(\\psi_{\\frac{1}{4}, \\sqrt{2}}(t)\\)",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "웨이블릿 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/wavelet_transform.html#연속-웨이블릿-변환",
    "href": "src/image_processing/wavelet/wavelet_transform.html#연속-웨이블릿-변환",
    "title": "웨이블릿 변환",
    "section": "3 연속 웨이블릿 변환",
    "text": "3 연속 웨이블릿 변환\n\n3.1 연속 웨이블릿 변환\n\n\n\n\n\n\n\n정의 4 (연속 웨이블릿 변환) 위에블릿 \\(\\psi (t)\\) 와 그 딸 웨이블릿 \\(\\psi_{a, b}(t)\\) 에 대해\n\\[\n\\mathfrak{W}_\\psi[f](a,\\,b) := \\langle f,\\, \\psi_{a,b}\\rangle = \\int_{-\\infty}^\\infty f(t)\\, \\overline{\\psi_{a,b}(t)}\\, dt\n\\tag{9}\\]\n를 \\(f(t)\\) 의 웨이블릿 \\(\\psi_{a,b}(t)\\) 에 대한 연속 웨이블릿 변환(continuous wavelet transformation)이라고 한다.\n\n\n\n\n\n웨이블릿 변환의 커널 \\(\\psi_{a,b}(t)\\) 는 푸리에 변환의 커널 \\(e^{i\\omega t}\\) 와 같은 역할을 한다. 푸리에 변환처럼 웨이블릿 변환도 선형변환이다. 푸리에 변환은 \\(\\omega\\) 에 대한 함수로 주어지지만 웨이블릿 변환은 scaling parameter \\(a\\) 와 translation parameter \\(b\\) 에 대한 함수이다.\n\n\n연습문제 1 Mother 웨이블릿 \\(\\psi(t)\\) 에 대한 푸리에 변환을 \\(\\hat{\\psi}(\\omega)\\), 딸 웨이블릿 \\(\\psi_{a,\\,b}(t)\\) 에 대한 푸리에 변환을 \\(\\hat{\\psi}_{a,\\,b}(\\omega)\\) 라고 할 때 다음이 성립함을 보여라.\n\\[\n\\hat{\\psi}_{a,\\,b}(\\omega) = \\sqrt{|a|} e^{-ib\\omega} \\hat{\\psi}(a\\omega)\n\\]\n\n\n(해답). \\[\n\\hat{\\psi}_{a,\\,b}(\\omega) = \\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-i\\omega t} \\psi_{a,\\,b}(t)\\, dt = \\sqrt{|a|} e^{-i\\omega b} \\hat{\\psi}(a\\omega)\n\\tag{10}\\]\n\n\n\n명제 4 Mother 웨이블릿 \\(\\psi(t)\\) 가 \\(k\\)-vanishing moment 를 가진다면 \\(f\\) 를 \\(b\\) 근처에서 푸리에 전개 했을 때 \\(\\mathfrak{W}_\\psi[f](a,\\,b)\\) 첫번째 \\(k\\) 개의 항이 \\(0\\) 이다.\n\n\n(증명). 웨이블릿 변환에서 \\(f(t)\\) 의 \\(b\\) 근방의 테일려 전개를 통해 다음을 보일 수 있다.\n\\[\n\\begin{aligned}\n\\mathfrak{W}_\\psi[f](a,\\,b) &= \\int_{-\\infty}^\\infty \\dfrac{1}{\\sqrt{|a|}} \\overline{\\psi\\left(\\dfrac{t-b}{a}\\right)} \\left[\\sum_{j=0}^\\infty \\dfrac{f^{(j)}(b)}{j!} (t-b)^j\\right]\\, dt \\\\\n&= \\sum_{j=0}^\\infty \\dfrac{f^{(j)}(b)}{(j!)\\sqrt{|a|}} \\int_{-\\infty}^\\infty \\overline{\\psi \\left(\\dfrac{t-b}{a}\\right)}(t-b)^j\\, dt &\\qquad &; x:=\\dfrac{t-b}{a} \\\\\n&= \\sum_{j=0}^\\infty \\dfrac{f^{(j)}(b)}{(j!)\\sqrt{|a|}} a^{j+1}\\int_{-\\infty}^\\infty \\overline{\\psi(x)} x^j\\, dx\\\\\n&=\\sum_{j=0}^\\infty \\dfrac{f^{(j)}(b)}{(j!)\\sqrt{|a|}} a^{j+1}m_k\n\\end{aligned}\n\\]\n이다. \\(j=0\\) 부터 \\(j=k\\) 까지의 항이 모두 \\(0\\) 이다. \\(\\square\\)\n\n\n\n\n3.2 웨이블릿 변환과 분해능\n웨이블릿 \\(\\psi(t)\\) 에 대해 \\(\\displaystyle \\int |\\psi(t)|^2=1\\) 인 경우를 생각하자. 이 때 \\(|\\psi(t)|^2\\) 를 \\(t\\) 에 대한 확률분포로 생각 할 수 있으며 \\(\\|\\psi\\|_2 = \\left\\|\\hat{\\psi}\\right\\|_2\\) 이므로 \\(|\\hat{\\psi}(\\omega)|^2\\) 는 \\(\\omega\\) 에 대한 확률분포로 생각할 수 있다. (양자역학의 파동함수를 생각하라.)\n\\(t\\) 와 \\(\\omega\\) 에 대한 평균값 \\(t_m\\) 과 \\(\\omega_m\\) 은 각각\n\\[\nt_m = \\int_{_\\infty}^\\infty t|\\psi(t)|^2\\, dt,\\qquad \\omega_m = \\dfrac{1}{2\\pi} \\int_{0}^\\infty \\omega \\left|\\hat{\\psi}(\\omega)\\right|^2\\, d\\omega\n\\]\n이며 각각의 표준편차 \\(\\sigma_t,\\,\\sigma_\\omega\\) 는 다음으로 부터 구할 수 있다.\n\\[\n{\\sigma_t}^2 = \\int_{-\\infty}^\\infty (t-t_m)^2 |\\psi(t)|^2\\, dt,\\qquad {\\sigma_\\omega}^2 = \\dfrac{1}{2\\pi} \\int_{-\\infty}^\\infty (\\omega - \\omega_m)^2 \\left|\\hat{\\psi}(\\omega)\\right|^2\\,d\\omega\n\\]\n따라서 연속 웨이블릿 변환 \\(\\mathfrak{W}_\\psi[f](a,\\,b)\\) 는 time window 에서\n\\[\n\\left[at_m + b - a\\sigma_t,\\, at_m + b +a\\sigma_t\\right]\n\\]\n영역의 정보를 준다고 할 수 있다. 또한\n\\[\n\\mathfrak{W}_\\psi[f](a,\\,b) = \\langle f,\\,\\psi_{a,b}\\rangle = \\langle \\hat{f},\\, \\hat{\\psi}_{a,b}\\rangle = \\dfrac{1}{2\\pi} \\int_{-\\infty}^\\infty \\left[\\sqrt{|a|} \\hat{f}(\\omega) \\overline{\\hat{\\psi}_{a,b}}\\right]\\, e^{ib\\omega}\\, d\\omega\n\\]\n이므로 frequency window 에서는\n\\[\n\\left[\\dfrac{\\omega_m}{a} - \\dfrac{\\sigma_\\omega}{a},\\, \\dfrac{\\omega_m}{a} + \\dfrac{\\sigma_\\omega}{a}\\right]\n\\]\n영역의 정보를 준다고 할 수 있다. 즉 \\(\\mathfrak{W}_\\psi[f](a,\\,b)\\) 는 \\(t-\\omega\\) 평면에서\n\\[\n\\left[at_m + b - a\\sigma_t,\\, at_m + b +a\\sigma_t\\right] \\times \\left[\\dfrac{\\omega_m}{a} - \\dfrac{\\sigma_\\omega}{a},\\, \\dfrac{\\omega_m}{a} + \\dfrac{\\sigma_\\omega}{a}\\right]\n\\]\n영역의 정보를 준다.\n\n\n\n\n\n\n그림 8: 웨이블렛 변환의 Time-frequency 평면\n\n\n\n\n\n정리 2 \\(\\phi,\\,\\psi\\) 가 웨이블릿이며 \\(f,\\,g\\in L^2(\\mathbb{R}),\\, \\alpha,\\, \\beta\\in \\mathbb{C},\\, c\\in \\mathbb{R}\\) 일 때 다음이 성립한다. (see 연산자들)\n  (\\(1\\)) \\(\\mathfrak{W}_\\psi [\\alpha f +  \\beta g](a,\\,b) = \\alpha\\mathfrak{W}_\\psi[f](a,\\,b) + \\beta \\mathfrak{W}_\\psi [g](a,\\,b)\\).\n  (\\(2\\)) \\(\\mathfrak{W}_\\psi[T_cf](a,\\,b) = \\mathfrak{W}_\\psi [f] (a,\\, b-c)\\).\n  (\\(3\\)) \\(c&gt;0\\) 일 때 \\(\\mathfrak{W}_\\psi \\left[D_c f\\right](a,\\,b) = \\dfrac{1}{\\sqrt{c}}\\mathfrak{W}_\\psi [f] \\left(\\dfrac{a}{c},\\,\\dfrac{b}{c}\\right)\\).\n  (\\(4\\)) \\(a\\ne 0\\) 일 때 \\(\\mathfrak{W}_\\psi [f] (a,\\,b)= \\overline{\\mathfrak{W}_f[\\psi] \\left(\\dfrac{1}{a},\\,-\\dfrac{b}{a}\\right)}\\).\n  (\\(5\\)) \\(\\mathfrak{W}_{P\\psi}[Pf](a,\\,b) = \\mathfrak{W}_\\psi[f] (a,\\,-b)\\).\n  (\\(6\\)) \\(\\mathfrak{W}_{\\alpha \\psi + \\beta \\phi} [f](a,\\,b) = \\overline{\\alpha} \\mathfrak{W}_\\psi [f](a,\\,b) + \\overline{\\beta} \\mathfrak{W}_\\phi[f](a,\\,b)\\).\n  (\\(7\\)) \\(\\mathfrak{W}_{T_c \\psi}[f] (a,\\,b) = \\mathfrak{W}_\\psi [f](a,\\, b+ca)\\).\n  (\\(8\\)) \\(c&gt;0\\) 에 대해 \\(\\mathfrak{W}_{D_c \\psi}[f](a,\\,b) = \\dfrac{1}{\\sqrt{c}} \\mathfrak{W}_\\psi [f](ac,\\,b)\\).\n\n\n(증명). (\\(1\\)) 정의에 의해 선형성은 자명하다.\n(\\(2\\))\n\\[\n\\begin{aligned}\n\\mathfrak{W}_\\psi[T_cf](a,\\,b) &= \\int_{-\\infty}^\\infty f(t-c) \\dfrac{1}{\\sqrt{a}}\\overline{\\psi \\left(\\dfrac{t-b}{a}\\right)}\\, dt= \\int_{-\\infty}^\\infty f(x) \\dfrac{1}{\\sqrt{a}} \\overline{\\psi \\left(\\dfrac{x-(b-c)}{a}\\right)}\\,dx\\\\[0.5em]\n&= \\mathfrak{W}_\\psi [f](a,\\, b-c).\n\\end{aligned}\n\\]\n(\\(3\\))\n\\[\n\\begin{aligned}\n\\mathfrak{W}_\\psi \\left[D_c f\\right](a,\\,b) &= \\int_{-\\infty}^\\infty \\dfrac{1}{\\sqrt{|c|}} f\\left(\\dfrac{t}{c}\\right) \\dfrac{1}{\\sqrt{a}}\\overline{\\psi \\left(\\dfrac{t-b}{a}\\right)}\\, dt \\\\\n&= \\int_{-\\infty}^\\infty \\dfrac{1}{\\sqrt{c}} f(x) \\dfrac{1}{\\sqrt{a}} \\overline{\\psi \\left(\\dfrac{x-b/c}{a/c}\\right)}\\, dt \\\\\n&=\\dfrac{1}{\\sqrt{c}} \\mathfrak{W}_{\\psi}[f](a/c,\\, b/c).\n\\end{aligned}\n\\]\n(\\(4\\))\n\\[\n\\begin{aligned}\n\\mathfrak{W}_\\psi [f] (a,\\,b) &= \\dfrac{1}{\\sqrt{a}} \\int_{-\\infty}^\\infty f(t) \\overline{\\psi\\left(\\dfrac{t-b}{a}\\right)}\\, dx \\\\\n&={\\sqrt{a}} \\int_{-\\infty}^\\infty f\\left(\\dfrac{x+b/a}{1/a}\\right) \\overline{\\psi (x)}\\, dx \\\\\n&={\\sqrt{a}} \\overline{ \\int_{-\\infty}^\\infty \\psi(x) \\, \\overline{f\\left(\\dfrac{x+b/a}{1/a}\\right)}} \\, dx\\\\\n&= \\overline{\\mathfrak{W}_{f}[\\psi]\\left(\\dfrac{1}{a},\\, -\\dfrac{b}{a}\\right)}.\n\\end{aligned}\n\\]\n(\\(5\\)) \\(\\displaystyle \\mathfrak{W}_{P\\psi}[Pf] = \\dfrac{1}{\\sqrt{|a|}}\\int_{-\\infty}^\\infty f(-t) \\overline{\\psi \\left(\\dfrac{-t-b}{a}\\right)}\\, dt = \\mathfrak{W}_\\psi [f](a,\\,-b).\\)\n(\\(6\\))\n\\[\n\\begin{aligned}\n\\mathfrak{W}_{\\alpha \\psi + \\beta \\phi} [f](a,\\,b) &= \\dfrac{1}{\\sqrt{|a|}}\\int_{-\\infty}^\\infty f(t) \\left[\\overline{\\alpha \\psi \\left(\\dfrac{t-b}{a}\\right) + \\beta \\phi\\left(\\dfrac{t-b}{a}\\right) }\\right] \\, dt \\\\\n&= \\dfrac{\\overline{\\alpha}}{\\sqrt{|a|}} \\int_{-\\infty}^\\infty f(t)  \\overline{\\psi \\left(\\dfrac{t-b}{a}\\right)}\\, dt + \\dfrac{\\overline{\\beta}}{\\sqrt{|a|}} \\int_{-\\infty}^\\infty f(t)  \\overline{\\phi \\left(\\dfrac{t-b}{a}\\right)}\\, dt \\\\\n&= \\overline{\\alpha} \\mathfrak{W}_\\psi [f](a,\\,b) + \\overline{\\beta} \\mathfrak{W}_\\phi [f](a,\\,b).\n\\end{aligned}\n\\]\n(\\(7\\)) \\(\\displaystyle \\mathfrak{W}_{T_c \\psi}[f] (a,\\,b) = \\dfrac{1}{\\sqrt{|a|}}\\int_{\\infty}^\\infty f(t) \\overline{\\psi \\left(\\dfrac{(t-b)}{a}-c\\right)}\\, dt = \\mathfrak{W}_\\psi [f](a,\\,b+ac)\\)\n(\\(8\\)) \\(D_c\\psi =  \\dfrac{1}{\\sqrt{c}} \\psi\\left(\\dfrac{t}{c}\\right)\\) 이므로,\n\\[\n\\begin{aligned}\n\\mathfrak{W}_{D_c \\psi}[f](a,\\,b) &= \\dfrac{1}{\\sqrt{|a|c}} \\int_{-\\infty}^{\\infty}f(t) \\overline{\\psi \\left(\\dfrac{1}{c}\\dfrac{t-b}{a}\\right)}\\, dt = \\dfrac{1}{\\sqrt{|a|c}} \\int_{-\\infty}^{\\infty}f(t) \\overline{\\psi \\left(\\dfrac{t-b}{ac}\\right)}\\, dt \\\\\n&= \\mathfrak{F}_\\psi [f](ac,\\,b). \\qquad \\square\n\\end{aligned}\n\\]\n\n\n\n정리 3 (Parseval’s formula) \\(\\psi \\in L^2(\\mathbb{R})\\) 이며 \\(f,\\,g\\in L^2(\\mathbb{R})\\) 일 때 다음이 성립한다.\n\\[\n\\langle f,\\,g \\rangle = \\dfrac{1}{C_\\psi} \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\,\\mathfrak{W}_\\psi [f](a,\\,b) \\overline{ \\mathfrak{W}_\\psi [g](a,\\,b)}\\,\\dfrac{db\\,da}{a^2}.\n\\tag{11}\\]\n여기서 \\(C_\\psi\\) 는 다음과 같다.\n\\[\nC_\\psi = \\displaystyle \\int_{-\\infty}^\\infty \\dfrac{\\left|\\hat{\\psi}(\\omega)\\right|^2}{|\\omega|} \\, d\\omega &lt; \\infty\n\\tag{12}\\]\n\n\n(증명). 일반화된 Parseval’s relation 과 연속 웨이블릿 변환 의 정의, 그리고 연습문제 1 로부터\n\\[\n\\begin{aligned}\n\\mathfrak{W}_\\psi [f](a,\\,b) &= \\langle f,\\, \\psi_{a,\\,b}\\rangle = \\langle \\hat{f},\\, \\hat{\\psi}_{a,\\,b}\\rangle = \\int_{-\\infty}^\\infty \\hat{f}(\\omega) \\, \\sqrt{|a|} e^{ib\\omega} \\overline{\\hat{\\psi}(a\\omega)}\\, d\\omega, \\\\\n\\overline{\\mathfrak{W}_\\psi [g](a,\\,b)} &= \\int_{-\\infty}^\\infty \\overline{\\hat{g}(\\mu)} \\, \\sqrt{|a|} e^{-ib\\mu} \\hat{\\psi}(a\\mu)\\, d\\mu,\n\\end{aligned}\n\\]\n식 11 의 우변은\n\\[\n\\begin{aligned}\n\\int_{-\\infty}^\\infty &\\int_{-\\infty}^\\infty \\mathfrak{W}_\\psi [f](a,\\,b) \\,\\overline{ \\mathfrak{W}_\\psi [g](a,\\,b)}\\,\\dfrac{db\\,da}{a^2} \\\\\n&= \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\dfrac{db\\,da}{a^2} \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty |a|e^{ib(\\omega-\\mu)} \\, \\hat{f}(\\omega)\\,\\overline{\\hat{g}(\\mu)} \\overline{\\hat{\\psi}(a\\omega)}\\hat{\\psi}(a\\mu)\\, d\\mu\\, d\\omega \\\\\n\\end{aligned}\n\\]\n여기서 \\(\\int_{-\\infty}^\\infty e^{ib(\\omega - \\mu)}\\, db = \\delta(\\omega-\\mu)\\) 이므로,\n\\[\n\\begin{aligned}\n\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\mathfrak{W}_\\psi [f](a,\\,b) \\,\\overline{ \\mathfrak{W}_\\psi [g](a,\\,b)}\\,\\dfrac{db\\,da}{a^2} &= \\int_{-\\infty}^\\infty \\dfrac{da}{|a|} \\int_{-\\infty}^\\infty \\hat{f}(\\omega)\\,\\overline{\\hat{g}(\\omega)} \\left|\\hat{\\psi}(a\\omega)\\right|^2\\, d\\omega \\\\\n&= \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty \\left|\\hat{\\psi}(a\\omega)\\right|^2 \\, \\dfrac{da}{|a|}\\right] \\,\\hat{f}(\\omega)\\,\\overline{\\hat{g}(\\omega)}\\,d\\omega \\\\\n&= \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty \\dfrac{\\left|\\hat{\\psi}(x)\\right|^2}{|x|}\\, dx\\right] \\,\\hat{f}(\\omega)\\,\\overline{\\hat{g}(\\omega)}\\,d\\omega \\\\\n&= C_\\psi  \\int_{-\\infty}^\\infty\\hat{f}(\\omega)\\,\\overline{\\hat{g}(\\omega)}\\,d\\omega\\\\\n&= C_\\psi \\langle \\hat{f},\\, \\hat{g}\\rangle = C_\\psi \\langle f,\\,g\\rangle\n\\end{aligned}\n\\]\n이다. \\(\\square\\)\n\n\n\n정리 4 \\(f\\in \\mathcal{L}^2(\\mathbb{R})\\) 일 때 아래의 식이 거의 모든 \\(\\mathbb{R}\\) 에서 성립한다.\n\\[\n\\dfrac{1}{C_\\psi} \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\mathfrak{W}_\\psi[f](a,\\,b) \\, \\psi_{a,\\,b}(t)\\,\\dfrac{db\\,da}{a^2} = f(t).\n\\tag{13}\\]\n\n\n(증명). 정리 3 로부터 시작한다. \\[\n\\begin{aligned}\nC_\\psi \\langle f,\\,g\\rangle &=\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\,\\mathfrak{W}_\\psi [f](a,\\,b) \\overline{ \\mathfrak{W}_\\psi [g](a,\\,b)}\\,\\dfrac{db\\,da}{a^2} \\\\\n&= \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\,\\mathfrak{W}_\\psi [f](a,\\,b) \\overline{\\left[\\int_{-\\infty}^\\infty g(t)\\, \\overline{\\psi_{a,\\,b}(t)}\\, dt\\right]}\\,\\dfrac{db\\,da}{a^2} \\\\\n&= \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\mathfrak{W}_\\psi [f](a,\\,b) \\psi_{a,\\,b}(t) \\,\\dfrac{db\\,da}{a^2} \\, \\overline{g(t)}\\,dt \\\\\n&= \\left\\langle  \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\mathfrak{W}_\\psi [f](a,\\,b) \\psi_{a,\\,b}(t) \\,\\dfrac{db\\,da}{a^2},\\, g(t)\\right\\rangle\n\\end{aligned}\n\\]\n이다. \\(g\\) 가 임의의 함수이므로\n\\[\nf(t) = \\dfrac{1}{C_\\psi }\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\mathfrak{W}_\\psi [f](a,\\,b) \\psi_{a,\\,b}(t) \\,\\dfrac{db\\,da}{a^2}\n\\]\n가 거의 모든 곳에서 성립한다. \\(\\square\\)",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "웨이블릿 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/wavelet_transform.html#이산-웨이블릿-변환",
    "href": "src/image_processing/wavelet/wavelet_transform.html#이산-웨이블릿-변환",
    "title": "웨이블릿 변환",
    "section": "4 이산 웨이블릿 변환",
    "text": "4 이산 웨이블릿 변환\n이산 푸리에 변환과 그 역변환은 이산적인 시간 \\(t\\) 와 진동수 \\(\\omega\\) 사이의 변환이지만 이산 웨이블릿 변환은 이산적인 \\(t\\) 에 대해, 역시 이산적인 scale parameter \\(a\\) 와 translation parameter \\(b\\) 사이의 변환이다. 이산 웨이블릿 변환을 위해서는 scale paramter 와 관련있는 \\(0&lt;a_0\\ne 1\\) 과 translation paramter 와 관련있는 \\(b_0&gt;0\\) 을 우선 정한다.\n\n\n예제 5 mother wavelet \\(\\psi\\) 에 대해 scale parameter 가 \\(a_0^m\\) 이며 translation parametr \\(nb_0 a_0^m\\) 일 때의 딸 웨이블릿은\n\\[\n\\dfrac{1}{\\sqrt{|a_0^{m}|}} \\psi \\left(\\dfrac{t-nb_0a_0^m}{a_0^m}\\right) = a_0^{-m/2} \\psi (a_0^{-m}t - nb_0)\n\\]\n이다.\n\n\n\nMother 웨이블릿 \\(\\psi\\) 와 정수 \\(m,\\,n\\) 에 대한 딸 웨이블릿의 집합을 아래와 같이 정의한다.\n\\[\nF_\\psi (a,\\,b) := \\left\\{ \\psi_{m,\\,n}(t) = a_0^{-m/2} \\psi \\left(a_0^{-m} t - nb_0\\right): m,\\,n \\in \\mathbb{Z}\\right\\}.\n\\tag{14}\\]\n\n가장 바람직한 것은 \\(F_\\psi (a,\\,b)\\) 가 \\(L^2(\\mathbb{R})\\) 에서 complete 하고 orthogonal 한 것이지만 그것을 보장할 수는 없다.\n\n\n\n\n\n\n\n정의 5 (아핀 웨이블릿과 프레임) 식 14 의 $F_(a,,b) $ 가 complete 하다면 \\(\\psi\\) 를 아핀 웨이블릿 (affine wavelet) 이라고 한다. 임의의 \\(f\\in L^2\\) 에 대해 어떤 양수 \\(A_f,\\,B_f\\) 가 존재하여\n\\[\nA_f\\|f\\|_2^2 \\le \\sum_{m,\\,n\\in \\mathbb{Z}} |\\langle f,\\,\\psi_{m,\\,n}\\rangle|^2 \\le B_f\\|f\\|_2^2\n\\]\n일 때 \\(F_\\psi\\) 를 프레임(frame) 이라고 한다.\n\n\n\n\n\n\n명제 5 \\(\\psi \\in L^2(\\mathbb{R})\\) 가 정의 1 의 admissiblitity 조건을 만족한다면 \\(F_\\psi\\) 는 frame 이다.\n\n\n(증명). 증명 생략\n\n\n일반적으로 DWT 에서는 \\(a_0=2,\\,b_0=1\\) 을 사용한다. 그렇다면 웨이블릿 \\(\\psi\\) 에 대해\n\\[\nF_\\psi = \\left\\{ \\psi_{m,n}:= 2^{-m/2} \\psi\\left( 2^{-m/2} - n \\right) : m,\\,n\\in \\mathbb{Z}\\right\\}\n\\]\n를 사용한다.\n\n\n\n\n\n\n그림 9: Dyadic sampling grid for the DWT",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "웨이블릿 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/wavelet_transform.html#orthogonal-wavelet",
    "href": "src/image_processing/wavelet/wavelet_transform.html#orthogonal-wavelet",
    "title": "웨이블릿 변환",
    "section": "5 Orthogonal wavelet",
    "text": "5 Orthogonal wavelet",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "웨이블릿 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/descrete_wavelet_transform.html",
    "href": "src/image_processing/wavelet/descrete_wavelet_transform.html",
    "title": "이산 웨이블릿 변환",
    "section": "",
    "text": "정의 1",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "이산 웨이블릿 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/descrete_wavelet_transform.html#정의와-기본적인-성질",
    "href": "src/image_processing/wavelet/descrete_wavelet_transform.html#정의와-기본적인-성질",
    "title": "이산 웨이블릿 변환",
    "section": "",
    "text": "정의 1",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "이산 웨이블릿 변환"
    ]
  },
  {
    "objectID": "src/image_processing/introduction_to_signal_processing.html",
    "href": "src/image_processing/introduction_to_signal_processing.html",
    "title": "디지털 신호처리의 기초",
    "section": "",
    "text": "신호 (signal) 는 시공간적으로, 혹은 어떤 독립변수에 의해 변하는 물리량을 의미한다. 즉 측정할 수 있는 양을 말하며 신호를 목적에 맞게 가공하는 과정을 신호처리 (signal processing) 라고 한다. 하나의 소스에서 발생되는 신호를 스칼라 신호라고 하고, 시간에 따라 두 개 이상의 소스에서 발생하는 신호를 벡터 신호라고 한다. 스칼라 신호는 하나의 독립 변수로 표현할 수 있으므로 1차원 신호라고 하고, 벡터 신호는 두 개 이상의 독립 변수로 표현되므로 다차원 신호라고 한다. 예를 들어 영상(image)신호는 2차원 신호이며, 동영상(video) 신호는 3차원 신호이다.\n\n\n\n\n신호 \\(s: A \\subset \\mathbb{R} \\to \\mathbb{R}\\) 를 생각하자. \\(A\\) 가 연속일 때 \\(A\\) 에 포함되는 수열 \\(\\langle t_k \\rangle \\subset A\\) 을 뽑아 내어 신호를 \\(\\{s_k = s(t_k)\\}\\) 로 재구성 하는 것을 샘플링 (sampling) 이라고 한다. 유한개의 \\(B \\subset \\{s(t):t\\in A\\}\\) 에 대한 함수 \\(\\phi : s(A) \\to B\\) 에 대해 \\(\\overline{s}(t) = (\\phi \\circ s)(t)\\) 로 \\(s(t)\\) 를 표현하는 것을 양자화 (quantization) 이라고 한다.\n보통 샘플링은 독립 변수에 대해 주기적으로 데이터를 얻는다. 즉 \\(s(t)\\) 에 대해 주기 \\(T\\) 간격으로 샘플링 할 경우 \\(t_k = kT\\) 라 놓고 \\(s_k = s(t_k) = s(kT)\\) 를 얻게 된다. 이 때 \\(1/T\\) 를 sampling rate 라고 한다.\n\n\n\n\n아날로그 신호가 band-limited 인 경우, 즉 아날로그 신호의 최대 주파수가 \\(\\nu_M\\) 일 경우 샘플링 주파수 \\(\\nu_S\\) 가 \\(\\nu_M\\) 의 2배보다 크다면 원본 아날로그 신호를 복원 할 수 있다. 이 때 \\(2\\nu_M\\) 을 Nyquist frequency 혹은 Nyqueist rate 라고 한다.\n\n\n\n\n\n\n\n표 1: 기본적인 신호함수들\n\n\n\n\n\n\n\n\n\n신호 이름\n신호의 정의\n\n\n\n\nHeaviside 계단 함수 (step function)\n\\(\\text{step}(x) = \\left\\{ \\begin{array}{l} 1 \\qquad & x \\ge 0 \\\\ 0 & \\text{otherwise}\\end{array}\\right.\\)\n\n\n직사각 함수 (rectangular function)\n\\(\\text{rect}(x) = \\left\\{ \\begin{array}{l} 1 \\qquad & |x| \\le 1/2 \\\\ 0 & \\text{otherwise}\\end{array}\\right.\\)\n\n\n삼각 함수 (trianle function)\n\\(\\text{tri}(x) = \\left\\{ \\begin{array}{l} 1 -|x| & x \\le 1/2 \\\\ 0 & \\text{otherwise}\\end{array}\\right.\\)\n\n\n\\(\\text{sinc}\\) 함수\n\\(\\text{sinc}(x) = \\dfrac{\\sin (\\pi x)}{\\pi x}\\)\n\n\n가우스 함수\n\\(\\text{gauss}(x) = \\dfrac{1}{\\sqrt{\\pi}} e^{-x^2}\\)\n\n\n디렉 델타 함수\n\\(\\delta (x)\\), 정의 1 참고\n\n\n\n\n\n\n\n\n\n\n\n기본적인 신호들\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n정의 1 (디렉 델타 함수) Dirac 델타 함수 (델타 함수) \\(\\delta (t)\\) 는 다음의 두가지 성질을 갖는 함수로 정의된다.\n  (\\(1\\)) \\(\\delta(t) = \\left\\{ \\begin{array}{ll} \\infty \\qquad &\\text{if } t= 0 \\\\ 0 & \\text{otherwise.} \\end{array}\\right.\\)\n  (\\(2\\)) \\(\\mathbb{R}\\) 에서 정의된 함수 \\(f(t)\\) 에 대해 \\(\\displaystyle \\int_{-\\infty}^\\infty  \\delta(t)\\, f(t) \\, dt = f(0)\\).\n\n\n\n\n\n\n명제 1 (델타 함수의 성질) 델타 함수는 다음과 같은 성질을 가진다.\n  (\\(1\\)) \\(\\delta(-x) = \\delta (x)\\).\n  (\\(2\\)) \\(\\displaystyle \\int_{-\\infty}^\\infty f(x)\\,\\delta(x-x_0)\\, dx = f(x_0)\\).\n  (\\(3\\)) \\(a\\ne 0\\) 에 대해 \\(\\delta (ax) = \\dfrac{1}{|a|} \\delta (x)\\).\n  (\\(4\\)) \\(\\displaystyle \\int_{-\\infty}^\\infty f(x) \\,\\delta'(x)\\, dx = - f'(0)\\)\n  (\\(5\\)) \\(g(x)\\) 가 미분가능하며 \\(g(x) = 0\\) 가 유한개의 근 \\(x_1,\\ldots,\\,x_n\\) 을 갖고, \\(g'(x_i)\\ne 0\\) 일 때\n\\[\n\\displaystyle \\delta (g(x))= \\sum_{i=1}^n \\dfrac{\\delta(x-x_i)}{|g'(x_i)|}.\n\\]\n  (\\(6\\)) \\(\\delta(x) = \\displaystyle \\dfrac{1}{2\\pi} \\int_{-\\infty}^\\infty e^{ikx}\\, dk\\).\n\n\n(증명). (\\(1\\)) \\(\\displaystyle \\int_{-\\infty}^\\infty f(x)\\, \\delta(-x)\\, dx = \\int_{-\\infty}^\\infty f(-x) \\, \\delta (x) \\, dx = f(0)\\).\n(\\(2\\)) \\(t=x-x_0\\) 로 치환하여 적분한다.\n(\\(3\\)) \\(t=ax\\) 로 치환하여 적분한다. \\(a&lt;0\\) 인 경우\n\\[\n\\int_{-\\infty}^\\infty f(x)\\, \\delta(ax) \\, dx = \\dfrac{1}{|a|}\\int_{-\\infty}^\\infty f(x/a) \\delta (x)\\, dx = \\dfrac{1}{|a|} f(0) = \\int_{-\\infty}^\\infty f(x) \\left(\\dfrac{1}{|a|} \\delta(x) \\right)dx\n\\]\n이다.\n(\\(4\\)) 부분적분을 이용한다.\n\\[\n\\int_{-\\infty}^\\infty f(x) \\, \\delta (x)\\,dx = f(x) \\delta(x){\\huge \\mid}_{-\\infty}^\\infty - \\int_{-\\infty}^\\infty f'(x)\\, \\delta (x)\\,dx = -f'(0).\n\\]\n(\\(5\\)) \\(g'(x_i) \\ne 0\\) 이므로 각각의 \\(x_i\\) 주위에 \\(g(x)\\) 가 전단사가 되는 \\((x_i-\\epsilon_i,\\, x_i+\\epsilon_i)\\) 구간이 존재한다. 이 구간에서의 \\(g(x)\\) 의 역함수를 \\(h_i(x)\\) 라고 하자. \\(g(x_i-\\epsilon_i),\\, g(x_i+\\epsilon_i)\\) 가운데 작은수와 큰 수를 각각 \\(t_1,\\,t_2\\) 라고 하면,\n\\[\n\\begin{aligned}\n\\int f(x) \\, \\delta(g(x))\\,dx &= \\sum_{i=1}^n \\int_{x_i - \\epsilon_i}^{x_i + \\epsilon_i} f(x)\\, \\delta(g(x))\\, dx \\\\\n&= \\sum_{i=1}^n \\int_{t_1}^{t_2} f(h_i(t))\\, \\delta(t) \\dfrac{dt}{|g'(h_i(t))|} \\\\\n&= \\sum_{i=1} \\dfrac{f(x_i)}{|g'(x_i)|} \\\\\n&= \\int_{-\\infty}^\\infty f(x) \\left(\\sum_{i=1}\\dfrac{\\delta(x-x_i)}{|g'(x_i)|}\\right)\\, dx\n\\end{aligned}\n\\]\n이다.\n(\\(6\\)) \\(d(x) = \\int_{-\\infty}^\\infty e^{ikx}\\, dk\\) 로 놓고 다음과 같이 적분을 할 수 있다.\n\\[\n\\begin{aligned}\nd(x) = \\int_{-\\infty}^\\infty e^{ikx}dk &= \\int_{-\\infty}^0 e^{ikx}\\, dk + \\int_0^{\\infty} e^{ikx}\\, dk \\\\\n&= \\int_0^\\infty e^{ikx}\\, dk + \\int_0^\\infty e^{-ikx}\\,dk \\\\\n&= \\lim_{\\epsilon \\to 0^+} \\int_{0}^\\infty (e^{ikx}+e^{-ikx})\\, e^{-\\epsilon k}\\, dk \\\\\n&= \\lim_{\\epsilon \\to 0^+} \\dfrac{2\\epsilon}{\\epsilon^2+x^2}\n\\end{aligned}\n\\]\n이 경우 \\(d(0) = +\\infty\\) 이며 \\(d(x\\ne 0) = 0\\) 이다. 우리는\n\\[\n\\displaystyle \\int_{-\\infty}^\\infty \\dfrac{\\epsilon}{x^2+\\epsilon^2}\\, dx = \\tan^{-1} (x/\\epsilon) {\\huge \\mid}_{-\\infty}^\\infty = \\pi\n\\]\n임을 안다. 즉 \\(\\epsilon\\) 값과 무관하게 \\(\\int_{-\\infty}^\\infty d(x)\\, dx = 2\\pi\\) 이다. 따라서 \\[\n\\int_{-\\infty}^\\infty f(x)\\, d(x) \\, dx = \\lim_{\\epsilon \\to 0^+} \\int_{-\\infty}^\\infty \\dfrac{2\\epsilon f(x)}{\\epsilon^2+x^2}\\, dx = 2\\pi f(0)\n\\]\n이므로 \\(d(x) = 2\\pi \\delta(x)\\) 이다. \\(\\square\\)\n\n\n\n\n\n우리가 자연적으로 들어오는 연속 시그널 \\(s(t)\\) 에 대해 \\(T\\) 간격으로 샘플링을 한다고 하자. 이 때 \\(t=0\\) 으로 부터 \\(Tk\\) 에서 샘플링된 신호 \\(\\tilde{s}_k\\) 는\n\\[\n\\tilde{s}_k = \\int_{-\\infty}^\\infty s(t) \\,\\delta(t-kT ) \\,dt\n\\tag{1}\\]\n로 부터 얻을 수 있다.\n그렇다면 \\(s(t)\\) 에 대한 샘플링을\n\\[\n\\tilde{s}(t) = \\sum_k \\int_{-\\infty}^\\infty s(t) \\delta (t-kT) \\, dt = \\int_{-\\infty}^\\infty s(t) \\left[ \\sum_{k} \\delta (t-kT)\\right]\\, dt\n\\tag{2}\\]\n로 부터 얻을 수 있다. 이 때 델타함수가 주기적으로 반복되는 \\(S_T(t) = \\sum_{k} \\delta (t-kT)\\) 를 샘플링 함수(sampling function) 혹은 주기적 임펄스 함수 (periodic impulse function) 이라고 한다.\n\n\n\n\n우리가 컴퓨터에서 다루는 대부분의 데이터는 이산데이터이다. 연속함수의 샘플링과 양자화로 얻어진 이산데이터일 수도 있고, 자체로서 이산적인 값을 갖는 데이터 일 수도 있다. 이산데이터는 수학에서 수열을 다룰 때 처럼 1차원의 경우 \\(s_k\\) 나 혹은 \\(s[k]\\) 로 표현하며, 2차원의 경우 \\(s_{i, j}\\) 나 \\(s[i, j]\\) 로 표현한다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "디지털 신호처리의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/introduction_to_signal_processing.html#신호",
    "href": "src/image_processing/introduction_to_signal_processing.html#신호",
    "title": "디지털 신호처리의 기초",
    "section": "",
    "text": "신호 (signal) 는 시공간적으로, 혹은 어떤 독립변수에 의해 변하는 물리량을 의미한다. 즉 측정할 수 있는 양을 말하며 신호를 목적에 맞게 가공하는 과정을 신호처리 (signal processing) 라고 한다. 하나의 소스에서 발생되는 신호를 스칼라 신호라고 하고, 시간에 따라 두 개 이상의 소스에서 발생하는 신호를 벡터 신호라고 한다. 스칼라 신호는 하나의 독립 변수로 표현할 수 있으므로 1차원 신호라고 하고, 벡터 신호는 두 개 이상의 독립 변수로 표현되므로 다차원 신호라고 한다. 예를 들어 영상(image)신호는 2차원 신호이며, 동영상(video) 신호는 3차원 신호이다.\n\n\n\n\n신호 \\(s: A \\subset \\mathbb{R} \\to \\mathbb{R}\\) 를 생각하자. \\(A\\) 가 연속일 때 \\(A\\) 에 포함되는 수열 \\(\\langle t_k \\rangle \\subset A\\) 을 뽑아 내어 신호를 \\(\\{s_k = s(t_k)\\}\\) 로 재구성 하는 것을 샘플링 (sampling) 이라고 한다. 유한개의 \\(B \\subset \\{s(t):t\\in A\\}\\) 에 대한 함수 \\(\\phi : s(A) \\to B\\) 에 대해 \\(\\overline{s}(t) = (\\phi \\circ s)(t)\\) 로 \\(s(t)\\) 를 표현하는 것을 양자화 (quantization) 이라고 한다.\n보통 샘플링은 독립 변수에 대해 주기적으로 데이터를 얻는다. 즉 \\(s(t)\\) 에 대해 주기 \\(T\\) 간격으로 샘플링 할 경우 \\(t_k = kT\\) 라 놓고 \\(s_k = s(t_k) = s(kT)\\) 를 얻게 된다. 이 때 \\(1/T\\) 를 sampling rate 라고 한다.\n\n\n\n\n아날로그 신호가 band-limited 인 경우, 즉 아날로그 신호의 최대 주파수가 \\(\\nu_M\\) 일 경우 샘플링 주파수 \\(\\nu_S\\) 가 \\(\\nu_M\\) 의 2배보다 크다면 원본 아날로그 신호를 복원 할 수 있다. 이 때 \\(2\\nu_M\\) 을 Nyquist frequency 혹은 Nyqueist rate 라고 한다.\n\n\n\n\n\n\n\n표 1: 기본적인 신호함수들\n\n\n\n\n\n\n\n\n\n신호 이름\n신호의 정의\n\n\n\n\nHeaviside 계단 함수 (step function)\n\\(\\text{step}(x) = \\left\\{ \\begin{array}{l} 1 \\qquad & x \\ge 0 \\\\ 0 & \\text{otherwise}\\end{array}\\right.\\)\n\n\n직사각 함수 (rectangular function)\n\\(\\text{rect}(x) = \\left\\{ \\begin{array}{l} 1 \\qquad & |x| \\le 1/2 \\\\ 0 & \\text{otherwise}\\end{array}\\right.\\)\n\n\n삼각 함수 (trianle function)\n\\(\\text{tri}(x) = \\left\\{ \\begin{array}{l} 1 -|x| & x \\le 1/2 \\\\ 0 & \\text{otherwise}\\end{array}\\right.\\)\n\n\n\\(\\text{sinc}\\) 함수\n\\(\\text{sinc}(x) = \\dfrac{\\sin (\\pi x)}{\\pi x}\\)\n\n\n가우스 함수\n\\(\\text{gauss}(x) = \\dfrac{1}{\\sqrt{\\pi}} e^{-x^2}\\)\n\n\n디렉 델타 함수\n\\(\\delta (x)\\), 정의 1 참고\n\n\n\n\n\n\n\n\n\n\n\n기본적인 신호들\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n정의 1 (디렉 델타 함수) Dirac 델타 함수 (델타 함수) \\(\\delta (t)\\) 는 다음의 두가지 성질을 갖는 함수로 정의된다.\n  (\\(1\\)) \\(\\delta(t) = \\left\\{ \\begin{array}{ll} \\infty \\qquad &\\text{if } t= 0 \\\\ 0 & \\text{otherwise.} \\end{array}\\right.\\)\n  (\\(2\\)) \\(\\mathbb{R}\\) 에서 정의된 함수 \\(f(t)\\) 에 대해 \\(\\displaystyle \\int_{-\\infty}^\\infty  \\delta(t)\\, f(t) \\, dt = f(0)\\).\n\n\n\n\n\n\n명제 1 (델타 함수의 성질) 델타 함수는 다음과 같은 성질을 가진다.\n  (\\(1\\)) \\(\\delta(-x) = \\delta (x)\\).\n  (\\(2\\)) \\(\\displaystyle \\int_{-\\infty}^\\infty f(x)\\,\\delta(x-x_0)\\, dx = f(x_0)\\).\n  (\\(3\\)) \\(a\\ne 0\\) 에 대해 \\(\\delta (ax) = \\dfrac{1}{|a|} \\delta (x)\\).\n  (\\(4\\)) \\(\\displaystyle \\int_{-\\infty}^\\infty f(x) \\,\\delta'(x)\\, dx = - f'(0)\\)\n  (\\(5\\)) \\(g(x)\\) 가 미분가능하며 \\(g(x) = 0\\) 가 유한개의 근 \\(x_1,\\ldots,\\,x_n\\) 을 갖고, \\(g'(x_i)\\ne 0\\) 일 때\n\\[\n\\displaystyle \\delta (g(x))= \\sum_{i=1}^n \\dfrac{\\delta(x-x_i)}{|g'(x_i)|}.\n\\]\n  (\\(6\\)) \\(\\delta(x) = \\displaystyle \\dfrac{1}{2\\pi} \\int_{-\\infty}^\\infty e^{ikx}\\, dk\\).\n\n\n(증명). (\\(1\\)) \\(\\displaystyle \\int_{-\\infty}^\\infty f(x)\\, \\delta(-x)\\, dx = \\int_{-\\infty}^\\infty f(-x) \\, \\delta (x) \\, dx = f(0)\\).\n(\\(2\\)) \\(t=x-x_0\\) 로 치환하여 적분한다.\n(\\(3\\)) \\(t=ax\\) 로 치환하여 적분한다. \\(a&lt;0\\) 인 경우\n\\[\n\\int_{-\\infty}^\\infty f(x)\\, \\delta(ax) \\, dx = \\dfrac{1}{|a|}\\int_{-\\infty}^\\infty f(x/a) \\delta (x)\\, dx = \\dfrac{1}{|a|} f(0) = \\int_{-\\infty}^\\infty f(x) \\left(\\dfrac{1}{|a|} \\delta(x) \\right)dx\n\\]\n이다.\n(\\(4\\)) 부분적분을 이용한다.\n\\[\n\\int_{-\\infty}^\\infty f(x) \\, \\delta (x)\\,dx = f(x) \\delta(x){\\huge \\mid}_{-\\infty}^\\infty - \\int_{-\\infty}^\\infty f'(x)\\, \\delta (x)\\,dx = -f'(0).\n\\]\n(\\(5\\)) \\(g'(x_i) \\ne 0\\) 이므로 각각의 \\(x_i\\) 주위에 \\(g(x)\\) 가 전단사가 되는 \\((x_i-\\epsilon_i,\\, x_i+\\epsilon_i)\\) 구간이 존재한다. 이 구간에서의 \\(g(x)\\) 의 역함수를 \\(h_i(x)\\) 라고 하자. \\(g(x_i-\\epsilon_i),\\, g(x_i+\\epsilon_i)\\) 가운데 작은수와 큰 수를 각각 \\(t_1,\\,t_2\\) 라고 하면,\n\\[\n\\begin{aligned}\n\\int f(x) \\, \\delta(g(x))\\,dx &= \\sum_{i=1}^n \\int_{x_i - \\epsilon_i}^{x_i + \\epsilon_i} f(x)\\, \\delta(g(x))\\, dx \\\\\n&= \\sum_{i=1}^n \\int_{t_1}^{t_2} f(h_i(t))\\, \\delta(t) \\dfrac{dt}{|g'(h_i(t))|} \\\\\n&= \\sum_{i=1} \\dfrac{f(x_i)}{|g'(x_i)|} \\\\\n&= \\int_{-\\infty}^\\infty f(x) \\left(\\sum_{i=1}\\dfrac{\\delta(x-x_i)}{|g'(x_i)|}\\right)\\, dx\n\\end{aligned}\n\\]\n이다.\n(\\(6\\)) \\(d(x) = \\int_{-\\infty}^\\infty e^{ikx}\\, dk\\) 로 놓고 다음과 같이 적분을 할 수 있다.\n\\[\n\\begin{aligned}\nd(x) = \\int_{-\\infty}^\\infty e^{ikx}dk &= \\int_{-\\infty}^0 e^{ikx}\\, dk + \\int_0^{\\infty} e^{ikx}\\, dk \\\\\n&= \\int_0^\\infty e^{ikx}\\, dk + \\int_0^\\infty e^{-ikx}\\,dk \\\\\n&= \\lim_{\\epsilon \\to 0^+} \\int_{0}^\\infty (e^{ikx}+e^{-ikx})\\, e^{-\\epsilon k}\\, dk \\\\\n&= \\lim_{\\epsilon \\to 0^+} \\dfrac{2\\epsilon}{\\epsilon^2+x^2}\n\\end{aligned}\n\\]\n이 경우 \\(d(0) = +\\infty\\) 이며 \\(d(x\\ne 0) = 0\\) 이다. 우리는\n\\[\n\\displaystyle \\int_{-\\infty}^\\infty \\dfrac{\\epsilon}{x^2+\\epsilon^2}\\, dx = \\tan^{-1} (x/\\epsilon) {\\huge \\mid}_{-\\infty}^\\infty = \\pi\n\\]\n임을 안다. 즉 \\(\\epsilon\\) 값과 무관하게 \\(\\int_{-\\infty}^\\infty d(x)\\, dx = 2\\pi\\) 이다. 따라서 \\[\n\\int_{-\\infty}^\\infty f(x)\\, d(x) \\, dx = \\lim_{\\epsilon \\to 0^+} \\int_{-\\infty}^\\infty \\dfrac{2\\epsilon f(x)}{\\epsilon^2+x^2}\\, dx = 2\\pi f(0)\n\\]\n이므로 \\(d(x) = 2\\pi \\delta(x)\\) 이다. \\(\\square\\)\n\n\n\n\n\n우리가 자연적으로 들어오는 연속 시그널 \\(s(t)\\) 에 대해 \\(T\\) 간격으로 샘플링을 한다고 하자. 이 때 \\(t=0\\) 으로 부터 \\(Tk\\) 에서 샘플링된 신호 \\(\\tilde{s}_k\\) 는\n\\[\n\\tilde{s}_k = \\int_{-\\infty}^\\infty s(t) \\,\\delta(t-kT ) \\,dt\n\\tag{1}\\]\n로 부터 얻을 수 있다.\n그렇다면 \\(s(t)\\) 에 대한 샘플링을\n\\[\n\\tilde{s}(t) = \\sum_k \\int_{-\\infty}^\\infty s(t) \\delta (t-kT) \\, dt = \\int_{-\\infty}^\\infty s(t) \\left[ \\sum_{k} \\delta (t-kT)\\right]\\, dt\n\\tag{2}\\]\n로 부터 얻을 수 있다. 이 때 델타함수가 주기적으로 반복되는 \\(S_T(t) = \\sum_{k} \\delta (t-kT)\\) 를 샘플링 함수(sampling function) 혹은 주기적 임펄스 함수 (periodic impulse function) 이라고 한다.\n\n\n\n\n우리가 컴퓨터에서 다루는 대부분의 데이터는 이산데이터이다. 연속함수의 샘플링과 양자화로 얻어진 이산데이터일 수도 있고, 자체로서 이산적인 값을 갖는 데이터 일 수도 있다. 이산데이터는 수학에서 수열을 다룰 때 처럼 1차원의 경우 \\(s_k\\) 나 혹은 \\(s[k]\\) 로 표현하며, 2차원의 경우 \\(s_{i, j}\\) 나 \\(s[i, j]\\) 로 표현한다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "디지털 신호처리의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/introduction_to_signal_processing.html#sec-image_convolution_correlation",
    "href": "src/image_processing/introduction_to_signal_processing.html#sec-image_convolution_correlation",
    "title": "디지털 신호처리의 기초",
    "section": "2 Convolution 과 Correlation",
    "text": "2 Convolution 과 Correlation\n식 1 과 같은 형태의 함수는 매우 중요하다. 즉 두 함수 \\(f,\\,g\\) 에 대해 \\(\\int f(t) g(x-t) \\, dt\\) 는 \\(x\\) 에 대한 함수이며 convolution 이라 하고 \\(f \\ast g\\) 로 표현한다. 이와 유사한 것이 \\(\\int f(t) g(x+t)\\, dt\\) 로 정의되는 cross-correlation 혹은 줄여서 correlation 이며 \\(f\\otimes g\\) 로 표현한다.\n\n\n\n\n\n\n\n정의 2 (convolution 과 cross-correlation) 연속 함수 \\(f,\\,g\\) 에 대해 convolution \\(f \\ast g\\) 와 cross-correlation \\(f \\otimes g\\) 는 다음과 같이 정의된다.\n  (\\(1\\)) 1 차원 \\[\n\\begin{aligned}\n(f \\ast g) (x) &= \\int_{-\\infty}^\\infty f(t)\\, g(x-t)\\, dt, \\\\\n(f \\otimes g) (x) & = \\int_{-\\infty}^\\infty f(t)\\, g(x+t)\\, dt\n\\end{aligned}\n\\]\n  (\\(2\\)) 2차원\n\\[\n\\begin{aligned}\n(f \\ast g) (x, y) &= \\int_{-\\infty}^\\infty f(x,  y)\\, g(t-x, s-y)\\, dtds, \\\\\n(f \\otimes g) (x, y) & = \\int_{-\\infty}^\\infty f(x, y)\\, g(t+x, s+y)\\, dtds\n\\end{aligned}\n\\]\n이산 데이터 \\(f,\\,g\\) 에 대해서는 다음과 같이 정의된다.\n  (\\(1\\)) 1 차원 \\[\n\\begin{aligned}\n(f \\ast g)[i] &= \\sum_n f[n]\\, g[i-n], \\\\\n(f \\otimes g)[i] &= \\sum_n f[n]\\, g[i+n], \\\\\n\\end{aligned}\n\\]\n  (\\(2\\)) 2차원 \\[\n\\begin{aligned}\n(f \\ast g)[i, j] &= \\sum_{m, n} f[m, n]\\, g[i-m, j-n], \\\\\n(f \\otimes g)[i, j] &= \\sum_{m,n} f[m, n]\\, g[i+m, j+n], \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n기본적인 성질\n\n명제 2 \\(f,\\,g,\\,h\\) 에 대해 다음이 성립한다.\n  (\\(1\\)) \\(f\\ast g = g \\ast f\\).\n  (\\(2\\)) \\(f \\ast (g \\ast h) = (f \\ast g) \\ast h\\).\n  (\\(3\\)) \\(f \\ast (g+h) = f\\ast g + f \\ast h\\).\n\n\n(증명). (\\(1\\)) 연속함수인 경우 정의에서 \\(s=x-t\\) 로 치환해서 증명 할 수 있다.\n(\\(2\\)) (\\(1\\)) 에 의해 \\(f\\ast (g\\ast h) = (g \\ast h)\\ast f\\) 이다. 그리고\n\\[\n\\int f(t-a)g(x-t)\\, dx = \\int f(t')\\,g(x-a-t')\\,dt = (f\\ast g)(x-a) \\tag{A}\n\\]\n를 이용한다. \\[\n\\begin{aligned}\nf \\ast (g \\ast h) &= \\int \\left[\\int h(t) g(x-t)\\, dt \\right] f(s-x)\\, ds \\\\\n&= \\int h(t) \\left[\\int g(x-t) f(s-x)\\, ds\\right] \\, dt &; (\\textrm{A}) \\\\\n&= \\int h(t) ((g \\ast f)(x-t)) \\, dt = (g \\ast f)\\ast h\n\\end{aligned}\n\\]\n(\\(3\\)) 쉽게 보일 수 있다.\n\n즉 convolution 에 대해서는 교환법칙, 결합법칙, 분배법칙이 성립한다. 그러나 cross-correlation 에 대해서는 분배법칙만 성립한다.\n우리가 직접 다루는 데이터는 크기가 한정된 이산적인 데이터이다. \\(f\\) 가 1 부터 \\(M\\) 까지의 index 를 가지며 \\(g\\) 가 1 부터 \\(N\\) 까지의 인덱스를 갖는 1차원 배열이라고 하자. $",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "디지털 신호처리의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/introduction_to_signal_processing.html#시스템",
    "href": "src/image_processing/introduction_to_signal_processing.html#시스템",
    "title": "디지털 신호처리의 기초",
    "section": "3 시스템",
    "text": "3 시스템\n어떤 신호 \\(s:A \\to X\\) 를 받아들여 이 신호에 대한 다른 신호 \\(g: A \\to X\\) 를 만들어내는 것을 시스템 (System) 이라고 한다. 즉 \\(g(x) = \\mathcal{L}[s(x)]\\) 일 때 \\(\\mathcal{L}\\) 이 시스템을 기술하는 변환이다.\n\n\n시스템의 종류\n\n선형 시스템\n시스템 \\(\\mathcal{L}\\) 이 신호 \\(s_1,\\,s_2\\) 와 신호 \\(s_1,\\,s_2\\) 에 대해\n\\[\n\\mathcal{L}[a_1 s_1 (x) + a_2 s_2(x)] = a_1 \\mathcal{L}[s_1(x)] + a_2 \\mathcal{L}[s_2(x)]\n\\]\n를 만족 할 때 이 시스템을 선형 시스템(linear system) 이라고 한다.\n\n\n\nTranslation invariant system\n임의의 \\(x_0\\) 에 대해\n\\[\n\\mathcal{L}[s(x-x_0)] = g(x-x_0)\n\\]\n일 때 이 시스템을 translation invariant system 이라고 한다. \\(x\\) 가 시간 변수이면 시간-불변 시스템, 공간 변수이면 공간 불변 시스템이라고 한다.\n\n\n\nLTI 시스템\n선형이며 Translation invariant 인 시스템을 LTI (Linear translation invariant) 시스템 이라고 한다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "디지털 신호처리의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_07.html",
    "href": "src/image_processing/image_processing_07.html",
    "title": "칼라 이미지 처리",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "칼라 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_07.html#색의-구분",
    "href": "src/image_processing/image_processing_07.html#색의-구분",
    "title": "칼라 이미지 처리",
    "section": "1 색의 구분",
    "text": "1 색의 구분\n\n1.1 색상 (hue, H), 채도 (saturation, S), 그리고 명 도 (brightness or value, V)\n\n색상은 여러 파장의 빛이 섞여 있을 때 관찰자 입장 에서 가장 눈에 띄는 대표적인 파장으로 나타나는 색을 의미하며,\n채도는 색상에 얼마나 높은 비율로 하얀색의 빛 (백색광)이 섞여 있는지를 의미하고 (즉, 하얀색은 채도가 0, 삼 원색이나 2차색 (CMY)은 채도가 1),\n명도는 색상 정보와 상관 없이 명암 이미지에서 말 하는 픽셀 값 (강도)를 의미합니다.\n\n\n\n\n1.2 CIE가 20세기 초반에 했던 실험\n\nCIE는 인간의 원추 세포가 적어도 단파장, 중파장, 장파장 의 세 가지 빛을 구분할 수 있다는 것을 알고 있었으므로, 원칙적으로 세 개의 변수 만 으로 인간이 인지할 수 있는 색감을 전부 표현할 수 있을 것이라는 가설을 세웠습니다.\n이를 기반으로, CIE는 XYZ 색 공간이라는 개념을 도입하였는데, X, Y, Z는 각 파장 (즉, 각 색깔)에 대응하는 삼색 자극값 (tristimulus values)을 의미합니다. 예를 들어, X 빨강, Y는 초록, Z는 파랑과 비슷한 색깔에 대응합니다. 인간의 색상 인지 특성 중 하나는 다양한 파장을 가진 두 가지 다른 색깔을 섞으면, 다른 색과 비슷하게 보일 수 있다는 것입니다. 이러한 현상을 조건등색 (metamerism) 이라고도 하는데, 두 개의 색깔이 가진 삼색 자극 값의 합은 두 개의 색깔이 갖는 파장과 상관 없이, 똑같아 보이는 다른 색의 삼색 자극 값과 동일함을 의미합니다.\n\n\n\n\n1.3 삼색자극갑과 RGB\n삼색 자극값 좌표 \\(\\begin{bmatrix} X &Y &Z \\end{bmatrix}\\) 를 인간이 인지하는 좌표인 \\(\\begin{bmatrix} R & G &B \\end{bmatrix}\\) 로 바꾸는 것 (혹은 역변환은)은 다음의 행렬 연산 과정을 거치면 됩니다.\n\\[\n\\begin{aligned}\n\\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} & = \\begin{bmatrix} 3.063  & -1.393  & -0.476 \\\\ -0.969 & 1.876 &  0.042 \\\\ 0.068  & -0.229 & 1.069 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} \\\\\n{}\\\\\n\\begin {bmatrix} X\\\\ Y \\\\ Z \\end{bmatrix} &=\\begin{bmatrix} 0.431  & 1.342  & 0.178 \\\\ 0.222 & 0.797 &  0.071 \\\\ 0.020  & 0.130 & 0.939 \\end{bmatrix} \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix}\n\\end{aligned}\n\\]\n참고로 \\(C,\\,M,\\,Y\\) 는 다음과 같이 계산된다. (C=청록, M=분홍, Y= 노랑 으로 색소의 삼원색이다.)\n\\[\n\\begin{bmatrix} C\\\\ M \\\\ Y \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} R\\\\ G \\\\ B \\end{bmatrix}\n\\]\n\n\n\n1.4 RGB to HSV\n\\[\n\\begin{aligned}\nV &= \\max(R,G,B),\\,\\\\\n\\delta &= V -\\min (R,G,B) ,\\,\\\\\nS &= \\dfrac{\\delta}{V} \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH &= \\left \\{ \\begin{array}{ll}\n\\dfrac{G-B}{6\\delta} \\qquad&\\text{when } V=R \\\\\n\\dfrac{1}{6} \\left( 2 + \\dfrac{B-R}{\\delta} \\right) & \\text{when } V=G \\\\\n\\dfrac{1}{6} \\left(4 + \\dfrac{R-G}{\\delta} \\right) & \\text{when } V=B\n\\end{array}\n\\right.\n\\end{aligned}\n\\]\n만약 \\(H\\) 가 음수이면 \\(1+H\\) 값이 대신한다. 또한 다음과 같이 정의한다. \\[\n\\begin{bmatrix} R & G & B \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix} \\implies \\begin{bmatrix} H & S & V \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix}\n\\]\n\n\n1.5 HSV to RGB\n\\[\n\\begin{aligned}\nH' & =  [6H ], \\\\\nF &= 6H - H', \\\\\nP &= V (1 -S) , \\\\\nQ &= V (1- SF) ,\\\\\nT &= V(1-S(1-F)).\n\\end{aligned}\n\\]\n여기서 \\([6H]\\) 는 소위 가우스 기호로 \\(6H\\) 에 가장 가까운 정수값을 의미한다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "칼라 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_07.html#가상-칼라-이미징-pseudocoloring",
    "href": "src/image_processing/image_processing_07.html#가상-칼라-이미징-pseudocoloring",
    "title": "칼라 이미지 처리",
    "section": "2 가상 칼라 이미징 (Pseudocoloring)",
    "text": "2 가상 칼라 이미징 (Pseudocoloring)",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "칼라 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_05.html",
    "href": "src/image_processing/image_processing_05.html",
    "title": "이미지 복원 및 재구성",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 복원 및 재구성"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_05.html#이미지-복원",
    "href": "src/image_processing/image_processing_05.html#이미지-복원",
    "title": "이미지 복원 및 재구성",
    "section": "1 이미지 복원",
    "text": "1 이미지 복원\n\n1.1 이미지 정보의 열화, 왜곡 원인\n\n보통 환경적인 요인,\n잡음 (noise),\n이미지 센서의 열화 (degradation),\n화각의 불균형으로 인한 기하학적 왜곡 (distortion)\n\n\n\n\n1.2 수학적 가정\n\n이미지 열화 (degradation)의 원인을 이미지에 작용하는 어떤 선형이고 위치불변인 ’연산자 (operator)’로 표현 할 수 있음.\n\n\n\n선형성 (Linearity)\n행렬에 대한 연산자 \\(\\hat{H}\\) 가 두 행렬 \\(f_1(x,\\,y),\\,f_2 (x,\\,y)\\) 와 임의의 스칼라 \\(a,\\,b\\) 에 대해 다음을 만족시킬 때 \\(\\hat{H}\\) 을 선형연산자라 한다.\n\\[\n\\hat{H} [af_1(x,\\,y)+bf_2(x,\\,y)]= a\\hat{H} [f_1(x,\\,y)] + b\\hat{H}[f_2 (x,\\,y)]\n\\]\n\n\n\n위치불변성 (Position invariance)\n연산자 \\(\\hat{H}\\) 가 이미지 행렬 \\(f(x,\\,y)\\) 에 대해 다음을 만족하면 \\(\\hat{H}\\) 를 위치불변 (position -invariant) 라 한다.\n\\[\ng(x,\\,y) = \\hat{H}[f(x,\\,y)] \\implies \\hat{H}[f(x-a,\\,y-b)] = g(x-a,\\,y-b)\n\\]\n\n이제 선형이고 위치불변인 연산자를 LPI (Linear and Position-Invariant) 연산자라 하자.\n\n\n\n\n\n1.3 역공간에서의 표현\n\n\\(f(x,\\,y) = \\mathfrak{I}^{-1}[F(u,\\,v)]\\) : 원본 이미지\n\\(h(x,\\,y) = \\mathfrak{I}^{-1}[H(u,\\,v)]\\) : 열화 원인 연산자\n\\(\\eta (x,\\,y) = \\mathfrak{I}^{-1} [N(u,\\,v)]\\) : 노이즈 행렬\n\\(g(x,\\,y) = \\mathfrak{I}^{-1}[G (u,\\,v)]\\) : 최종 이미지\n\n라 할 때 실공간과 역공간에서 표현하면 다음과 같은 관계식을 만족한다.\n\\[\n\\begin{aligned}\ng(x,\\,y) &= h(x,\\,y) \\ast f(x,\\,y) + \\eta(x,\\,y)\\\\\nG(u, v) &= H (u, v)  F (u, v) + N (u, v)\n\\end{aligned}\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 복원 및 재구성"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_05.html#노이즈-행렬",
    "href": "src/image_processing/image_processing_05.html#노이즈-행렬",
    "title": "이미지 복원 및 재구성",
    "section": "2 노이즈 행렬",
    "text": "2 노이즈 행렬\n\n2.1 노이즈의 원인에 따른 분류\n\n백색 잡음 (white noise) : 역공간에서 잡음 행렬의 분포가 일정\n가우시안 잡음 (Gaussian noise) : 온도 분포의 불균일성, 광량의 불균일성 등으로 인해 전자 회로의 노이즈, 센서의 노이즈 때문에 주로 발생\n레일리 잡음 (Rayleigh noise) : 레인지 이미징 (range imaging, High Dynamic Range imaging ,HDRI)에서 주로 발생\n\n\n레인지 이미징 : 주로 피사체의 광량 분포가 불균일할 때, 그것을 균일하게 만들어 주는 이미징을 의미합니다.\n\n\n\n\n2.2 많이 사용되는 노이즈 확률 분포\n\n가우시안 잡음(Gaussian noise)\n레일리 잡음 (Rayleigh noise),\n에를랑 잡음 (Erlang noise or gamma noise),\n지수적 잡음 (exponential noise),\n균일 잡음 (uniform noise),\n임펄스 잡음 (impulse noise)\n\n\n\n가우시안 잡음의 확률분표\n\\[\np(z) =  \\dfrac{1}{\\sqrt{2\\pi \\sigma}} e^{(z - \\langle z \\rangle)^2/2\\sigma^2 }\n\\]\n여기서 \\(\\langle z \\rangle, \\, \\sigma^2\\) 는 가우시안 잡음의 평균과 분산\n\n\n\n레일리 잡음의 확률분포\n\\[\np(z) = \\left\\{\\begin{array}{ll} \\dfrac{2}{b}(z-a)^2 e^{-(z-a)^2/b} \\qquad & \\text{for }z \\ge a \\\\ 0 & \\text{for }z&lt;a \\end{array} \\right.\n\\]\n이 때, \\(z\\) 는 픽셀 값, \\(a\\) 는 픽셀값의 문턱 값이다. 이 확률분포의 평균과 분산은 다음과 같다.\n\\[\n\\langle z \\rangle = a+ \\left(\\dfrac{\\pi b}{4}\\right)^{1/2}, \\qquad \\sigma^2 = \\dfrac{b(4-\\pi)}{4}\n\\]\n\n\n\n에를랑 잡음의 확률 분포\n\\[\np(z) = \\left\\{\\begin{array}{ll}\n\\dfrac{a^bz^{b-1}}{(b-1)!}e^{-az} \\qquad & \\text{for }z \\ge 0 \\\\\n0 & \\text{for } z&lt;0\n\\end{array} \\right.\n\\]\n이 확률분포의 평균과 분산은 다음과 같다.\n\\[\n\\langle z \\rangle = \\dfrac{b}{a},\\qquad \\sigma^2 = \\dfrac{b}{a^2}\n\\]\n\n\n\n지수적 잡음\n\\[\np(z) = \\left\\{\\begin{array}{ll}\nae^{-az} \\qquad & \\text{for }z \\ge 0 \\\\\n0 & \\text{for } z&lt;0\n\\end{array} \\right.\n\\]\n이 확률분포의 평균과 분산은 다음과 같다.\n\\[\n\\langle z \\rangle = \\dfrac{1}{a},\\qquad \\sigma^2 = \\dfrac{1}{a^2}\n\\]\n\n\n\n균일 잡음\n\\[\np(z) = \\left\\{\\begin{array}{ll}\n\\dfrac{1}{b-a} \\qquad & \\text{for } a\\le z \\le  b \\\\\n0 & \\text{otherwise }\n\\end{array} \\right.\n\\]\n이 확률분포의 평균과 분산은 다음과 같다.\n\\[\n\\langle z \\rangle = \\dfrac{a+b}{2},\\qquad \\sigma^2 = \\dfrac{(b-a)^2}{12}\n\\]\n\n\n\n임펄스 잡음\n\\[\np(z) = \\left\\{\\begin{array}{ll} P_a \\qquad & \\text{for }z=a \\\\\nP_b & \\text{for } z=b \\\\ 0 &\\text{otherwise} \\end{array} \\right.\n\\]\n\nSalt-and-Pepper (S&P) 노이즈가 대표적",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 복원 및 재구성"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_05.html#임펄스-신호-와-lpi-연산자",
    "href": "src/image_processing/image_processing_05.html#임펄스-신호-와-lpi-연산자",
    "title": "이미지 복원 및 재구성",
    "section": "3 임펄스 신호 와 LPI 연산자",
    "text": "3 임펄스 신호 와 LPI 연산자\n임펄스 신호 \\(\\delta (x-\\alpha,\\,y-\\beta)\\) 를 LPI 연산자 \\(\\hat{H}\\) 에 적용한 것을 \\(h(x,\\,\\alpha,\\,y,\\,\\beta)\\) 라 하자. 즉\n\\[\nh(x,\\,\\alpha,\\,y,\\,\\beta) = \\hat{H}[\\delta (x-\\alpha,\\,y-\\beta)]\n\\]\n이다. \\(\\delta\\) 함수의 성질에 의해 다음이 성립함을 안다.\n\\[\nf(x,\\,y) = \\iint f(\\alpha,\\, \\beta) \\,\\delta(x-\\alpha,\\, y-\\beta)\\, d\\alpha\\,d\\beta\n\\]\n이제 이미지 \\(f(x,\\,y)\\) 에 \\(\\hat{H}\\) 를 적용하면,\n\\[\n\\begin{aligned}\ng(x,\\,y) &= \\hat{H}[f(x,\\,y)] \\\\\n&= \\hat{H} \\left[ \\iint f(\\alpha,\\, \\beta) \\,\\delta(x-\\alpha,\\, y-\\beta)\\, d\\alpha\\,d\\beta \\right] \\\\\n&= \\iint \\hat{H} [f(\\alpha,\\, \\beta) \\,\\delta(x-\\alpha,\\, y-\\beta)] \\, d\\alpha\\,d\\beta \\\\\n&= \\iint f(\\alpha,\\, \\beta) \\,\\hat{H} [\\delta(x-\\alpha,\\, y-\\beta)] \\, d\\alpha\\,d\\beta \\\\\n&= \\iint f(\\alpha,\\, \\beta) \\,h(x,\\,\\alpha,\\, y,\\,\\beta)] \\, d\\alpha\\,d\\beta \\\\\n&= \\iint f(\\alpha,\\,\\beta) \\,h(x-\\alpha,\\, y-\\beta)\\, d\\alpha\\, d\\beta\\\\\n&= h (x,\\,y) * f(x,\\,y)\n\\end{aligned}\n\\]\n위 식에서 적분 내에서 \\(h(x,\\,\\alpha,\\, y,\\,\\beta) \\to h(x-\\alpha,\\, y-\\beta)\\) 로 변환되는 데는 Fredholm integral of the first kind 를 사용하였다. \\(h(x-\\alpha,\\, y-\\beta) = h(x,\\,\\alpha,\\,y,\\,\\beta)\\) 를 점 확산 함수 (point spread function, PSF) 이라 한다.\n\n푸리에 변환의 성질에 의해 역공간에서는\n\\[\nG(u,\\,v) = H(u,\\,v) F(u,\\,v)\n\\]\n가 성립한다.\n\n이를 조금 더 알기 쉽게 설명하자면 이렇습니다. 어떤 이미지가 열화된 것처럼 보인다면, 그 원인에 해당하는 특정한 LPI(라고가정할수있는) 열화연산자 \\(\\hat{H}\\)가 있을것입니다.그리고 또 그것에 해당하는 고유의 점확산함수 \\(h(x,y)\\)가 있을 것이고요. 점,즉 픽셀값이 확산되는 경향 (강도, 방향 등)이 바로 이미지가 열화되는 정도를 결정하는데, 그 정도는 점 확산 함수와 원본 이미지 행렬의 합성곱으로 결정됩니다. 다시 말해, 이미지에 있는 모든 픽셀 (즉, ‘점’)이 얼마나 퍼질 수 있는지 그래서 얼마나 왜곡이 되고 열화되는지가 결과적으로 LPI 연산자의 함수 특성에 의해 결정되는 것입니다.\n\n\n이제 이미지의 열화를 야기한 연산자에 대한 사전 정보가 없을 때, 연산자 정보를 어떻게 추정할 수 있을지 생각해 봅시다. 열화과정은 이미지를 관찰하거나, 실험해 보거나, 수학적으로 모델링함으로써 추정할 수 있습니다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 복원 및 재구성"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_05.html#열화-연산자-모델",
    "href": "src/image_processing/image_processing_05.html#열화-연산자-모델",
    "title": "이미지 복원 및 재구성",
    "section": "4 열화 연산자 모델",
    "text": "4 열화 연산자 모델\n\n\n4.1 공기의 흔들림 (가우시안) 에 의한 이미지 열화\n흔들림 연산자는 수학적으로 다음과 같은 식이 많이 쓰인다. \\[\nH(u,\\,v) = e^{-k(u^2+v^2)^{5/6}}\n\\]\n\n\n4.2 피사체의 흔들림에 의한 열화 (motion blur)\n피사체가 \\(T\\) 의 시간동안 카메라에 노출되어 \\(x,\\,y\\) 방향으로 각각 \\(x_0(t),\\, y_0(t)\\) 만큼 움직이는 이미지는 다음과 같이 표현된다.\n\\[\ng(x,\\,y) = \\int_0^T f[x-x_0(t),\\, y-y_0(t)]\\, dt\n\\]\n이에 대한 푸리에 변환은 다음과 같다.\n\\[\n\\begin{aligned}\nG(u,\\,v) &= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,\\,y)\\, e^{-2i\\pi (ux+vy)}\\,dx\\,dy \\\\\n&=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\left[ \\int_0^Tf(x,\\,y) e^{-2i\\pi(ux+vy)}e^{-2i\\pi(ux_0(t)+vy_0(t))}\\,\\, dt\\right] dx\\,dy \\\\\n&=F(u,\\,v) \\int_0^T e^{-2i\\pi(ux_0(t)+vy_0(t))}\\, dt = F(u,\\,v) H(u,\\,v) \\\\\n&\\qquad \\qquad \\qquad \\text{where }H(u,\\,v) =\\int_0^T e^{-2i\\pi(ux_0(t)+vy_0(t))}\\, dt\n\\end{aligned}\n\\]\n즉 피사체 움직임에 의한 열화 LPI 연산자는 위 식의 \\(H(u,\\,v)\\) 이다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 복원 및 재구성"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_05.html#motion-bluring",
    "href": "src/image_processing/image_processing_05.html#motion-bluring",
    "title": "이미지 복원 및 재구성",
    "section": "5 Motion Bluring",
    "text": "5 Motion Bluring\n카메라에 \\(T\\) 의 시간만큼 노출되며 이 시간동안의 피사체의 움직임을 \\(x_0(t),\\,y_0(t)\\) 로 기술 할 수 있다고 하자. \\(f(x,\\,y)\\) 를 정지상태의 이상적인 피사체의 이미지라 하면, 이 때 피사체의 이미지 \\(g(x,\\,y)\\) 는 다음과 같이 기술된다.\n\\[\ng(x,\\,y) = \\int_0^T f(x-x_0(t),\\, y-y_0(t))\\, dt\n\\] 이를 푸리에 변환하면,\n\\[\n\\begin{aligned}\nG(u,\\,v) &= \\mathfrak{I}[g(x\\,y)] = \\iint g(x,\\,y) e^{-2i\\pi (ux+vy)}\\,dxdy\\\\\n&= \\iint \\left[ \\int_0^T f(x-x_0(t),\\, y-y_0(t))\\, dt\\right] e^{-2i\\pi (ux+vy)}\\,dxdy\\\\\n&= \\int_0^T \\left[\\iint f(x-x_0(t),\\, y-y_0(t)  e^{-2i\\pi (ux+vy)}\\,dxdy\\right] \\, dt\\\\\n&= \\int_0^T F(u,\\,v)  e^{-2i\\pi (ux_0(t)+vy_0(t))}\\,dxdy \\\\\n&= \\int F(u,\\,v) H(u,\\,v),\\qquad \\text{where}\\quad H(u,\\,v) = \\int_0^T e^{-2i\\pi (ux_0(t)+vy_0(t))}\\,dxdy\n\\end{aligned}\n\\]\n즉 피사체의 열화 LPI 연산자 $H(u,,v) = _0^T e^{-2i(ux_0(t)+vy_0(t))},dxdy $ 를 알 수 있다. 만약 피사체가 속도 \\((a/T,\\, b/T)\\) 의 단순선형등속은동을 한다면, \\(x_0(t) = at/T,\\, y_0(t) =bt/T\\) 이므로,\n\\[\nH(u,\\,v) = \\int_0^T e^{-2i\\pi(au+bv)t/T}\\, dt=\\dfrac{T}{au+bv} \\sin \\left[\\pi(au+bv)\\right]e^{-2i\\pi(au+bv)}\n\\]\n가 된다. 이로부터,\n\\[\nf(x,\\,y) = \\mathfrak{I}^{-1}[F(u,\\,v)] = \\mathfrak{I}^{-1}\\left[G(u,\\,v)/H(u,\\,v)\\right]\n\\]\n의 관계를 통해 \\(f(x,\\,y)\\) 를 복원 할 수 있다.\n\n\n5.1 Inverse filtering 을 통한 motion blur 복원\n피사체의 움직임을 단순한 선형 등속 움직임 즉, \\(x_0(t) = \\dfrac{at}{T},\\, y_0(t) = \\dfrac{bt}{T}\\) 라고 표현 할 수 있다고 가정하면,\n\\[\nH(u,\\,v) = \\int_0^T e^{-2i\\pi (ux_0(t)+vy_0(t))} \\, dt = \\dfrac{T}{\\pi (ua+vb)} \\sin (\\pi (ua+vb)) e^{-i\\pi (ua+bv)}\n\\]\n이다.\n\n\n\n5.2 노이즈가 있는 이미지의 복원\n\n사실 노이즈를 처리한 후 열화 연산자를 추정하여 이미지를 복원하는 것은 이상적인 시나리오에 가깝다. 왜냐하면 노이즈의 종류와 열화 연산자의 정체를 미리 알고 있어야만 이러한 처리가 깔끔하게 이루어질 것이기 때문이다. 노이즈의 정체를 모르는 상황이라면 노이즈만 따로 전처리 (pre-treatment)하기 어려울 것이다.\n노이즈가 없는 이미지 \\(f(x,y)\\) 와 노이즈 \\(n(x,y)\\) 를 생각하자. 이미지는 \\(g(x,\\,y)= f(x,\\,y)+n(x,\\,y)\\) 가 될 것이다. 이 때\n\n\\[\n\\begin{aligned}\nF(u,v) &= \\mathfrak{I} [f(x,y)]\\\\\nN(u,v) &= \\mathfrak{I} [n(x,y)]\\\\\nG(u,v) &= \\mathfrak{I}[g(x,y)] = F(u,v)+N(u,v)\n\\end{aligned}\n\\]\n라 하자\n\n노이즈를 처리하지 않은 상황이라면 \\(H\\) 연산에 대해 처리된 결과는 \\(F(u,v)\\) 가 아닌 \\(\\hat{F}(u,\\,v) = F(u,v) +\\dfrac{N(u,v)}{H(u,v)}=\\dfrac{G(u,v)}{H(u,v)}\\) 가 될 것이다.\n\\(\\hat{F}(u,v)\\) 에서 \\(H(u,v)\\) 가 매우 작거나 어떤 노이즈에 의해 \\(N(u,v)\\) 가 매우 크다면 결과적으로 \\(F(u,v)\\) 보다 \\(\\dfrac{N(u,v)}{H(u,v)}\\) 가 더 지배적일 수 있다. 이를 막기 위한 방밥중에는 전에 배웠던 버터워스 필터링, 위너 필터링이 있다.\n\n\n\n\n5.3 계수조절 비너 필터링 (Parametric Wiener filtering)\n\n\n\n5.4 제한된 최소자승법 필터링 (Constrained least squarefiltering, CLSF)\n\n\n\n5.5 기하평균 필터링 (Geometric mean filtering)",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 복원 및 재구성"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_05.html#이미지-재구성-image-reconstruction",
    "href": "src/image_processing/image_processing_05.html#이미지-재구성-image-reconstruction",
    "title": "이미지 복원 및 재구성",
    "section": "6 이미지 재구성 (Image Reconstruction)",
    "text": "6 이미지 재구성 (Image Reconstruction)\n\n\n6.1 이미지 복원과 이미지 재구성\n\n이미지 복원은 열화된 이미지를 최대한 품질을 개선시키는 방법 에 초점을 맞췄다면, 이미지 재구성은 여러 장의 이미지 혹은 여러 방향에서 찍은 이미 지를 하나의 이미지로 재구축하는 과정에 초점을 맞춥니다.\n이미지 재구성은 CT (computer tomography) 같은 첨단 의료영상정보 처리에 활발하게 활용되는 매우 중요 한 이미지 처리 방법입니다.\n\n\n\n\n6.2 후방 사영법 (Backprojection)\n\n피사체 앞에서 빔을 나란히 쏜 후, 피사체를 통과한 빔의 강도를 후 방에서 모아서 하나의 이미지로 만드는 방법.\n피사체의 부분마다 빔에 대한 투과도 (transmission coefficient)나 흡광도 (absorption coefficient)가 다르다는 것에 착안한 방법.\n예를 들어 암세포는 다른 세포에 비해 X-ray를 더 잘 흡수하는 경향이 있는데, 그 세포를 향해 X-ray를 쏜다면, 세포 뒤에 건판에는 서로 다른 강도의 X-ray 신호가 나타날 것이다. 그리고 그 세포를 향해 쏘는 X-ray의 방향을 계속 돌려 보면, 그 세포의 상대적 위치에 대해 모든 정보를 얻을 수 있을 것이다.\n후방 사영법은 단순하지만 후방 사영하는 각도 단위가 작아질수록 (즉, 해상력 이 높아질수록) 재구축되는 이미지가 더 정교해진다는 장점이 있다.\n그럼에도 불구하고, 후방 사영법만으로는 피사체의 디테일을 전부 다 살리기 어려운데, 그 이유는 후방 사영법의 특징 상, 사영된 이미지의 겹침 과정에서 피사체의 경계 부분이 블러링되는 것을 피할 수 없기 때문. 이 때문에, 후방 사영법을 필터링하는 알고리듬이 필요하다.\n\n\n\n\n6.3 라돈 변환 (Radon Transform)\npython 에서는 opencv 에서는 제공하지 않는 것 같고, skimage 에서 제공한다. 역변환인\nfrom skimage.transform import radon\n# get sinogram by radon transform\nsinogram = radon(img1, theta)\n\n# reconstruct by iradon\nreconstruction_img = iradon(sinogram, theta=theta, filter='ramp')\n으로 사용 할 수 있다.\n아래 radon transform 은 아래 사이트를 참고하라\nhttps://scikit-image.org/docs/dev/auto_examples/transform/plot_radon_transform.html",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 복원 및 재구성"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_02.html",
    "href": "src/image_processing/image_processing_02.html",
    "title": "푸리에 변환을 이용한 이미지 처리",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "푸리에 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_02.html#푸리에-급수와-푸리에-변환의-수학적-성질",
    "href": "src/image_processing/image_processing_02.html#푸리에-급수와-푸리에-변환의-수학적-성질",
    "title": "푸리에 변환을 이용한 이미지 처리",
    "section": "1 푸리에 급수와 푸리에 변환의 수학적 성질",
    "text": "1 푸리에 급수와 푸리에 변환의 수학적 성질\n푸리에 변환은 이미지 처리 뿐만 아니라 수치 해석에서도 아주 중요하기 때문에 여기서 확실하게 정리하기로 하자.\n\n1.1 푸리에 급수\n어떤 함수가 주기 \\(T\\) 를 갖는다는 것은 모든 \\(t\\in \\mathbb{R}\\) 에 대해 \\(f(t+T)=f(T)\\) 임을 의미한다. 주기 \\(T\\) 에 대해 \\(\\nu = 1/T\\) 를 진동수(frequency) 라고 하고 \\(\\omega = 2\\pi/T = 2\\nu\\) 를 각진동수 (angular frequency) 라고 한다. 이 경우 \\(f(t)\\) 는 다음과 같이 표현 할 수 있다.\n\n\n\n\n\n\n\n정의 1 (실함수의 푸리에 급수) 주기 \\(T\\) 를 갖는 실함수 \\(f: X \\subset \\mathbb{R} \\to \\mathbb{R}\\) 는 다음과 같은 푸리에 급수로 표현 할 수 있다. 여기서 \\(\\omega = 2\\pi/T\\) 이다.\n\\[\n\\begin{aligned}\nf(t) &= \\dfrac{a_0}{2} + \\sum_{n=1}^{\\infty} a_n \\cos (n\\omega t) + \\sum_{n=1}^{\\infty} b_n \\sin (n \\omega t)\\,,\\\\\n\\text{where}\\qquad  a_n &= \\dfrac{2}{T}\\int_{-T/2}^{T/2} f(t) \\cos (n\\omega t)\\, dt\\\\\nb_n &= \\dfrac{2}{T}\\int_{-T/2}^{T/2} f(t) \\sin (n\\omega t)\\, dt.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n정의 2 (푸리에 급수) 주기 \\(T\\) 를 갖는 함수 \\(f(t)\\) 는 다음과 같은 푸리에 급수로 표현 할 수 있다. 여기서 \\(\\omega = 2\\pi/T\\) 이다.\n\\[\n\\begin{aligned}\nf(t) &= \\sum_{n=-\\infty}^{\\infty} c_n e^{in\\omega t}\\,,\\\\\n\\text{where}\\qquad c_n &= \\dfrac{1}{T}\\int_{-T/2}^{T/2} f(t) e^{-in\\omega t}\\, dt\\,, \\qquad n\\in \\mathbb{Z}\n\\end{aligned}\n\\]\n\n\n\n\n\n정의 1 는 실함수의 경우, 정의 2 는 실함수를 포함한 복소함수의 경우에 사용 할 수 있다. 아래 그림은 두가지 함수에 대한 푸리에 급수를 \\(n=1,\\ldots,\\,4\\) 까지 표한 것이다.\n\n\n\n\n\n\n그림 1: 푸리에 급수\n\n\n\n\n\\([-T/2,\\, T/2]\\) 구간에서 연속인 함수의 집합 \\(X\\) 를 생각하자. 이 집합은 벡터공간이다. 여기에 내적을 다음과 같이 정의한다.\n\\[\n\\langle \\phi,\\, \\psi \\rangle = \\int_{-T/2}^{T/2} \\psi^\\ast(t)\\, \\phi(t)\\, dt\n\\]\n수학적으로 푸리에 급수가 의미 있기 위해서는 \\(\\omega = 2\\pi/T\\) 에 대해 \\(\\{e^{in\\omega t} : n\\in \\mathbb{Z}\\}\\) 가 \\([T/2,\\,T/2]\\) 구간에서 연속인 모든 복소 함수의 집합에 대해 정규직교기저이어야 한다. 직교성은 보이기 쉽지만 completeness 즉, 주기 \\(T\\) 인 모든 함수가 \\(\\{\\sin (n\\omega t),\\, \\cos (n \\omega t) : n \\in \\mathbb{Z}\\}\\) 의 선형결합으로 표현되는 것은 여기의 범위를 벗어난다. 단지 직교성과 completeness 를 알고 넘어가자.\n\n\n1.2 기저함수의 직교성\n\\[\n\\int_{-T/2}^{T/2} \\exp \\left( \\dfrac{i 2\\pi m}{T}t\\right) \\exp \\left(-\\dfrac{i 2\\pi n}{T}t \\right)\\, dt = \\delta_{nm}\n\\tag{1}\\]\n\n\n\n1.3 주기함수와 푸리에 급수\n\n모든 주기성을 갖는 연속함수는 푸리에 급수로 표현 할 수 있다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "푸리에 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_02.html#푸리에-변환",
    "href": "src/image_processing/image_processing_02.html#푸리에-변환",
    "title": "푸리에 변환을 이용한 이미지 처리",
    "section": "2 푸리에 변환",
    "text": "2 푸리에 변환\n\n\n\n\n\n\n\n정의 3 (1차원 푸리에 변환) 함수 \\(f:\\mathbb{R}\\to \\mathbb{R}\\) 에 대한 푸리에 변환은 \\(\\mathfrak{F}[f(t)]\\) 로 쓰고 다음과 같이 정의된다. 푸리에 변환에 대한 역변환 \\(\\mathfrak{F}^{-1}\\) 도 아래와 같이 정의된다.\n\\[\n\\begin{aligned}\n\\mathfrak{F}\\left[ f(t)\\right] (\\omega) &:= \\int_{-\\infty}^{\\infty} f(t) \\, e^{-2i\\pi u t} \\, dt,\\\\\n\\mathfrak{F}^{-1}[F(u)](t) &:= \\int_{-\\infty}^{\\infty} F(u)\\, e^{2i \\pi u t} \\, du\\,.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n정의 4 (2차원 푸리에 변환) 함수 \\(f:\\mathbb{R}^2 \\to \\mathbb{R}\\) 에 대한 푸리에 변환은 \\(\\mathfrak{F}[f(x, y)]\\) 로 쓰고 다음과 같이 정의된다. 푸리에 변환에 대한 역변환 \\(\\mathfrak{F}^{-1}\\) 도 아래와 같이 정의된다.\n\\[\n\\begin{aligned}\n\\mathfrak{F} \\left[f(t)\\right] (u,\\,v) &:=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x,\\,y)e^{-2i\\pi (ux+vy)}\\, dxdy\\,, \\\\\n\\mathfrak{F}^{-1} \\left[F(u,\\,v)\\right](x,\\,y) &:= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} F(u, v) e^{2i \\pi (ux+vy)} \\, dudv\n\\end{aligned}\n\\]\n\n\n\n\n\n\\(F(u,\\,v) = \\mathfrak{F}[f](u,\\,v)\\) 는 복소수이므로 다음과 같이 표현 할 수 있다.\n\\[\nF(u,\\,v) = |F(u,\\,v)| e^{i\\Phi(u,\\,v)}\n\\]\n이 때 \\(|F(u,\\,v)|\\) 를 푸리에 스펙트럼 (Fourier spectrum) 이라 하고, \\(\\Phi(u,\\,v)\\) 를 위상 (phase) 이라 한다. 또한 \\(P(u,\\,v) =|F(u,\\,v)|^2\\) 를 푸리에 파워 스펙트럼 (Fourier power spectrum) 이라 한다.\n\n\n2.1 푸리에 변환의 몇가지 수학적 성질\n2차원 푸리에 변환에 대해서 증명한다. 1차원에 대해서는 그 결과를 쉽게 유추 할 수 있다.\n\n\n\n명제 1 \\(F=\\mathfrak{F}[f],\\, G=\\mathfrak{F}[g]\\), 상수 \\(a,\\,b\\) 에 대해 다음이 성립한다.\n1. Linearity : \\(\\mathfrak{F}[af(x, y) + bg(x, y)] = aF(u,\\, v) + bG(u,\\,v)\\),\n2. Similarity : \\(\\mathfrak{F}[f(ax,\\,by)] =\\dfrac{1}{ab} F\\left(\\dfrac{u}{a},\\, \\dfrac{v}{b}\\right)\\),\n3. Shift property : \\(\\mathfrak{F}[f(x-a,\\, y-b)] = \\exp (i2\\pi (au+bv)) F(u,\\,v)\\),\n4. Derivative : \\(\\mathfrak{F}\\left[ \\dfrac{d^k}{dx^k}f(x) \\right] =( 2i\\pi u)^k F(u)\\),\n\n\n\n\n(증명). (\\(1\\)) \\[\n\\begin{aligned}\n\\mathfrak{F}[af(x, y) + bg(x, y)]&= \\iint (af(x, y) + bg(x, y)) e^{-2i\\pi (ux+vy)}\\, dxdy \\\\\n&= \\iint af(x, y)  e^{-2i\\pi (ux+vy)}\\, dxdy + \\iint  bg(x, y) e^{-2i\\pi (ux+vy)}\\, dxdy  \\\\\n&=  a\\mathfrak{F}[f(x, y)] + b\\mathfrak{F}[g(x, y)] \\qquad \\square\n\\end{aligned}\n\\]\n(\\(2\\)) \\[\n\\begin{aligned}\n\\mathfrak{F}[f(ax,\\,by)] &= \\iint f(ax,\\, by) e^{-2i\\pi (ux+vy)}\\,dxdy \\qquad &; t=ax,\\, s=by\\\\\n&= \\dfrac{1}{ab} \\iint f(s,\\,t) e^{-2i \\pi (ut/a+vs/b)}\\, dtds \\\\\n&= \\dfrac{1}{ab} F \\left(\\dfrac{u}{a},\\, \\dfrac{v}{b} \\right)\n\\end{aligned}\n\\]\n(\\(3\\)) \\[\n\\begin{aligned}\n\\mathfrak{F}[f(x-a,\\, y-b) ]&= \\iint f(x-a,\\, y-b) e^{-2i\\pi (ux+vy)} \\,dxdy \\qquad &; t=x-a,\\, s=y-b\\\\\n&=\\iint f(t,\\,s) e^{-2i\\pi (ux+vy)} e^{2i\\pi (au+bv)} \\, dxdy \\\\\n&= e^{2i\\pi (au+bv)}F(u,\\,v)\n\\end{aligned}\n\\]\n(\\(4\\)) \\(k=1\\) 인 경우만 보이면 나머지는 쉽게 일반화 된다.\\(f(x)=\\displaystyle \\int_{-\\infty}^{\\infty} F(u)e^{2i\\pi ux}\\, du\\) 이므로\n\\[\nf'(x) =  \\int_{-\\infty}^{\\infty}(2i\\pi u) F(u)e^{2i \\pi ux} \\, du\n\\]\n이다. 따라서\n\\[\n\\mathfrak{F}[f'(x)] = (2i\\pi u) F(u)\n\\] 이다. \\(\\square\\)\n\n\n\n\n예제 1 \\(\\text{rect}\\left(\\dfrac{x}{w}\\right)\\) 함수의 푸리에 변환은 다음과 같다.\n\\[\n\\begin{aligned}\nF(u) = \\mathfrak{F}\\left[\\text{rect}\\left(\\dfrac{x}{w}\\right)\\right] &= \\int_{-w/2}^{w/2} e^{-2\\pi i u t}\\, dt = \\dfrac{\\sin \\pi w u}{\\pi u}\n\\end{aligned}\n\\]\n이것을 그래프로 그리면 아래 그림과 같다.\n\n\n\n\n\n\n그림 2: rect 함수의 푸리에 변환\n\n\n\n\\(\\text{rect}\\left(\\dfrac{x}{w}\\right)\\) 함수의 폭은 \\(w\\) 이다. 그리고 \\(F(u) =0\\) 을 만족하는 주기는 \\(u=0\\) 일 때를 제외하면 \\(1/w\\) 이다. 즉 신호의 폭과, 푸리에 변환된 신호의 폭은 대략적으로 반비례 관계에 있다는 것을 알 수 있다.\n\n\n\n\n\n\n2.2 주기적 임펄스 함수의 푸리에 변환\n단위 임펄스 함수(디렉 델타 함수)가 \\(T\\) 를 주기로 반복되는 함수를 주기적 임펄스 함수라 하며 이를 \\(S_T(t)\\) 라 하자. 이는 주기함수이므로 푸리에 급수로 표현 할 수 있으며 다음과 같다.\n\\[\n\\begin{aligned}\nS_T(t) &= \\sum_{n=-\\infty}^{\\infty} \\delta \\left(t-nT \\right) =\\dfrac{1}{T} \\sum_{n=-\\infty}^{\\infty} \\exp \\left( \\dfrac{-i2n\\pi t}{T}\\right)\n\\end{aligned}\n\\]\n단일 임펄스 함수 \\(\\delta(\\mu-\\mu_0)\\) 에 대한 푸리에 변환은 다음과 같다.\n\\[\n\\mathfrak{F}[\\delta (\\mu-\\mu_0)] = \\int_{-\\infty}^{\\infty} \\delta (\\mu-\\mu_0) e^{2i\\pi \\mu t}\\, d\\mu = e^{2i\\pi \\mu_0 t}\n\\]\n따라서,\n\\[\n\\mathfrak{F}^{-1} \\left[ \\delta \\left( \\mu-\\dfrac{n}{T} \\right) \\right] = \\exp \\left( \\dfrac{2in\\pi t}{t} \\right) \\iff \\mathfrak{F}\\left[  \\exp \\left( \\dfrac{2in\\pi t}{t} \\right) \\right] =  \\delta \\left( \\mu-\\dfrac{n}{T} \\right)\n\\]\n이다. 이를 이용하면 \\(s_T(t)\\) 의 푸리에 변환식 \\(S(\\mu)=\\mathfrak{F}[s_N(t)]\\) 를 계산 할 수 있으며 다음과 같다.\n\\[\nS (\\mu) = \\mathfrak{F}\\left[\\dfrac{1}{T} \\sum_{n=-\\infty}^{\\infty} \\exp \\left( \\dfrac{-i2n\\pi t}{T}\\right)\\right] = \\dfrac{1}{T} \\sum_{n=-\\infty}^{\\infty} \\mathfrak{F}\\left[ \\exp \\left( \\dfrac{-i2n\\pi t}{T}\\right) \\right] = \\dfrac{1}{T} \\sum_{n=-\\infty}^{\\infty} \\delta \\left(\\mu - \\dfrac{n}{T}\\right)\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "푸리에 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_02.html#합성곱의-푸리에-변환",
    "href": "src/image_processing/image_processing_02.html#합성곱의-푸리에-변환",
    "title": "푸리에 변환을 이용한 이미지 처리",
    "section": "3 합성곱의 푸리에 변환",
    "text": "3 합성곱의 푸리에 변환\n두 함수 \\(f(t),\\, g(t)\\) 의 convolution \\(f(t) \\ast g(t)\\) 은 다음과 같이 정의된다.\n\\[\nf(t) \\ast g(t) := \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) \\, d\\tau\n\\]\n\\(F(\\mu)= \\mathfrak{F}[f(t)],\\, G(\\mu)=\\mathfrak{F}[g(t)]\\) 라 할 때, \\(f(t)\\ast g(t)\\) 의 푸리에 변환을 구하면,\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f(t) \\ast g(t)] &= \\int_{-\\infty}^{\\infty} \\left[ \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau) d\\tau\\right] e^{-2in\\pi \\mu t}\\, dt \\\\\n&= \\int_{-\\infty}^{\\infty}f(\\tau) \\left[  \\int_{-\\infty}^{\\infty} g(t-\\tau)e^{-2in \\pi \\mu t}\\, dt\\right] \\,d\\tau \\\\\n&= \\int_{-\\infty}^{\\infty} f(\\tau) G(\\mu)e^{-2in \\pi \\mu \\tau} \\, d\\tau \\\\\n&= F(\\mu)G(\\mu)\n\\end{aligned}\n\\]\n즉 합성곱의 푸리에 변환은 푸리에 변환의 곱이다. 이를 응용하면 두 함수의 곱의 푸리에변환은 합성곱이 됨을 보일 수 있다. 즉,\n\\[\n\\mathfrak{F}[f(t)g(t)]=F(\\mu)\\ast G(\\mu)\n\\]\n가 된다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "푸리에 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_02.html#푸리에-변환과-샘플링-sampling",
    "href": "src/image_processing/image_processing_02.html#푸리에-변환과-샘플링-sampling",
    "title": "푸리에 변환을 이용한 이미지 처리",
    "section": "4 푸리에 변환과 샘플링 (sampling)",
    "text": "4 푸리에 변환과 샘플링 (sampling)\n\n4.1 샘플링\n임의의 연속함수 \\(f(t)\\) 를 \\(T\\) 를 주기로 샘플링 하였다고 하자. 그렇다면 이 샘플링 \\(\\tilde{f}(t)\\) 은 다음과 같은 식으로 표현 할 수 있다.\n\\[\n\\tilde{f}(t) = f(t)S_T(t)\\sum_{n=-\\infty}^{\\infty} f(t) \\delta (t-nT)\n\\]\n즉 연속 신호에 대해 주기적으로 신호를 뽑아 불연속 신호로 만드는 것을 sampling 이라 한다.\n\n\n\n4.2 샘플링 된 신호의 푸리에 변환\n이 때 \\(\\mathfrak{F}[f(t)]= F(\\mu),\\, \\mathfrak{F}[s_T(t)]= S(\\mu)\\) 라 하자 \\(\\displaystyle S(\\mu)=\\dfrac{1}{T} \\sum_{n=-\\infty}^{\\infty} \\delta \\left( \\mu - \\dfrac{n}{T}\\right)\\) 임은 알고 있다. 그렇다면 \\(\\tilde{F}(\\mu)=\\mathfrak{I}\\left[\\tilde{f}(t) \\right]\\) 를 구하면,\n\\[\n\\begin{aligned}\n\\tilde{F}(\\mu) & = F(\\mu)\\ast S(\\mu) = \\int_{-\\infty}^{\\infty} F(\\tau)S(\\mu-\\tau)\\, d\\tau \\\\\n&= \\dfrac{1}{T} \\sum_{n=-\\infty}^{\\infty}\\int_{-\\infty} F(\\tau) \\delta \\left(\\mu-\\tau-\\dfrac{n}{T}\\right)\\, d\\tau \\\\\n&= \\dfrac{1}{T}\\sum_{n=-\\infty}^{\\infty} F\\left( \\mu - \\dfrac{n}{T} \\right)\n\\end{aligned}\n\\]\n이다. 즉, 주기 \\(T\\) 로 샘플링된 연속 함수 \\(f(t)\\)의 함수의 푸리에 변환은 원래 함수의 푸리에 변환 \\(F(\\mu)\\) 를 주기 \\(1/T\\) 를 갖는 임펄스 함수로 바꾼 형태임을 알 수 있다.\n\n\n\n4.3 밴드-한계 신호 (band-limited signal), Over-sampling, under-sampling, critical-sampling, aliasing\n\n위의 그림을 보자. 원본연속함수(연속신호) \\(f(t)\\) 를 푸리에 변환한 함수 중, 그래프 (a) 와 같이 일부 구간 \\([-\\mu_M,\\,\\mu_M]\\)에 대한 변환 신호 \\(F(\\mu)\\) 만을 알고 있는 상황이라고 가정하자. 이렇계 특정 구간을 제외하고 \\(F(\\mu)=0\\) 일 때 이를 밴드한계신호라 한다.\n\\(f (t)\\) 를 주기 \\(T\\) 를 조절해 가며 샘플링한 함수를 푸리에 변환한다고 하자. 만약, 주기를 \\(1/T&gt;2\\mu_M\\) 이 되도록 샘플링하면 \\(F(\\mu)\\) 가 주기적으로 배열되며 그간격은 \\(1/T\\) 가 이되겠지만, \\(1/T&gt;2\\mu_M\\) 이므로, 결과는 그림 (b) 와 같이 띄엄띄엄 배열된 삼각형 신호가 될 것이다. 즉 \\(\\tilde{F}(\\mu)\\) 가 불연속 함수가 된다. 그러나 원래 \\(F(\\mu)\\) 가 가지고 있던 함수정보가 고스란히 남아 있으므로, 결과물로 부터 다시 역변환을 취하면 원래 함수 (신호값) 을 복원 할 수 있다. 이러한 결과를 오버-샘플링 (over-sampling) 이라 한다.\n만약, 주기 조건이 \\(1/T &lt; 2\\mu_M\\) 이라면 이번에도 \\(F(\\mu)\\) 가 주기적으로 배열되겠지만, \\(1/T &lt; 2\\mu_M\\) 이므로, 결과는 그래프 (d) 처럼 삼각형 신호 중 일부가 파묻힌 결과가 될 것이다. 비록 \\(F(\\mu)\\) 는 연속 함수가 되었지만, 원래의 정보를 잃어 버리게 되며, 이러한 결과를 언더-샘플링 (under-sampling) 이라 한다.\n만약 주기가 \\(1/T= 2\\mu_M\\) 로서 딱 맞다면, 그림 (c) 처럼, \\(F(\\mu)\\) 는 삼각형 신호가 제대로 연속 배열된 결과가 된다. 이러한 결과를 critical-sampling 이라 한다.\n따라서 원래 함수의 신호를 복원하기 위한 샘플링 충분 조건은 \\(1/T\\ge 2\\mu_M\\) 이 될 것이다. 이것은 소위 나이퀴스트-샤논 샘플링 정리의 결과이다.\n이 정리에 따르면, 어떤 신호든 밴드-한계 함수가 그 함수의 샘플들로부터 복원되기 위해서는, 샘플링 주기의 역수\\(1/T\\) 가 샘플링된 밴드의 최대값의 두 배보다 커야 한다. 만약, 주기의 역수가 이 조건을 만족시키지 못한다면 (즉, 언더-샘플링된 상황), 복원된 신호는 완벽하지 않을 것이며, 이러한 상황을 주파수 앨리어싱 (frequency aliasing) 혹은 그냥 앨리어싱 이라고 한다.\n앨리어싱은 주로 원래 이미지의 해상도보다 낮은 해상도로 이미지를 샘플링하거나, 이미지 획득 장치의 샘플링 간격이 피사체 자체의 고유 구조 특징보다 더 클 때 발생합니다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "푸리에 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_02.html#이산-푸리에-변환-discrete-fourier-transformation-dft",
    "href": "src/image_processing/image_processing_02.html#이산-푸리에-변환-discrete-fourier-transformation-dft",
    "title": "푸리에 변환을 이용한 이미지 처리",
    "section": "5 이산 푸리에 변환 (Discrete Fourier Transformation, DFT)",
    "text": "5 이산 푸리에 변환 (Discrete Fourier Transformation, DFT)\n\n5.1 1차원 DFT\n1차원 공간상의 함수 \\(f\\) 에 대해 \\(k\\) 번째 에서의 값을 \\(f_k\\) 라 하자. 그리고 이 수열을 \\(\\langle f\\rangle\\) 라 하자. 즉, \\[\n\\langle f \\rangle =(f_1,\\ldots,\\,f_{N})\n\\]\n이다. 우리는 여기서 한가지 가정을 하게 되는데 \\(f\\) 이 주기 \\(N\\) 을 갖는다는 것이다. 즉 \\(f_{k+N}=f_k\\) 이다. 예를 들어 \\((1, 3, 2, 4)\\) 에 대한 이산 푸리에 변환은\n\\[\n\\ldots , \\underline{1, 3, 2 , 4}, 1, 3,2,4, \\underline{1, 3, 2,4},\\ldots\n\\]\n에 대한 푸리에 변환이라는 의미이다 (주기적으로 반복되는 것을 잘 보이게 하기 위해 밑줄로 표현하였다). \\(N\\) 개의 데이터가 \\(N\\) 의 주기로 반복되기 때문에 전체적으로 주기는 \\(0, 1,\\ldots,\\, N-1\\) 이며, 이 때 \\(\\mathfrak{F}[f]=\\langle F\\rangle = (F_1,\\ldots,\\,F_{N})\\) 의 \\(F_u\\) 는 다음과 같이 구할 수 있다.\n\\[\nF_u = \\mathfrak{F}[f]_u=\\dfrac{1}{N} \\sum_{k=1}^{N} f_k \\exp\\left(-\\dfrac{i2\\pi (u-1) }{N} (k-1) \\right).\n\\]\n위의 식, 그리고 아래에서 에서 \\((u-1)\\) 이라던가 \\((n-1)\\) 같은 약간 어색한 항이 있는 이유는 우리가 앞으로 사용할 이산 푸리에 변환 라이브러리가 C/C++ 언어 기반이며, 따라서 인덱스가 0 부터 시작하기 때문이다.\n또한 \\(f_k\\) 는 다음과 같이 표현 할 수 있다.\n\\[\nf_k = \\mathfrak{F}^{-1}[F]_k = \\sum_{u=1}^{N} F_u \\exp \\left(\\dfrac{i2\\pi (k-1)}{N} (u-1) \\right)\n\\]\n\n이제 푸리에 변환과 역변환을 통해 \\(f_k\\) 를 복원 할 수 있음을 보이자. 증명을 좀 더 깔끔하게 하기 위해 \\(\\sum_{1}^N\\) 을 \\(\\sum_{0}^{N-1}\\) 로 바꾸었다.\n\\[\n\\begin{aligned}\nf_{k'} = \\mathfrak{F}^{-1}[F]_{k'} &=  \\sum_{u=0}^{N-1} F_u \\exp \\left(\\dfrac{i2\\pi k'u}{N}  \\right) \\\\\n&= \\dfrac{1}{N}\\sum_{u=0}^{N-1} \\left[\\sum_{k=0}^{N-1} f_k \\exp \\left(\\dfrac{-i2\\pi k u}{N}  \\right)  \\right]\\exp \\left(\\dfrac{i2\\pi k'u}{N} \\right)  \\\\\n&= \\dfrac{1}{N}\\sum_{k=0}^{N-1} f_k \\sum_{u=0}^{N-1} \\exp\\left( \\dfrac{2\\pi i (k'-k)u}{N}  \\right) \\\\\n% &= \\dfrac{1}{N}\\sum_{k=0}^{N-1} f_k \\dfrac{{1-\\exp \\left( \\dfrac{2\\pi i (k'-k) N}{N}\\right)}}{1-\\exp \\left( \\dfrac{2\\pi i (k'-k)}{N}\\right)}\n\\end{aligned}\n\\]\n우리는 여기서 \\(k'=k\\) 이면 \\(\\sum_{u=0}^{N-1} \\exp (\\cdots )= N\\) 임을 안다. \\(k'\\ne k\\) 라면 \\(|k'-k|\\le N-1\\) 이므로\n\\[\n\\sum_{u=0}^{N-1} \\exp\\left( \\dfrac{2\\pi i (k'-k)u}{N}  \\right) = \\dfrac{{1-\\exp \\left( \\dfrac{2\\pi i (k'-k) N}{N}\\right)}}{1-\\exp \\left( \\dfrac{2\\pi i (k'-k)}{N}\\right)} = 0\n\\]\n이며, 이로부터 \\(\\mathfrak{F}^{-1}[F]_{k} = f_k\\) 을 얻었다. 즉 DFT 로도 푸리에 변환과 역변환이 똑같이 성립한다.\n\n우리는 위의 식으로 부터 다음을 알 수 있다.\n\n푸리에 변환으로 얻는 \\(F_u = \\mathfrak{F}[f_k]\\) 의 변수 \\(u\\) 는 각진동수(angular frequency) 이다. 따라서 이 신호를 진동수(frequency) 에 대한 신호로 바꾸고 싶다면 변수를 \\(2\\pi u\\) 로 변경시켜야 한다.\n입력되는 신호 \\(f\\) 가 실수 신호더라도 푸리에 변환 신호는 복소수가 된다. 각진동수에 대한 분포를 알고싶다면 \\(|F_u|\\) 를 보아야 한다. Julia 에서는 abs 함수가 복소수에 대한 절대값을 반환한다.\n\n다음은 1차원 DFT 와 IDFT 를 julia 로 구현한 것이다. 1차원 벡터 x 에 대해 dft(x) 나 idft(x) 와 같이 사용한다. 하지만 아래의 코드는 앞으로 사용하지 않는다. 그 이유는 곧 밝힌다.\nfunction _dft(\n    f::Vector{T}, \n    inverse = false) where T&lt;:Union{Real, Complex}\n    \n    N = length(f)\n    if T &lt;:AbstractFloat\n        F = zeros(Complex{T}, N)\n    elseif T&lt;:Complex \n        F = zeros(eltype(f), N)\n    else \n        F = zeros(Complex{Float64}, N)\n    end\n\n    if inverse\n        for i in 1:N\n            F[i] = [f[k] * exp(2.0im * π* (i-1) * (k-1)/N) for k ∈ 1:N]./N |&gt; sum\n        end\n    else     \n        for i in 1:N\n            F[i] = [f[k] * exp(-2.0im * π* (i-1) * (k-1)/N) for k ∈ 1:N] |&gt; sum\n        end\n    end\n\n    return F\nend\n\ndft = v-&gt; _dft(v, false)\nidft = v-&gt; _dft(v, true)\n\n참고로 정수 \\(k,\\, N\\) 에 대해 \\(1 = e^{-i2\\pi } = e^{-i 2 \\pi N(k-1)/N}\\) 이므로\n\\[\n\\begin{aligned}\nF_{-u} &= \\dfrac{1}{N} \\sum_{k=1}^N f_k \\exp \\left( \\dfrac{i 2 \\pi (u +1)(k-1) }{N}\\right) \\\\\n&= \\dfrac{1}{N} \\sum_{k=1}^N f_k \\exp \\left( \\dfrac{i 2 \\pi (u +1)(k-1) }{N}\\right) \\exp \\left( \\dfrac{-i 2 \\pi N (k-1)}{N} \\right) \\\\\n&= \\dfrac{1}{N} \\sum_{k=1}^Nf_k \\exp \\left( \\dfrac{i 2 \\pi (N-u-1)(k-1)}{N}\\right) \\\\\n&= F_{N-u}\n\\end{aligned}\n\\]\n이다. 이때문에 \\(u\\) 의 범위를 \\(1\\le u \\le N\\) 로 표현하는 것보다 \\(N\\) 이 홀수일 때는 \\(-\\dfrac{N-1}{2} \\le u \\le \\dfrac{N-1}{2}\\) 로, \\(N\\) 이 짝수일 때는 \\(-\\dfrac{N}{2} \\le u \\le \\dfrac{N}{2}-1\\) 로 관례적으로 표현한다.\n\n\n5.2 2차원 DFT\n2차원 DFT 의 경우 \\(F(u, v)\\) 는 아래와 같이 구할 수 있다.\n\\[\n\\begin{aligned}\nF_{uv} &= \\dfrac{1}{MN} \\sum_{j=1}^{M}\\sum_{k=1}^{N} f_{jk} \\exp \\left[-2i\\pi \\left(\\dfrac{(j-1)(u-1)}{M}+\\dfrac{(k-1)(v-1)}{N}\\right)\\right] \\\\\nf_{jk} &=  \\sum_{j=1}^{M}\\sum_{k=1}^{N} F_{uv} \\exp \\left[2i\\pi \\left(\\dfrac{(j-1)(u-1)}{M}+\\dfrac{(k-1)(v-1)}{N}\\right)\\right]\n\\end{aligned}\n\\]\n여기서도 \\(\\mathfrak{F}^{-1}\\left[\\mathfrak{F}[f]\\right] = f\\) 와 \\(\\mathfrak{F}\\left[\\mathfrak{F}^{-1}[F]\\right]=F\\) 가 성립한다.\n\n\n\n5.3 연속 푸리에 변환과의 관계에서 보는 DFT 의 성질.\n\n1. 주기성 : 연속 푸리에 변환은 앞서 보았듯이 아래와 같다.\n\\[\n\\begin{aligned}\nF(u,\\,v) &= \\mathfrak{I} \\left[f(x,\\,y)\\right]=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x,\\,y)e^{-2i\\pi (ux+vy)}\\, dxdy\\,, \\\\\nf (x,\\,y) &= \\mathfrak{I}^{-1} \\left[F(u,\\,v)\\right] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} F(\\mu) e^{2i \\pi (ux+vy)} \\, dudv\n\\end{aligned}\n\\]\n불연속 푸리에 변환쌍 에서는 \\(x, y\\) 대신 \\(x/N, y/M\\)가,그리고 \\(u,v\\) 대신 \\(u/N, v/M\\) 가 쓰였다. 이는 원본 함수가 정해진 구간 \\(x = 0,\\,1,\\ldots,\\,N-1\\) 혹은 \\(y=1,\\ldots,\\,M-1\\) 을 벗어나면 다시 주기 \\(N, M\\) 을 가지고 모든 1차원 공간에서 반복된다는 가정이 내포되어 있다. 즉, 정수 \\(p,\\,q\\) 에 대해\n\\[\n\\begin{aligned}\nf(x+pN,\\, y+qN) &= f(x,\\, y) \\\\\nF(u+pN,\\, v+qN) &= F(u,\\, v)\n\\end{aligned}\n\\] 가 성립한다는 뜻이다.\n\n2. 정규화 인자 (normalization factor) : 또한, \\(F(u,\\,v)\\) 의 계수에 연속 푸리에 변환에는 없던 \\(\\dfrac{1}{NM}\\) 가 생긴것을 볼 수 있다.우리가 다루는 이미지 같은 신호는 공간상에서 제약이 있고 (즉, 가로와 세로의 길이가 정해져 있고), 또한 신호들은 불연속적으로 나열되어 있으므로, 이를 푸리에 변환 과정에서 보상하기 위해, \\(F (u,\\,v)\\) 의 계수에 \\(\\dfrac{1}{MN}\\) 가 정규화 인자 (normalization factor)로서 생긴 것이다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "푸리에 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_02.html#고속-푸리에-변환-fft",
    "href": "src/image_processing/image_processing_02.html#고속-푸리에-변환-fft",
    "title": "푸리에 변환을 이용한 이미지 처리",
    "section": "6 고속 푸리에 변환 (FFT)",
    "text": "6 고속 푸리에 변환 (FFT)\n고속 푸리에 변환(Fast Fourier Transform, FFT)은 DFT 과 그 역변환을 빠르게 수행하는 효율적인 알고리즘이다. 위의 이산 푸리에 변환은 \\(N^2\\) 번의 곱셉과 \\(N(N-1)\\) 번의 덧셈 연산, 즉 대략 \\(2N^2\\) 번의 연산이 필요하지만 FFT를 이용하면 대략 \\(\\mathcal{O}(N \\log_2 N)\\) 의 연산만으로 가능하다. 기본적인 아이디어는 가우스가 생각했었으나 잊혀졌고 이후 Cooley 와 Tukey 가 1965년도에 고속 푸리에 변환 알고리즘을 발표하였으며 이후 전 세계적으로 광범위하게 사용되었으며 다양한 다른 방법들도 개발되었다. 여기서는 Cooley 와 Turkey 의 기본적인 알고리즘을 알아보기로 하자.\n\n\n6.1 기본적인 아이디어\n우선 \\(N=2^n\\) 일 경우에 대해 생각하자. 그리고 우리는 \\(f_k\\) 의 인덱스 \\(k\\) 를 \\(1\\le k \\le N\\) 에서 생각했지만 여기서는 \\(0\\le k \\le N-1\\) 로 생각하자. 이 경우,\n\\[\n\\mathfrak{F}[f]_u=\\dfrac{1}{N}\\sum_{k=0}^{N-1} f_k e^{-2 \\pi i ku /N},\\qquad u=0,\\ldots,N-1\n\\] 이다. \\(k=0,\\ldots,\\,N-1\\) 에 대해 \\((f_k)\\) 의 짝수 인덱스만 모아 수열 \\((a_k)\\) 를 만들고 홀수 인덱스만 모아 수열 \\((b_k)\\) 를 만든다.\n\\[\n\\begin{aligned}\na_k &= f_{2k}, \\\\\nb_k &= f_{2k+1}\n\\end{aligned}\n\\]\n이렇게 하면\n\\[\n(f_k) = (a_0,\\,b_0,\\,a_1,\\,b_1,\\ldots,\\, a_{N/2-2},\\, b_{N/2-2},\\, a_{N/2-1},\\, b_{N/2-1})\n\\]\n이다. \\(w_N := e^{-2\\pi i /N},\\, M=N/2=2^{n-1}\\) 이라고 하면 \\({w_N}^2 = w_M = e^{-2\\pi i /M} = e^{-2\\pi i /(N/2)}\\) 이며, 이를 이용하여 \\(\\mathfrak{F}_u\\) 를 계산하면 아래와 같다.\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f]_u &= \\sum_{k=0}^{N-1} f_k e^{-2\\pi i uk/N} =  \\sum_{k=0}^{N-1}f_k {w_N}^{uk} \\\\\n&= \\sum_{k=0}^{M-1}a_k {w_N}^{2uk} + \\sum_{k=0}^{M-1} b_k {w_N}^{(2k+1)u}\\\\\n&= \\sum_{k=0}^{M-1} a_k (w_M)^{uk} + \\left({w_N}^u\\right) \\sum_{k=0}^{M-1} b_k (w_M)^{uk} \\\\\n\\end{aligned}\n\\tag{2}\\]\n이다. \\(M-1\\) 개의 점에 대한 \\((a_k)\\) 와 \\((b_k)\\) 의 푸리에 변환 \\(\\mathfrak{F}[a]_u,\\, \\mathfrak{F}[b]_u\\) 를 생각하자. \\(0 \\le u \\le M-1\\) 일 경우\n\\[\n\\mathfrak{F}[f]_{u} = \\mathfrak{F}[a]_u + ({w_N}^u) \\mathfrak{F}[b]_u\n\\tag{3}\\]\n이며, \\(M\\le u \\le N-1\\) 일 경우 \\(v=u-M\\) 으로 놓고 \\({w_N}^{-M} = (e^{-2\\pi i /N})^{-M} = e^{i\\pi} = -1\\) 과 \\({w_M}^M=1\\) 을 이용하면,\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f]_{M \\le u \\le N-1} &= \\sum_{k=0}^{M-1} a_k (w_M)^{uk} + ({w_N}^u) \\sum_{k=0}^{M-1} b_k (w_M)^{uk} \\\\\n&= \\sum_{k=0}^{M-1}a_k (w_M)^{vk} (w_M)^{Mk} + (w_N)^{v}(w_N)^M \\sum_{k=0}^{M-1}b_k (w_M)(w_M)^{vk} (w_M)^{Mk} \\\\\n&= \\sum_{k=0}^{M-1} a_k (w_M)^{vk} - ({w_N}^v) \\sum_{k=0}^{M-1} b_k (w_M)^{vk} \\\\\n&= \\mathfrak{F}[a]_{u-M} - ({w_N}^{u-M}) \\mathfrak{F}[b]_{u-M}\n\\end{aligned}\n\\tag{4}\\]\n이다. 즉 \\(1\\le u\\le M\\) 에 대해\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f]_u &= \\mathfrak{F}[a]_u + ({w_N}^u) \\mathfrak{F}[b]_u, \\\\[0.3em]\n\\mathfrak{F}[f]_{u+M} &= \\mathfrak{F}[a]_u - ({w_N}^u) \\mathfrak{F}[b]_u,\n\\end{aligned}\n\\tag{5}\\]\n이다. 우리는 \\(N=2^n\\) 개의 점에 대한 DFT 가 \\(M=N/2\\) 인 두 푸리에 변환의 선형결합과 같다는 것을 알게 되었다. 만약 \\(N\\) 개의 점에 대한 DFT 의 연산 횟수를 \\(X(N)\\) 이라고 하자. \\(N=2^n\\) 일 때 \\(N/2=2^{n-1}\\) 개의 점에 대한 두번의 DFT 와 \\((w_N^m)\\) 과 \\(\\mathfrak{F}[b]_u\\) 와의 \\(N/2\\) 번의 곱셈 연산이 필요하므로\n\\[\n\\begin{aligned}\nX(N=2^n) &= 2X(2^{n-1}) + 2^{n-1} \\\\\n&= 2(2X(2^{n-2}) + 2^{n-2}) + 2^{n-1}\\\\\n&\\qquad \\vdots \\\\\n&= 2^{n-1} X(2^1) + (n-1)2^{n-1}\n\\end{aligned}\n\\]\n\\(N=2\\) 일 때 \\(X(2^1)=X(2)=4\\) 이므로\n\\[\nX(2^n) = 2^{n+1} + (n-1)2^{n-1} \\approx n 2^{n} = 2^n \\log_2 (2^n) = N \\log_2(N)\n\\]\n이다. 즉 \\(N=2^n\\) 개의 점에 대해 대략 \\(N \\log_2 (N)\\) 번의 연산을 통해 DFT 를 수행 할 수 있다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "푸리에 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_02.html#차원-불연속-푸리에-변환-2d-dft을-이용한-이미지-처리",
    "href": "src/image_processing/image_processing_02.html#차원-불연속-푸리에-변환-2d-dft을-이용한-이미지-처리",
    "title": "푸리에 변환을 이용한 이미지 처리",
    "section": "7 2차원 불연속 푸리에 변환 (2D DFT)을 이용한 이미지 처리",
    "text": "7 2차원 불연속 푸리에 변환 (2D DFT)을 이용한 이미지 처리\nJulia 는 복소수 타입 배열이 존재하지만 OpenCV 의 Mat 은 복소수 타입을 사용 할 수 없으며, OpenCV 는 내부적으로 3차원 행렬의 첫번째 차원을 실수로, 두번째 차원을 허수로 하여 복소수 행렬을 다룬다. OpenCV 에서는 2 채널 배열이라고 한다. 기본적으로 푸리에 변환은 부동소수 타입이 되어야 하기 때문에 푸리에 변환을 하기 전에 정수 타입의 이미지 행렬을 실수 타입으로 바꾸어 주어야 한다. 아래의 cvConvertTo(img0, Float32) 는 OpenCV.Mat{UInt8} 타입의 배열을 OpenCV.Mat{Float32} 타입으로 바꾸어주는 함수로 직접 코딩하였다.\nfunction cvConvertTo(mat::OpenCV.Mat, t::T) where T&lt;:Type\n    return cv.Mat(convert.(t, mat.data))\nend\n\nimg0= img2mat(testimage(\"cameraman.tif\"));\nimg1 = cvConvertTo(img0, Float32)\nft1 = cv.dft(img1, flags=cv.DFT_COMPLEX_OUTPUT);\ncv.dft 즉 OpenCV.dft 가 2차원 이산 푸리에 변환을 수행하는 함수이며 flag=cv.DFT_COMPLEX_OUTPUT 은 푸리에 변환의 결과를 복소수를 표현하는 2채널 행렬로 반환하라는 의미이다.\n물론 FFTW.jl 과 같은 julia 의 FFT 패키지를 사용 할 수도 있지만 이 경우 OpenCV.Mat 을 juila 배열로 변환하고 푸리에 변환 이후에 다시 OpenCV.Mat 으로 변환시켜야 하기 때문에 여기서는 다소 번거롭더라도 OpenCV.dft 를 사용하였다.\n\n\n7.1 Ideal Low Pass Filtering\n2D DFT 행렬을 DC 인자를 중앙에 배치하여 재배열했을 경우, 원본 이미지에서 낮은 픽셀 빈도를 갖는 성분은 중앙 (즉, \\((u,v) \\approx (0,0)\\)) 근처 에 모이게 된다. 낮은 빈도를 갖는다는 것은 공간 상에서 신호 주기가 크다는 의미고, 역공간에서는 ’1/주기’가 작은 값으로 대응이 될 것이므로, \\((u, v)\\) 가 중앙 부분으로 몰리게 되는 것이다. 따라서, 원본 이미지의 로우-패스 필터링을 하고 싶다면, 푸리에 변환 된 행렬의 중앙 부분만 따로 골라내어 다시 역변환하면 될 것이다.\n가장 간단히 생각 할 수 있는 방법은 원점 주위의 특정 반경 안에 들어오는 2D DFT 이미지만 뽑아 역변환 해주는 것이며 이를 ideal low pass filtering 이라 한다.\n\n\n\n7.2 Ideal High Pass Filtering\nIdeal low pass filtering 과 반대로 생각해 주면 된다.\n\n\n\n7.3 버터워스 필터링 (Butterworth filtering)\nIdeal low/high pass filter 는 계단 함수 형태로, 그 과정에서 물결무늬 같은 원하지 않는 패턴이 생성된 것도 확인할 수 있었다. 이러한 부작용을 방지하기 위해 컷오프 거리를 기준으로 로우-패스 혹은 하이-패스 영역을 설정하는 방식을 계단 함수가 아닌, 조금 더 부드러운 연속 함수로 바꿀 수 있다. 그중 하나가 버터위스 필터링으로 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\n\\text{Low pass filter } h(u,\\,v) &= \\dfrac{1}{1+\\left( 1+\\dfrac{(u^2+d^2)^{1/2}}{D} \\right)^{2n}} \\\\\n\\text{High pass filter } h(u,\\,v) &= \\dfrac{1}{1+\\left(1+\\dfrac{D}{(u^2+d^2)^{1/2}}\\right)^{2n}}\n\\end{aligned}\n\\]\n\n\n\n7.4 Gaussian Filtering\n\n\n\n7.5 위상 보존 필터 (zero-phase-shift filter), 위상 비보존 필터 (Nonzero-phase-shift filter)\n우리는 지금까지 원본 이미지 \\(f(x,\\,y)\\)의 역공간 \\(F(u,\\,v)\\) 에서 작업해 왔다. \\(F(u,\\,v)\\) 는 복소함수로 실수부 \\(R(u,\\,v)\\) 와 허수부 \\(I(u,\\,v)\\) 가 존재하여 \\(F(u,\\,v) = R(u,\\,v) +iI(u,\\,v)\\) 이다. 이 때 \\(\\arctan \\left( \\dfrac{I(u,\\,v)}{R(u,\\,v)}\\right)\\) 를 \\(F(u,\\,v)\\) 의 위상이라고 한다.\n만약 어떤 필터에 의해 위상이 변하지 않는다면 이를 위상 보존 필터라 하고, 그렇지 않다면 위상 비보존 필터 라 한다.\n\n\n\n7.6 밴드삭제 필터링, 놋치 필터링.\n\n\n\n7.7 Periodic noise reduction - Bragg filtering\n\n\n\n7.8 Deblurring using inverse filtering\n잘 안됨… 더 알아 봐야 할 듯..",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "푸리에 변환을 이용한 이미지 처리"
    ]
  },
  {
    "objectID": "src/image_processing/fourier_transform_for_image_processing.html",
    "href": "src/image_processing/fourier_transform_for_image_processing.html",
    "title": "푸리에 변환",
    "section": "",
    "text": "어떤 함수가 주기 \\(T\\) 를 갖는다는 것은 모든 \\(t\\in \\mathbb{R}\\) 에 대해 \\(f(t+T)=f(T)\\) 임을 의미한다. 이 경우 \\(f(t)\\) 는 다음과 같이 표현 할 수 있다. 주기 \\(T\\) 에 대해 \\(\\nu = 1/T\\) 를 진동수(frequency) 라고 하고 \\(\\omega = 2\\pi/T = 2\\nu\\) 를 각진동수 (angular frequency) 라고 한다.\n\n\n\n\n\n\n\n정의 1 (실함수의 푸리에 급수) 주기 \\(T\\) 를 갖는 실함수 \\(f: X \\subset \\mathbb{R} \\to \\mathbb{R}\\) 는 다음과 같은 푸리에 급수로 표현 할 수 있다. 여기서 \\(\\omega = 2\\pi/T\\) 이다.\n\\[\n\\begin{aligned}\nf(t) &= \\dfrac{a_0}{2} + \\sum_{n=1}^{\\infty} a_n \\cos (n\\omega t) + \\sum_{n=1}^{\\infty} b_n \\sin (n \\omega t)\\,,\\\\\n\\text{where}\\qquad  a_n &= \\dfrac{2}{T}\\int_{-T/2}^{T/2} f(t) \\cos (n\\omega t)\\, dt\\\\\nb_n &= \\dfrac{2}{T}\\int_{-T/2}^{T/2} f(t) \\sin (n\\omega t)\\, dt.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n정의 2 (푸리에 급수) 주기 \\(T\\) 를 갖는 함수 \\(f(t)\\) 는 다음과 같은 푸리에 급수로 표현 할 수 있다. 여기서 \\(\\omega = 2\\pi/T\\) 이다.\n\\[\n\\begin{aligned}\nf(t) &= \\sum_{n=-\\infty}^{\\infty} c_n e^{in\\omega t}\\,,\\\\\n\\text{where}\\qquad c_n &= \\dfrac{1}{T}\\int_{-T/2}^{T/2} f(t) e^{-in\\omega t}\\, dt\\,, \\qquad n\\in \\mathbb{Z}\n\\end{aligned}\n\\]\n\n\n\n\n\n정의 1 는 실함수의 경우, 정의 2 는 실함수를 포함한 복소함수의 경우에 사용 할 수 있다. 아래 그림은 두가지 함수에 대한 푸리에 급수를 \\(n=1,\\ldots,\\,4\\) 까지 표한 것이다.\n\n\n\n푸리에 급수\n\n\n\n\\([-T/2,\\, T/2]\\) 구간에서 연속인 함수의 집합 \\(X\\) 를 생각하자. 이 집합은 벡터공간이다. 여기에 내적을 다음과 같이 정의한다.\n\\[\n\\langle \\phi,\\, \\psi \\rangle = \\int_{-T/2}^{T/2} \\psi^\\ast(t)\\, \\phi(t)\\, dt\n\\]\n수학적으로 푸리에 급수가 의미 있기 위해서는 \\(\\omega = 2\\pi/T\\) 에 대해 \\(\\{e^{in\\omega t} : n\\in \\mathbb{Z}\\}\\) 가 \\([T/2,\\,T/2]\\) 구간에서 연속인 모든 복소 함수의 집합에 대해 정규직교기저이어야 한다. 직교성은 보이기 쉽지만 completeness 즉, 주기 \\(T\\) 인 모든 함수가 \\(\\{\\sin (n\\omega t),\\, \\cos (n \\omega t) : n \\in \\mathbb{Z}\\}\\) 의 선형결합으로 표현되는 것은 여기의 범위를 벗어난다. 단지 직교성과 completeness 를 알고 넘어가자.\n\n예제 1 (기저함수의 직교성) 임의의 정수 \\(m,\\,n\\) 에 대해 다음이 성립함을 보여라.\n\\[\n\\int_{-T/2}^{T/2} \\exp \\left( \\dfrac{i 2\\pi m}{T}t\\right) \\exp \\left(-\\dfrac{i 2\\pi n}{T}t \\right)\\, dt = \\delta_{nm}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n정의 3 (1차원 푸리에 변환) 함수 \\(f(t)\\) 에 대한 푸리에 변환은 \\(\\mathfrak{F}[f(t)]\\) 로 쓰고 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\nF(u) &:= \\mathfrak{F}\\left[ f(t)\\right] = \\int_{-\\infty}^{\\infty} f(t) \\, e^{-2i\\pi u t} \\, dt,\\\\\nf(t) &= \\mathfrak{F}^{-1}[F(\\mu)] = \\int_{-\\infty}^{\\infty} F(u)\\, e^{2i \\pi u t} \\, d\\mu\\,.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n정의 4 (2차원 푸리에 변환) 함수 \\(f(x, y)\\) 에 대한 푸리에 변환은 \\(\\mathfrak{F}[f(x, y)]\\) 로 쓰고 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\nF(u,\\,v) &= \\mathfrak{F} \\left[f(x,\\,y)\\right]=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x,\\,y)e^{-2i\\pi (ux+vy)}\\, dxdy\\,, \\\\\nf (x,\\,y) &= \\mathfrak{F}^{-1} \\left[F(u,\\,v)\\right] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} F(u, v) e^{2i \\pi (ux+vy)} \\, dudv\n\\end{aligned}\n\\]\n\n\n\n\n위의 경우 \\(F(u)\\) 와 \\(F(u,\\,v)\\) 는 복소수이므로 다음과 같이 표현 할 수 있다.\n\\[\nF(u,\\,v) = |F(u,\\,v)| e^{i\\Phi(u,\\,v)}\n\\]\n이 때 \\(|F(u,\\,v)|\\) 를 푸리에 스펙트럼 (Fourier spectrum) 이라 하고, \\(\\Phi(u,\\,v)\\) 를 위상 (phase) 이라 한다. 또한 \\(P(u,\\,v) =|F(u,\\,v)|^2\\) 를 푸리에 파워 스펙트럼 (Fourier power spectrum) 이라 한다.\n\n\n\n\n여기서 \\(F(u) = \\mathfrak{F}[f(t)]\\), \\(F(u, v) = \\mathfrak{F}[f(x, y)]\\) 이며, 2차원 푸리에 변환에 대해서 증명한다. 1차원에 대해서는 그 결과를 쉽게 유추 할 수 있다.\n\n명제 1 (선형성 (linearity)) 상수 \\(a,\\,b\\) 에 대해 다음이 성립한다. \\[\n\\mathfrak{F}[af(x, y) + bg(x, y)] = a\\mathfrak{F}[f(x, y)] + b\\mathfrak{F}[g(x, y)].\n\\]\n\n\n(증명). \\[\n\\begin{aligned}\n\\mathfrak{F}[af(x, y) + bg(x, y)]&= \\iint (af(x, y) + bg(x, y)) e^{-2i\\pi (ux+vy)}\\, dxdy \\\\\n&= \\iint af(x, y)  e^{-2i\\pi (ux+vy)}\\, dxdy + \\iint  bg(x, y) e^{-2i\\pi (ux+vy)}\\, dxdy  \\\\\n&=  a\\mathfrak{F}[f(x, y)] + b\\mathfrak{F}[g(x, y)] \\qquad \\square\n\\end{aligned}\n\\]\n\n\n명제 2 (Similarity) \\[\n\\mathfrak{F}[f(ax,\\,by)] =\\dfrac{1}{ab} F\\left(\\dfrac{u}{a},\\, \\dfrac{v}{b}\\right)\n\\]\n\n\n(증명). \\[\n\\begin{aligned}\n\\mathfrak{F}[f(ax,\\,by)] &= \\iint f(ax,\\, by) e^{-2i\\pi (ux+vy)}\\,dxdy \\qquad &; t=ax,\\, s=by\\\\\n&= \\dfrac{1}{ab} \\iint f(s,\\,t) e^{-2i \\pi (ut/a+vs/b)}\\, dtds \\\\\n&= \\dfrac{1}{ab} F \\left(\\dfrac{u}{a},\\, \\dfrac{v}{b} \\right)\n\\end{aligned}\n\\]\n\n\n\n명제 3 (Shift property) \\[\n\\mathfrak{F}[f(x-a,\\, y-b)] = \\exp (i2\\pi (au+bv)) F(u,\\,v)\n\\]\n\n\n(증명). \\[\n\\begin{aligned}\n\\mathfrak{F}[f(x-a,\\, y-b) ]&= \\iint f(x-a,\\, y-b) e^{-2i\\pi (ux+vy)} \\,dxdy \\qquad &; t=x-a,\\, s=y-b\\\\\n&=\\iint f(t,\\,s) e^{-2i\\pi (ux+vy)} e^{2i\\pi (au+bv)} \\, dxdy \\\\\n&= e^{2i\\pi (au+bv)}F(u,\\,v)\n\\end{aligned}\n\\]\n\n\n\n명제 4 (도함수의 푸리에 변환) \\[\n\\mathfrak{F}\\left[ \\dfrac{d^k}{dx^k}f(x) \\right] =( 2i\\pi u)^k F(u)\n\\]\n\n\n(증명). \\(k=1\\) 인 경우만 보이면 나머지는 쉽게 일반화 된다.\\(f(x)=\\displaystyle \\int_{-\\infty}^{\\infty} F(u)e^{2i\\pi ux}\\, du\\) 이므로\n\\[\nf'(x) =  \\int_{-\\infty}^{\\infty}(2i\\pi u) F(u)e^{2i \\pi ux} \\, du\n\\]\n이다. 따라서\n\\[\n\\mathfrak{F}[f'(x)] = (2i\\pi u) F(u)\n\\] 이다.\n\n\n\n예제 2 \\(\\text{rect}\\left(\\dfrac{x}{w}\\right)\\) 함수의 푸리에 변환은 다음과 같다.\n\\[\n\\begin{aligned}\nF(u) = \\mathfrak{F}\\left[\\text{rect}\\left(\\dfrac{x}{w}\\right)\\right] &= \\int_{-w/2}^{w/2} e^{-2\\pi i u t}\\, dt = \\dfrac{\\sin \\pi w u}{\\pi u}\n\\end{aligned}\n\\]\n이것을 그래프로 그리면 아래 그림과 같다.\n\n\n\n\n\n\n그림 1: rect 함수의 푸리에 변환\n\n\n\n\\(\\text{rect}\\left(\\dfrac{x}{w}\\right)\\) 함수의 폭은 \\(w\\) 이다. 그리고 \\(F(u) =0\\) 을 만족하는 주기는 \\(u=0\\) 일 때를 제외하면 \\(1/w\\) 이다. 즉 신호의 폭과, 푸리에 변환된 신호의 폭은 대략적으로 반비례 관계에 있다는 것을 알 수 있다.\n\n\n\n\n\n샘플링 함수 \\(S_T(t)\\) 를 생각하자. 이는 주기함수이므로 푸리에 급수로 표현 할 수 있으며 다음과 같다.\n\\[\n\\begin{aligned}\nS_T(t) &= \\sum_{k=-\\infty}^{\\infty} \\delta \\left(t-kT \\right) = \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\exp \\left( \\dfrac{-2\\pi i k t}{T}\\right)\n\\end{aligned}\n\\]\n단일 임펄스 함수 \\(\\delta(u-u_0)\\) 에 대한 푸리에 변환은 다음과 같다.\n\\[\n\\mathfrak{F}[\\delta (u-u_0)] = \\int_{-\\infty}^{\\infty} \\delta (u-u_0) e^{2\\pi i u t}\\, du = e^{2\\pi i u_0 t}\n\\]\n따라서,\n\\[\n\\mathfrak{F}^{-1} \\left[ \\delta \\left( u-\\dfrac{k}{T} \\right) \\right] = \\exp \\left( \\dfrac{2ik\\pi t}{t} \\right) \\iff \\mathfrak{F}\\left[  \\exp \\left( \\dfrac{2ik\\pi t}{t} \\right) \\right] =  \\delta \\left(u-\\dfrac{k}{T} \\right)\n\\]\n이다. 이를 이용하면 \\(s_T(t)\\) 의 푸리에 변환식 \\(S(u)=\\mathfrak{F}[s_N(t)]\\) 를 계산 할 수 있으며 다음과 같다.\n\\[\nS (u) = \\mathfrak{F}\\left[\\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\exp \\left( \\dfrac{-2\\pi i kt}{T}\\right)\\right] = \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\mathfrak{F}\\left[ \\exp \\left( \\dfrac{-2\\pi i k t}{T}\\right) \\right] = \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\delta \\left(u - \\dfrac{k}{T}\\right)\n\\]\n\n\n\n\n두 함수 \\(f(t),\\, g(t)\\) 의 convolution \\(f(t) \\ast g(t)\\) 은 다음과 같이 정의된다.\n\\[\nf(t) \\ast g(t) \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) \\, d\\tau\n\\]\n\\(F(\\mu)= \\mathfrak{F}[f(t)],\\, G(\\mu)=\\mathfrak{F}[g(t)]\\) 라 할 때, \\(f(t)\\otimes g(t)\\) 의 푸리에 변환을 구하면,\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f(t) \\ast g(t)] &= \\int_{-\\infty}^{\\infty} \\left[ \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau) d\\tau\\right] e^{-2in\\pi \\mu t}\\, dt \\\\\n&= \\int_{-\\infty}^{\\infty}f(\\tau) \\left[  \\int_{-\\infty}^{\\infty} g(t-\\tau)e^{-2in \\pi \\mu t}\\, dt\\right] \\,d\\tau \\\\\n&= \\int_{-\\infty}^{\\infty} f(\\tau) G(\\mu)e^{-2in \\pi \\mu \\tau} \\, d\\tau \\\\\n&= F(\\mu)G(\\mu)\n\\end{aligned}\n\\]\n즉 합성곱의 푸리에 변환은 푸리에 변환의 곱이다. 이를 응용하면 두 함수의 곱의 푸리에변환은 합성곱이 됨을 보일 수 있다. 즉,\n\\[\n\\mathfrak{F}[f(t)g(t)]=F(\\mu)\\ast G(\\mu)\n\\]\n가 된다.\n일반적으로 컴퓨터에서 계산할 때 크기가 크게 다르지 않은 두 함수의 합성곱을 직접 구하는 것보다 푸리에 변환을 시킨 후 이것을 곱하고 역 푸리에 변환을 하는 것이 계산 속도가 더 빠르다. 이것은 이산 푸리에 변환의 빠른 속도 때문인데, 많은 경우 convolution의 구현은 푸리에 변환과 역푸리에 변환을 이용하여 구현된다.\n\n\n\n\n임의의 연속함수 \\(f(t)\\) 를 \\(T\\) 를 주기로 샘플링 하였다고 하자. 그렇다면 이 샘플링 \\(\\tilde{f}(t)\\) 은 다음과 같은 식으로 표현 할 수 있다.\n\\[\n\\tilde{f}(t) = \\sum_{k=-\\infty}^{\\infty} f(t) \\, \\delta (t-kT)\n\\]\n이 때 \\(\\mathfrak{F}[f(t)]= F(\\mu),\\, \\mathfrak{F}[s_T(t)]= S(\\mu)\\) 라 하자 \\(\\displaystyle S(\\mu)=\\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\delta \\left( \\mu - \\dfrac{k}{T}\\right)\\) 임은 알고 있다. 그렇다면 \\(\\tilde{F}(\\mu)=\\mathfrak{F}\\left[\\tilde{f}(t) \\right]\\) 를 구하면,\n\\[\n\\begin{aligned}\n\\tilde{F}(\\mu) & = F(\\mu)\\otimes S(\\mu) = \\int_{-\\infty}^{\\infty} F(\\tau)S(\\mu-\\tau)\\, d\\tau \\\\\n&= \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty}\\int_{-\\infty} F(\\tau) \\delta \\left(\\mu-\\tau-\\dfrac{k}{T}\\right)\\, d\\tau \\\\\n&= \\dfrac{1}{T}\\sum_{k=-\\infty}^{\\infty} F\\left( \\mu - \\dfrac{k}{T} \\right)\n\\end{aligned}\n\\]\n이다. 즉, 주기 \\(T\\) 로 샘플링된 연속 함수 \\(f(t)\\)의 함수의 푸리에 변환은 원래 함수의 푸리에 변환 \\(F(\\mu)\\) 를 주기 \\(1/T\\) 를 갖는 임펄스 함수로 바꾼 형태임을 알 수 있다."
  },
  {
    "objectID": "src/image_processing/fourier_transform_for_image_processing.html#푸리에-급수와-푸리에-변환",
    "href": "src/image_processing/fourier_transform_for_image_processing.html#푸리에-급수와-푸리에-변환",
    "title": "푸리에 변환",
    "section": "",
    "text": "어떤 함수가 주기 \\(T\\) 를 갖는다는 것은 모든 \\(t\\in \\mathbb{R}\\) 에 대해 \\(f(t+T)=f(T)\\) 임을 의미한다. 이 경우 \\(f(t)\\) 는 다음과 같이 표현 할 수 있다. 주기 \\(T\\) 에 대해 \\(\\nu = 1/T\\) 를 진동수(frequency) 라고 하고 \\(\\omega = 2\\pi/T = 2\\nu\\) 를 각진동수 (angular frequency) 라고 한다.\n\n\n\n\n\n\n\n정의 1 (실함수의 푸리에 급수) 주기 \\(T\\) 를 갖는 실함수 \\(f: X \\subset \\mathbb{R} \\to \\mathbb{R}\\) 는 다음과 같은 푸리에 급수로 표현 할 수 있다. 여기서 \\(\\omega = 2\\pi/T\\) 이다.\n\\[\n\\begin{aligned}\nf(t) &= \\dfrac{a_0}{2} + \\sum_{n=1}^{\\infty} a_n \\cos (n\\omega t) + \\sum_{n=1}^{\\infty} b_n \\sin (n \\omega t)\\,,\\\\\n\\text{where}\\qquad  a_n &= \\dfrac{2}{T}\\int_{-T/2}^{T/2} f(t) \\cos (n\\omega t)\\, dt\\\\\nb_n &= \\dfrac{2}{T}\\int_{-T/2}^{T/2} f(t) \\sin (n\\omega t)\\, dt.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n정의 2 (푸리에 급수) 주기 \\(T\\) 를 갖는 함수 \\(f(t)\\) 는 다음과 같은 푸리에 급수로 표현 할 수 있다. 여기서 \\(\\omega = 2\\pi/T\\) 이다.\n\\[\n\\begin{aligned}\nf(t) &= \\sum_{n=-\\infty}^{\\infty} c_n e^{in\\omega t}\\,,\\\\\n\\text{where}\\qquad c_n &= \\dfrac{1}{T}\\int_{-T/2}^{T/2} f(t) e^{-in\\omega t}\\, dt\\,, \\qquad n\\in \\mathbb{Z}\n\\end{aligned}\n\\]\n\n\n\n\n\n정의 1 는 실함수의 경우, 정의 2 는 실함수를 포함한 복소함수의 경우에 사용 할 수 있다. 아래 그림은 두가지 함수에 대한 푸리에 급수를 \\(n=1,\\ldots,\\,4\\) 까지 표한 것이다.\n\n\n\n푸리에 급수\n\n\n\n\\([-T/2,\\, T/2]\\) 구간에서 연속인 함수의 집합 \\(X\\) 를 생각하자. 이 집합은 벡터공간이다. 여기에 내적을 다음과 같이 정의한다.\n\\[\n\\langle \\phi,\\, \\psi \\rangle = \\int_{-T/2}^{T/2} \\psi^\\ast(t)\\, \\phi(t)\\, dt\n\\]\n수학적으로 푸리에 급수가 의미 있기 위해서는 \\(\\omega = 2\\pi/T\\) 에 대해 \\(\\{e^{in\\omega t} : n\\in \\mathbb{Z}\\}\\) 가 \\([T/2,\\,T/2]\\) 구간에서 연속인 모든 복소 함수의 집합에 대해 정규직교기저이어야 한다. 직교성은 보이기 쉽지만 completeness 즉, 주기 \\(T\\) 인 모든 함수가 \\(\\{\\sin (n\\omega t),\\, \\cos (n \\omega t) : n \\in \\mathbb{Z}\\}\\) 의 선형결합으로 표현되는 것은 여기의 범위를 벗어난다. 단지 직교성과 completeness 를 알고 넘어가자.\n\n예제 1 (기저함수의 직교성) 임의의 정수 \\(m,\\,n\\) 에 대해 다음이 성립함을 보여라.\n\\[\n\\int_{-T/2}^{T/2} \\exp \\left( \\dfrac{i 2\\pi m}{T}t\\right) \\exp \\left(-\\dfrac{i 2\\pi n}{T}t \\right)\\, dt = \\delta_{nm}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n정의 3 (1차원 푸리에 변환) 함수 \\(f(t)\\) 에 대한 푸리에 변환은 \\(\\mathfrak{F}[f(t)]\\) 로 쓰고 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\nF(u) &:= \\mathfrak{F}\\left[ f(t)\\right] = \\int_{-\\infty}^{\\infty} f(t) \\, e^{-2i\\pi u t} \\, dt,\\\\\nf(t) &= \\mathfrak{F}^{-1}[F(\\mu)] = \\int_{-\\infty}^{\\infty} F(u)\\, e^{2i \\pi u t} \\, d\\mu\\,.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n정의 4 (2차원 푸리에 변환) 함수 \\(f(x, y)\\) 에 대한 푸리에 변환은 \\(\\mathfrak{F}[f(x, y)]\\) 로 쓰고 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\nF(u,\\,v) &= \\mathfrak{F} \\left[f(x,\\,y)\\right]=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x,\\,y)e^{-2i\\pi (ux+vy)}\\, dxdy\\,, \\\\\nf (x,\\,y) &= \\mathfrak{F}^{-1} \\left[F(u,\\,v)\\right] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} F(u, v) e^{2i \\pi (ux+vy)} \\, dudv\n\\end{aligned}\n\\]\n\n\n\n\n위의 경우 \\(F(u)\\) 와 \\(F(u,\\,v)\\) 는 복소수이므로 다음과 같이 표현 할 수 있다.\n\\[\nF(u,\\,v) = |F(u,\\,v)| e^{i\\Phi(u,\\,v)}\n\\]\n이 때 \\(|F(u,\\,v)|\\) 를 푸리에 스펙트럼 (Fourier spectrum) 이라 하고, \\(\\Phi(u,\\,v)\\) 를 위상 (phase) 이라 한다. 또한 \\(P(u,\\,v) =|F(u,\\,v)|^2\\) 를 푸리에 파워 스펙트럼 (Fourier power spectrum) 이라 한다.\n\n\n\n\n여기서 \\(F(u) = \\mathfrak{F}[f(t)]\\), \\(F(u, v) = \\mathfrak{F}[f(x, y)]\\) 이며, 2차원 푸리에 변환에 대해서 증명한다. 1차원에 대해서는 그 결과를 쉽게 유추 할 수 있다.\n\n명제 1 (선형성 (linearity)) 상수 \\(a,\\,b\\) 에 대해 다음이 성립한다. \\[\n\\mathfrak{F}[af(x, y) + bg(x, y)] = a\\mathfrak{F}[f(x, y)] + b\\mathfrak{F}[g(x, y)].\n\\]\n\n\n(증명). \\[\n\\begin{aligned}\n\\mathfrak{F}[af(x, y) + bg(x, y)]&= \\iint (af(x, y) + bg(x, y)) e^{-2i\\pi (ux+vy)}\\, dxdy \\\\\n&= \\iint af(x, y)  e^{-2i\\pi (ux+vy)}\\, dxdy + \\iint  bg(x, y) e^{-2i\\pi (ux+vy)}\\, dxdy  \\\\\n&=  a\\mathfrak{F}[f(x, y)] + b\\mathfrak{F}[g(x, y)] \\qquad \\square\n\\end{aligned}\n\\]\n\n\n명제 2 (Similarity) \\[\n\\mathfrak{F}[f(ax,\\,by)] =\\dfrac{1}{ab} F\\left(\\dfrac{u}{a},\\, \\dfrac{v}{b}\\right)\n\\]\n\n\n(증명). \\[\n\\begin{aligned}\n\\mathfrak{F}[f(ax,\\,by)] &= \\iint f(ax,\\, by) e^{-2i\\pi (ux+vy)}\\,dxdy \\qquad &; t=ax,\\, s=by\\\\\n&= \\dfrac{1}{ab} \\iint f(s,\\,t) e^{-2i \\pi (ut/a+vs/b)}\\, dtds \\\\\n&= \\dfrac{1}{ab} F \\left(\\dfrac{u}{a},\\, \\dfrac{v}{b} \\right)\n\\end{aligned}\n\\]\n\n\n\n명제 3 (Shift property) \\[\n\\mathfrak{F}[f(x-a,\\, y-b)] = \\exp (i2\\pi (au+bv)) F(u,\\,v)\n\\]\n\n\n(증명). \\[\n\\begin{aligned}\n\\mathfrak{F}[f(x-a,\\, y-b) ]&= \\iint f(x-a,\\, y-b) e^{-2i\\pi (ux+vy)} \\,dxdy \\qquad &; t=x-a,\\, s=y-b\\\\\n&=\\iint f(t,\\,s) e^{-2i\\pi (ux+vy)} e^{2i\\pi (au+bv)} \\, dxdy \\\\\n&= e^{2i\\pi (au+bv)}F(u,\\,v)\n\\end{aligned}\n\\]\n\n\n\n명제 4 (도함수의 푸리에 변환) \\[\n\\mathfrak{F}\\left[ \\dfrac{d^k}{dx^k}f(x) \\right] =( 2i\\pi u)^k F(u)\n\\]\n\n\n(증명). \\(k=1\\) 인 경우만 보이면 나머지는 쉽게 일반화 된다.\\(f(x)=\\displaystyle \\int_{-\\infty}^{\\infty} F(u)e^{2i\\pi ux}\\, du\\) 이므로\n\\[\nf'(x) =  \\int_{-\\infty}^{\\infty}(2i\\pi u) F(u)e^{2i \\pi ux} \\, du\n\\]\n이다. 따라서\n\\[\n\\mathfrak{F}[f'(x)] = (2i\\pi u) F(u)\n\\] 이다.\n\n\n\n예제 2 \\(\\text{rect}\\left(\\dfrac{x}{w}\\right)\\) 함수의 푸리에 변환은 다음과 같다.\n\\[\n\\begin{aligned}\nF(u) = \\mathfrak{F}\\left[\\text{rect}\\left(\\dfrac{x}{w}\\right)\\right] &= \\int_{-w/2}^{w/2} e^{-2\\pi i u t}\\, dt = \\dfrac{\\sin \\pi w u}{\\pi u}\n\\end{aligned}\n\\]\n이것을 그래프로 그리면 아래 그림과 같다.\n\n\n\n\n\n\n그림 1: rect 함수의 푸리에 변환\n\n\n\n\\(\\text{rect}\\left(\\dfrac{x}{w}\\right)\\) 함수의 폭은 \\(w\\) 이다. 그리고 \\(F(u) =0\\) 을 만족하는 주기는 \\(u=0\\) 일 때를 제외하면 \\(1/w\\) 이다. 즉 신호의 폭과, 푸리에 변환된 신호의 폭은 대략적으로 반비례 관계에 있다는 것을 알 수 있다.\n\n\n\n\n\n샘플링 함수 \\(S_T(t)\\) 를 생각하자. 이는 주기함수이므로 푸리에 급수로 표현 할 수 있으며 다음과 같다.\n\\[\n\\begin{aligned}\nS_T(t) &= \\sum_{k=-\\infty}^{\\infty} \\delta \\left(t-kT \\right) = \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\exp \\left( \\dfrac{-2\\pi i k t}{T}\\right)\n\\end{aligned}\n\\]\n단일 임펄스 함수 \\(\\delta(u-u_0)\\) 에 대한 푸리에 변환은 다음과 같다.\n\\[\n\\mathfrak{F}[\\delta (u-u_0)] = \\int_{-\\infty}^{\\infty} \\delta (u-u_0) e^{2\\pi i u t}\\, du = e^{2\\pi i u_0 t}\n\\]\n따라서,\n\\[\n\\mathfrak{F}^{-1} \\left[ \\delta \\left( u-\\dfrac{k}{T} \\right) \\right] = \\exp \\left( \\dfrac{2ik\\pi t}{t} \\right) \\iff \\mathfrak{F}\\left[  \\exp \\left( \\dfrac{2ik\\pi t}{t} \\right) \\right] =  \\delta \\left(u-\\dfrac{k}{T} \\right)\n\\]\n이다. 이를 이용하면 \\(s_T(t)\\) 의 푸리에 변환식 \\(S(u)=\\mathfrak{F}[s_N(t)]\\) 를 계산 할 수 있으며 다음과 같다.\n\\[\nS (u) = \\mathfrak{F}\\left[\\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\exp \\left( \\dfrac{-2\\pi i kt}{T}\\right)\\right] = \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\mathfrak{F}\\left[ \\exp \\left( \\dfrac{-2\\pi i k t}{T}\\right) \\right] = \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\delta \\left(u - \\dfrac{k}{T}\\right)\n\\]\n\n\n\n\n두 함수 \\(f(t),\\, g(t)\\) 의 convolution \\(f(t) \\ast g(t)\\) 은 다음과 같이 정의된다.\n\\[\nf(t) \\ast g(t) \\equiv \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) \\, d\\tau\n\\]\n\\(F(\\mu)= \\mathfrak{F}[f(t)],\\, G(\\mu)=\\mathfrak{F}[g(t)]\\) 라 할 때, \\(f(t)\\otimes g(t)\\) 의 푸리에 변환을 구하면,\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f(t) \\ast g(t)] &= \\int_{-\\infty}^{\\infty} \\left[ \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau) d\\tau\\right] e^{-2in\\pi \\mu t}\\, dt \\\\\n&= \\int_{-\\infty}^{\\infty}f(\\tau) \\left[  \\int_{-\\infty}^{\\infty} g(t-\\tau)e^{-2in \\pi \\mu t}\\, dt\\right] \\,d\\tau \\\\\n&= \\int_{-\\infty}^{\\infty} f(\\tau) G(\\mu)e^{-2in \\pi \\mu \\tau} \\, d\\tau \\\\\n&= F(\\mu)G(\\mu)\n\\end{aligned}\n\\]\n즉 합성곱의 푸리에 변환은 푸리에 변환의 곱이다. 이를 응용하면 두 함수의 곱의 푸리에변환은 합성곱이 됨을 보일 수 있다. 즉,\n\\[\n\\mathfrak{F}[f(t)g(t)]=F(\\mu)\\ast G(\\mu)\n\\]\n가 된다.\n일반적으로 컴퓨터에서 계산할 때 크기가 크게 다르지 않은 두 함수의 합성곱을 직접 구하는 것보다 푸리에 변환을 시킨 후 이것을 곱하고 역 푸리에 변환을 하는 것이 계산 속도가 더 빠르다. 이것은 이산 푸리에 변환의 빠른 속도 때문인데, 많은 경우 convolution의 구현은 푸리에 변환과 역푸리에 변환을 이용하여 구현된다.\n\n\n\n\n임의의 연속함수 \\(f(t)\\) 를 \\(T\\) 를 주기로 샘플링 하였다고 하자. 그렇다면 이 샘플링 \\(\\tilde{f}(t)\\) 은 다음과 같은 식으로 표현 할 수 있다.\n\\[\n\\tilde{f}(t) = \\sum_{k=-\\infty}^{\\infty} f(t) \\, \\delta (t-kT)\n\\]\n이 때 \\(\\mathfrak{F}[f(t)]= F(\\mu),\\, \\mathfrak{F}[s_T(t)]= S(\\mu)\\) 라 하자 \\(\\displaystyle S(\\mu)=\\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\delta \\left( \\mu - \\dfrac{k}{T}\\right)\\) 임은 알고 있다. 그렇다면 \\(\\tilde{F}(\\mu)=\\mathfrak{F}\\left[\\tilde{f}(t) \\right]\\) 를 구하면,\n\\[\n\\begin{aligned}\n\\tilde{F}(\\mu) & = F(\\mu)\\otimes S(\\mu) = \\int_{-\\infty}^{\\infty} F(\\tau)S(\\mu-\\tau)\\, d\\tau \\\\\n&= \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty}\\int_{-\\infty} F(\\tau) \\delta \\left(\\mu-\\tau-\\dfrac{k}{T}\\right)\\, d\\tau \\\\\n&= \\dfrac{1}{T}\\sum_{k=-\\infty}^{\\infty} F\\left( \\mu - \\dfrac{k}{T} \\right)\n\\end{aligned}\n\\]\n이다. 즉, 주기 \\(T\\) 로 샘플링된 연속 함수 \\(f(t)\\)의 함수의 푸리에 변환은 원래 함수의 푸리에 변환 \\(F(\\mu)\\) 를 주기 \\(1/T\\) 를 갖는 임펄스 함수로 바꾼 형태임을 알 수 있다."
  },
  {
    "objectID": "src/image_processing/fourier_transform_for_image_processing.html#이산-푸리에-변환-discrete-fourier-transformation-dft",
    "href": "src/image_processing/fourier_transform_for_image_processing.html#이산-푸리에-변환-discrete-fourier-transformation-dft",
    "title": "푸리에 변환",
    "section": "2 이산 푸리에 변환 (Discrete Fourier Transformation, DFT)",
    "text": "2 이산 푸리에 변환 (Discrete Fourier Transformation, DFT)\n\n1차원 DFT\n1차원 공간상의 함수 \\(f\\) 에 대해 \\(k\\) 번째 에서의 값을 \\(f_k\\) 라 하자. 그리고 이 수열을 \\(\\langle f\\rangle\\) 라 하자. 즉, \\[\n\\langle f \\rangle =(f_1,\\ldots,\\,f_{N})\n\\]\n이다.\n우리는 여기서 한가지 가정을 하게 되는데 \\(f\\) 이 주기 \\(N\\) 을 갖는다는 것이다. 즉 \\(f_{k+N}=f_k\\) 이다. 예를 들어 \\((1, 3, 2, 4)\\) 에 대한 이산 푸리에 변환은\n\\[\n\\ldots , \\underline{1, 3, 2 , 4}, 1, 3,2,4, \\underline{1, 3, 2,4},\\ldots\n\\]\n에 대한 푸리에 변환이라는 의미이다 (주기적으로 반복되는 것을 잘 보이게 하기 위해 밑줄로 표현하였다). \\(N\\) 개의 데이터가 \\(N\\) 의 주기로 반복되기 때문에 전체적으로 주기는 \\(0, 1,\\ldots,\\, N-1\\) 이며\n이 때 \\(\\mathfrak{F}[f]=\\langle F\\rangle = (F_1,\\ldots,\\,F_{N})\\) 의 \\(F_u\\) 는 다음과 같이 구할 수 있다.\n\\[\nF_u = \\mathfrak{F}[f]_u=\\dfrac{1}{N} \\sum_{k=1}^{N} f_k \\exp\\left(-\\dfrac{i2\\pi (u-1) }{N} (k-1) \\right).\n\\]\n위의 식, 그리고 아래에서 에서 \\((u-1)\\) 이라던가 \\((n-1)\\) 같은 약간 어색한 항이 있는 이유는 우리가 앞으로 사용할 FFTW 가 C 언어 기반이며, 따라서 인덱스가 0 부터 시작하기 때문이다.\n또한 \\(f_k\\) 는 다음과 같이 표현 할 수 있다.\n\\[\nf_k = \\mathfrak{F}^{-1}[F]_k = \\sum_{u=1}^{N} F_u \\exp \\left(\\dfrac{i2\\pi (k-1)}{N} (u-1) \\right)\n\\]\n\n이제 푸리에 변환과 역변환을 통해 \\(f_k\\) 를 복원 할 수 있음을 보이자. 증명을 좀 더 깔끔하게 하기 위해 \\(\\sum_{1}^N\\) 을 \\(\\sum_{0}^{N-1}\\) 로 바꾸었다.\n\\[\n\\begin{aligned}\nf_{k'} = \\mathfrak{F}^{-1}[F]_{k'} &=  \\sum_{u=0}^{N-1} F_u \\exp \\left(\\dfrac{i2\\pi k'u}{N}  \\right) \\\\\n&= \\dfrac{1}{N}\\sum_{u=0}^{N-1} \\left[\\sum_{k=0}^{N-1} f_k \\exp \\left(\\dfrac{-i2\\pi k u}{N}  \\right)  \\right]\\exp \\left(\\dfrac{i2\\pi k'u}{N} \\right)  \\\\\n&= \\dfrac{1}{N}\\sum_{k=0}^{N-1} f_k \\sum_{u=0}^{N-1} \\exp\\left( \\dfrac{2\\pi i (k'-k)u}{N}  \\right) \\\\\n% &= \\dfrac{1}{N}\\sum_{k=0}^{N-1} f_k \\dfrac{{1-\\exp \\left( \\dfrac{2\\pi i (k'-k) N}{N}\\right)}}{1-\\exp \\left( \\dfrac{2\\pi i (k'-k)}{N}\\right)}\n\\end{aligned}\n\\]\n우리는 여기서 \\(k'=k\\) 이면 \\(\\sum_{u=0}^{N-1} \\exp (\\cdots )= N\\) 임을 안다. \\(k'\\ne k\\) 라면 \\(|k'-k|\\le N-1\\) 이므로\n\\[\n\\sum_{u=0}^{N-1} \\exp\\left( \\dfrac{2\\pi i (k'-k)u}{N}  \\right) = \\dfrac{{1-\\exp \\left( \\dfrac{2\\pi i (k'-k) N}{N}\\right)}}{1-\\exp \\left( \\dfrac{2\\pi i (k'-k)}{N}\\right)} = 0\n\\]\n이며, 이로부터 \\(\\mathfrak{F}^{-1}[F]_{k} = f_k\\) 을 얻었다. 즉 DFT 로도 푸리에 변환과 역변환이 똑같이 성립한다.\n\n우리는 위의 식으로 부터 다음을 알 수 있다.\n\n푸리에 변환으로 얻는 \\(F_u = \\mathfrak{F}[f_k]\\) 의 변수 \\(u\\) 는 각진동수(angular frequency) 이다. 따라서 이 신호를 진동수(frequency) 에 대한 신호로 바꾸고 싶다면 변수를 \\(2\\pi u\\) 로 변경시켜야 한다.\n입력되는 신호 \\(f\\) 가 실수 신호더라도 푸리에 변환 신호는 복소수가 된다. 각진동수에 대한 분포를 알고싶다면 \\(|F_u|\\) 를 보아야 한다. Julia 에서는 abs 함수가 복소수에 대한 절대값을 반환한다.\n\n다음은 1차원 DFT 와 IDFT 를 julia 로 구현한 것이다. 1차원 벡터 x 에 대해 dft(x) sk idft(x) 와 같이 사용한다. 하지만 아래의 코드는 앞으로 사용하지 않는다. 그 이유는 곧 밝힌다.\nfunction _dft(\n    f::Vector{T}, \n    inverse = false) where T&lt;:Union{Real, Complex}\n    \n    N = length(f)\n    if T &lt;:AbstractFloat\n        F = zeros(Complex{T}, N)\n    elseif T&lt;:Complex \n        F = zeros(eltype(f), N)\n    else \n        F = zeros(Complex{Float64}, N)\n    end\n\n    if inverse\n        for i in 1:N\n            F[i] = [f[k] * exp(2.0im * π* (i-1) * (k-1)/N) for k ∈ 1:N]./N |&gt; sum\n        end\n    else     \n        for i in 1:N\n            F[i] = [f[k] * exp(-2.0im * π* (i-1) * (k-1)/N) for k ∈ 1:N] |&gt; sum\n        end\n    end\n\n    return F\nend\n\ndft = v-&gt; _dft(v, false)\nidft = v-&gt; _dft(v, true)\n\n참고로 정수 \\(k,\\, N\\) 에 대해 \\(1 = e^{-i2\\pi } = e^{-i 2 \\pi N(k-1)/N}\\) 이므로\n\\[\n\\begin{aligned}\nF_{-u} &= \\dfrac{1}{N} \\sum_{k=1}^N f_k \\exp \\left( \\dfrac{i 2 \\pi (u +1)(k-1) }{N}\\right) \\\\\n&= \\dfrac{1}{N} \\sum_{k=1}^N f_k \\exp \\left( \\dfrac{i 2 \\pi (u +1)(k-1) }{N}\\right) \\exp \\left( \\dfrac{-i 2 \\pi N (k-1)}{N} \\right) \\\\\n&= \\dfrac{1}{N} \\sum_{k=1}^Nf_k \\exp \\left( \\dfrac{i 2 \\pi (N-u-1)(k-1)}{N}\\right) \\\\\n&= F_{N-u}\n\\end{aligned}\n\\]\n이다. 이때문에 \\(u\\) 의 범위를 \\(1\\le u \\le N\\) 로 표현하는 것보다 \\(N\\) 이 홀수일 때는 \\(-\\dfrac{N-1}{2} \\le u \\le \\dfrac{N-1}{2}\\) 로, \\(N\\) 이 짝수일 때는 \\(-\\dfrac{N}{2} \\le u \\le \\dfrac{N}{2}-1\\) 로 관례적으로 표현한다.\n\n\n2차원 DFT\n2차원 DFT 의 경우 \\(F(u, v)\\) 는 아래와 같이 구할 수 있다.\n\\[\n\\begin{aligned}\nF_{uv} &= \\dfrac{1}{MN} \\sum_{j=1}^{M}\\sum_{k=1}^{N} f_{jk} \\exp \\left[-2i\\pi \\left(\\dfrac{(j-1)(u-1)}{M}+\\dfrac{(k-1)(v-1)}{N}\\right)\\right] \\\\\nf_{jk} &=  \\sum_{j=1}^{M}\\sum_{k=1}^{N} F_{uv} \\exp \\left[2i\\pi \\left(\\dfrac{(j-1)(u-1)}{M}+\\dfrac{(k-1)(v-1)}{N}\\right)\\right]\n\\end{aligned}\n\\]\n여기서도 \\(\\mathfrak{F}^{-1}\\left[\\mathfrak{F}[f]\\right] = f\\) 와 \\(\\mathfrak{F}\\left[\\mathfrak{F}^{-1}[F]\\right]=F\\) 가 성립한다.\n\n\n\n고속 푸리에 변환과 FFTW.jl\n고속 푸리에 변환(Fast Fourier Transform, FFT)은 DFT 과 그 역변환을 빠르게 수행하는 효율적인 알고리즘이다. 위의 푸리에 변환은 \\(\\mathcal{O}(n^2)\\) 의 연산이 필요하지만, FFT를 이용하면 \\(\\mathcal{O}(n \\log n)\\) 의 연산만으로 가능하다. FFT 는 알고리즘이며 그것을 구현한 것 가운데 가장 널리 사용되는 것이 FFTW 이다. FFTW 는 Fastest Fourier Transform in the West 의 약자로 C 언어로 구현되어 있으며, Julia 에서 가장 많이 사용하는 FFT 라이브러리인 FFTW.jl 은 FFTW 의 Julia 바인딩 이다. 설치는 일반적인 라이브러리와 같이 using Pkg; Pkg.add(\"FFTW\") 명령어를 사용한다. 바인딩이란 간략히 말하면 어떤 언어로 구현된 라이브러리를 다른 언어에서 사용 할 수 있도록 해 주는 라이브러리이다.\n푸리에 변환을 하는 함수는 차원에 무관하게 fft 이며, 역푸리에 변환을 하는 함수는 ifft 이다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia 언어와 수치해석",
    "section": "",
    "text": "이 사이트는 수치해석과 이미지 처리를 다룹니다. 실제 구현되는 코드는 julia 언어로 작성되었습니다. Julia 언어에 대해서는 Introduction to Julia 를 참고하시기 바랍니다.\n이곳의 목적은 numpy 나 scipy, opencv 와 같이 이미 구현된 라이브러리를 사용하는 방법을 배우는 것이 아니라, 이 라이브러리의 함수나 기능들이 어떤 원리와 알고리즘으로 구현되는지를 알아보는 것입니다. 따라서 바닥부터 하나하나 쌓아 올리고자 합니다.\n수치해석에 많이 사용되는 언어는 Fortran, C/C++, Python, Matlab, Julia 가 있습니다. 여기서는 수치 해석을 julia 언어로 구현하려고 하는데 수치해석을 julia 언어로 배우는 장점은 다음과 같습니다.\n\nJulia 는 속도가 빠릅니다. 다른 무엇보다 for 나 while 루프가 빠릅니다. 가장 바람직한 경우에는 C/C++ 이나 fortran 에 비견할만한 속도가 나오며 Python 이나 Matlab 의 루프보다는 훨씬 빠릅니다. 멀티스레딩도 쉽게 사용 할 수 있습니다. 즉 여기서 구현된 코드가 어느 정도는 사용할 만큼의 속도가 나온다는 것입니다. 물론 많은 라이브러리들은 여러가지 최적화 기법을 사용하여 여기서 제시한 코드보다 훨씬 빠르게 동작합니다.\nC/C++ 보다 알고리즘의 표현력이 좋습니다. 보통 알고리즘은 언어 독립적인 유사 코드(pseudo code) 로 제시되는데 julia 의 코드와 이 유사 코드는 상당히 비슷하며 많은 경우 1-1 대응이 됩니다.\nJuia 는 Interpreter 언어입니다. 코드를 작성한 후 즉각적으로 테스트하고 확인할 수 있습니다. 물론 Python 이나 Matlab 으로도 수치 해석을 배울 수 있습니다. 그러나 numpy/scipy 나 cython, numba 등의 도움을 받지 않은 순수 파이썬 코드는 대부분의 경우 속도가 매우 느리며, numpy 등을 사용하는 경우는 수치해석을 배우는 것이 아니라 numpy 의 사용법을 배우게 됩니다. Matlab 의 경우도 유사합니다.\n\n물론 실전에서는 다릅니다. Julia 의 대표적인 이미지 처리 패키지인 Images.jl 과 OpenCV 를 비교해보면 비룍 OpenCV 는 C++ 언어로 작성되었지만 Python 에서 구동시키는 OpenCV 보다 Images.jl 이 좋다고 말하기는 매우 힘들것입니다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "latexmacros.html",
    "href": "latexmacros.html",
    "title": "수치해석과 이미지 처리",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]"
  },
  {
    "objectID": "src/image_processing/image_processing_01.html",
    "href": "src/image_processing/image_processing_01.html",
    "title": "이미지 프로세싱의 기초",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 프로세싱의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_01.html#소개",
    "href": "src/image_processing/image_processing_01.html#소개",
    "title": "이미지 프로세싱의 기초",
    "section": "1 소개",
    "text": "1 소개\n\n1.1 이미지\n\n우리의 시각에서 감지하는 바와 같은 2차원 이미지 일 수도 있지만, 측정에서 얻는 2차원 데이터 일 수 도 있다.\n여기서 다루는 이미지는 디지털 이미지이다. 정해진 크기의 수로 이루어진 2차원 배열을 이미지라고 통칭한다.\n2차원 배열을 \\(f[i,j]\\) 로 표기하며, 행렬과 같이 \\(i\\) 는 세로 방향의 인덱스, \\(j\\) 는 가로 방향의 인덱스이다.\n\n프로그래밍에서 2차원 배열의 인덱스는 행렬 표기의 관례를 따라 세로축-가로축 순서이지만, 수학에서 함수로서 표현할 때는 가로축-세로축 순서이다. 이것이 매우 혼동을 일으키지만 이 관례를 계속 사용하기로 한다. 즉\n\\[\n\\boxed{\n    I[i,\\,j]= I(j, i)\n}\n\\]\n이다.\n\n\n\n1.2 여기서\n\nJulia 에는 Images.jl 이라는 이미지 처리 라이브러리가 있지만,\n\n모든 데이터값을 \\([0,\\,1]\\) 사이의 고정 소수(fixed point number) 로 처리하며, 원래 이미지가 가지고 있던 0 부터 255 사이의 부호 없는 정수값을 숨긴다.\nopencv 에 비해 기능이 부족하고 무엇보다 느리다.\nOpenCV.jl 이라는 opencv 의 julia 포팅이 있다.\n\n그런데 OpenCV.jl 은\n\n기본 이미지 배열 타입은 OpenCV.Mat 이며 julia 의 Array 와 유사하지만 같지는 않다. Array 에서 사용하는 연산중 많은 것을 사용 할 수 없다. 파이썬의 경우는 opencv2 의 배열은 numpy 의 배열인데…\n흑백 이미지라도 OpenCV.Mat 은 3차원 배열이다.\nOpenCV.Mat 에 대한 연산 (배열간, 배열과 스칼라 사이의 사칙연산을 포함하여) 을 모두 다시 쓰느니 OpenCV.Mat 과 julia Array 사이의 변환 함수를 사용하겠다.\nOpenCV.Mat 의 배열의 저장 순서는 C++ 이나 파이썬 과 같이 행 우선 방식이지만 Julia 는 열 우선 방식이다. 따라서 변환시 이를 고려해야 한다.\n\nJulia 의 TestImages.jl 은 다양한 무료 이미지를 다운로드 받을 수 있도록 해 준다. 여기서의 이미지 처리에 사용하는 이미지는 특별한 언급이 없는 한 여기로부터 얻는다.\n\n\nOpenCV.jl 을 julia 에서 사용하기 위해 다음과 같은 함수를 사용한다. 앞으로의 모든 코드는\nusing OpenCV, TestImages\ncv = OpenCV;\n\n# Julia Matrix to OpenCV.MAT 변환\nfunction arr2mat(arr::Matrix{T}) where T&lt;:Real\n    cv.Mat(permutedims(stack([arr, ]), [3,2,1]))\nend\n\n# Julia Image to Matrix 변환\nfunction img2arr(img)\n    T = typeof(img[1, 1].val.i)\n    broadcast(q-&gt;T(q.val.i),img)\nend\n\n# Julia Image to OpenCV.Mat 변환\nfunction img2mat(img) \n    T = typeof(img[1, 1].val.i)\n    tm = broadcast(q-&gt;T(q.val.i),img)\n    cv.Mat(permutedims(stack([tm, ]), [3,2,1]))\nend\n\n# OpenCV.Mat to Julia Matrix 변환\nfunction mat2arr(mat::OpenCV.Mat)\n    return permutedims(mat.data, [3,2,1])\nend",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 프로세싱의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_01.html#점-기준-이미지-가공",
    "href": "src/image_processing/image_processing_01.html#점-기준-이미지-가공",
    "title": "이미지 프로세싱의 기초",
    "section": "2 점 기준 이미지 가공",
    "text": "2 점 기준 이미지 가공\n\n각 픽셀 단위의 이미지 처리를 의미한다. 즉 픽셀에 대한 연신이 다른 픽셀의 정보와 독립적으로 이루어진다.\nGray scale image \\(I[i, j]\\) 를 생각하자. \\(0\\le I[i,\\,j] \\le 255\\) 이다.\n\n\n\n2.1 픽셀 반전법에 의한 이미지 가공\n상수 \\(a\\) 와 \\(b&gt;0\\) 에 대해 \\[\nT_i[I][i, j] = a-b I[i, j],\\qquad 0 \\le T_i[I] \\le 255\n\\]\n인 이미지 연산을 픽셀 반전법 이라고 한다. \\(a=255,\\, b=1\\) 일 경우 완전한 흑백 반전이다. TestImages.jl 로부터 테스트 이미지를 다운받아 처리하였다.\nimg0= testimage_dip3e(\"Fig0108(a) (corn-fluorescence).tif\")\nimg1 = img2arr(img0)\nimg2 = (UInt8(255) .- img1)\nr = arr2mat(cat(img1, img2;dims=2))\n\n\n\n\n\n\n그림 1: 원본 이미지(좌) 와 반전된 이미지\n\n\n\n\n\n\n2.2 \\(\\gamma\\)-correction (or \\(\\gamma\\)-encoding)\n\n\\(\\gamma&gt;0\\) 에 대해 다음과 같이 변환한다. \\[\nT_\\gamma [I][i, j]  = I[i, j]^\\gamma\n\\]\n\\(\\gamma\\) 값이 \\(1\\) 보다 상당히 크면 픽셀 값이 클수록, \\(\\gamma\\) 값이 \\(1\\) 보다 상당히 작으면 픽셀값이 작을수록 대조가 현저해진다.\n\\(0\\le I[i,j]\\le 255\\) 일 때 \\(\\gamma&lt;1\\) 이면 \\(I^\\gamma[i, j] &lt; 255\\) 이며 \\(\\gamma&gt;1\\) 이면 \\(I^\\gamma[i,j]&gt;255\\) 일 수 있으므로 최대값이 255가 넘지 않도록 해 준다.\n원본 이미지가 1024x1024 로 크기 때문에 그 크기를 줄여주었다.\n\nimg0= testimage_dip3e(\"Fig0227(a)(washington_infrared).tif\")\nimg1 = cv.resize(img2mat(img0), cv.Size{Int32}(256, 256))\nimg2 = arr2mat(round.(UInt8, ((img1./255).^0.5)*255))\nimg3 = arr2mat(round.(UInt8, ((img1./255).^2)*255))\nimg4 = arr2mat(round.(UInt8, ((img1./255).^5)*255));\narr2mat(cat(img1, img2, img3, img4; dims=2))\n\n\n\n\n\n\n그림 2: 맨 왼쪽부터 \\(\\gamma=1\\), \\(\\gamma=0.5\\), \\(\\gamma=2\\), \\(\\gamma=5\\)\n\n\n\n\n\\(x\\in (0,\\,1)\\) 에 대해 \\(\\gamma&lt;1\\) 이면 \\(x^\\gamma &gt; x\\) 이므로 화소 값이 높은 쪽으로 몰린다. 반대로 \\(\\gamma&gt;1\\) 이면 \\(x^\\gamma &lt; x\\) 이므로 화소 값이 낮은 쪽으로 몰린다.\n\n\n\n\n2.3 히스토그램 균등화\n이미지의 각 픽셀은 0 에서 255 사이의 정수값을 가진다. 그 값의 빈도는 이미지의 성질을 파악하는데 중요하다. 예를 들어 그림 2 의 \\(\\gamma\\) 에 대한 히스토그램은 다음과 같다.\n\n# opencv 의 calcHist 함수를 julia 에서 쓰기 편하게 변환함.\nfunction histogram1d(mat::OpenCV.Mat{T}) where T&lt;:Union{UInt8, UInt16}\n    tm = Int32(typemax(T))\n    v = cv.calcHist(cv.InputArray[mat,], Int32[0], fill(UInt8(1), size(img1)), Int32[tm+1], Float32[0, tm+1])\n    return (0:1:tm, Int64.(v[1,1,:]))    \nend\n\nfig = Figure()\nax = Axis(fig[1,1])\nfor (img, g) in zip([img1, img2, img3, img4], [1.0, 0.5, 2, 5])\n    b, v = histogram1d(img)\n    lines!(ax, b, v, label = L\"\\gamma = %$g\")\nend\naxislegend()\nfig\n\n\n\n\n\n\n그림 3: 그림 2 의 \\(\\gamma\\) 값에 따른 히스토그램\n\n\n\n\n위의 그림에서 \\(\\gamma=5\\) 일 때의 히스토그램은 낮은 값으로 몰려 있다. 혹은 이미지 중에는 전체 256 의 채널 갑 중에 어떤 값을 중심으로 몰려 있을 수 있다. 이런 경우 컨트라스트를 조절 하기 위해 앞서의 \\(\\gamma\\)-correction 방법으로는 개선이 크게 되지 않는다. 이 때 사용하는 방법이 히스토그램 균등화이다. 한 채널을 중심으로 몰려 있는 히스토그램을 균등화 한다.\n원래의 히스토그램을 변수 \\(r\\) 에 대해 \\(h(r)\\) 이며 \\(r\\) 은 \\(0\\) 부터 \\(L-1\\) (여기서는 255) 까지 가질 수 있고 \\(h(r)\\) 확률 별수 \\(r\\) 에 대한 확률 밀도에 비례하는 값이라고 가정하자. \\(p_r(r)\\) 을 확률밀도라고 하면\n\\[\np_r(r) = \\dfrac{h(r)}{\\int_0^L h(r')\\,dr'}\n\\tag{1}\\]\n이며 이 때 새로운 변수 \\(s\\) 를 다음과 같이 정의한다.\n\\[\ns=(L-1)\\int_{0}^r p_r(r')\\,dr'.\n\\tag{2}\\]\n그렇다면\n\\[\n\\dfrac{ds}{dr} = (L-1)p_r(r)\n\\tag{3}\\]\n이며 새로운 변수 \\(s\\) 로 변환된 \\(p_r(r)\\) 은\n\\[\np_s(s) = p_r(r)\\left|\\dfrac{ds}{dr}\\right| = \\dfrac{1}{L-1}\n\\tag{4}\\]\n이다. 즉 새로운 변수 \\(s\\) 에 대해 \\(p_s(s)\\) 는 항상 같은 값을 갖게 된다.\n즉 어떤 픽셀의 강도가 \\(r\\) 이라면 새로운 강도는 \\(s\\) 가 된다. \\(r=0,\\,1,\\ldots,\\,L-1\\) 의 값을 가지므로 이에 대한 \\(s\\) 값을\n\\[\ns(r) = \\text{round}\\left[(L-1)\\sum_{i=1}^r \\dfrac{h(i)}{\\sum_{j=1}^{L-1} h(j)}\\right]\n\\tag{5}\\]\n를 이용혜 계산한다. \\(\\text{round}(t)\\) 는 \\(t\\) 를 반올림 하는 함수이다. OpenCV 에서는 equalizeHist() 함수로 구현되었으며 여기서는 이 함수를 사용한다.\n\n\n\n\n원본, \\(\\gamma=5\\) 처리된 이미지, 히스토그램 균등화 된 이미지\n\n\nimg5 = cv.equalizeHist(img4)\narr2mat(cat(img1, img4, img5; dims=2))\n\n\n\n원본, \\(\\gamma=5\\) 처리된 이미지, 히스토그램 균등화 된 이미지의 히스토그램",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 프로세싱의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_01.html#노이즈-생성",
    "href": "src/image_processing/image_processing_01.html#노이즈-생성",
    "title": "이미지 프로세싱의 기초",
    "section": "3 노이즈 생성",
    "text": "3 노이즈 생성\n노이즈의 원인은 다양하며, 노이즈에 대한 처리 방법도 다양하고 매우 중요하다. 일단 가장 빈번하고 다루기 쉬우며 보편적인 두가지 노이즈, 가우시안 노이즈 와 소금-후추 노이즈에 대해 알아보자. 노이즈의 원인은 생각하지 않고 실제 나타나는 양상으로만 분류한다. 또한 한 픽셀에서의 노이즈 발생 확률은 다른 픽셀의 노이즈 발셩 여부와 독립적이라고 가정하자.\n\n1. 가우시안 노이즈 : 가우시안 노이즈는 어떤 평균에 대해 가우시안 분포를 갖는 노이즈이다. 즉 노이즈를 생성시키려면 평균값과 표준편차로 정의되는 가우시안 분포에 따라 임의의 점에 대해 생성해야 한다.\n2. 소금-후추 노이즈 : 소금은 흰색이고 후추는 검은색이다. 소금-후추 노이즈는 보통 픽셀마다 최저값(0) 혹은 최고값 (UInt8 의 경우는 255) 에 가까운 노이즈가 발생하도록 한다. 흑백 이미지 상에서 최저값은 검은색, 최고값은 흰색으로 관례적으로 표현하기 때문에 노이즈가 마치 이미지에 소금과 후추를 뿌린 것 같다는 의미에서 소금-후추 노이즈라고 불린다.\n\n\n3.1 노이즈 생성 코드\n흑백 이미지에 대한 노이즈 생성 코드는 다음과 같다.\nusing Distributions\nfunction gaussian_noise(img::OpenCV.Mat{T}, μ, σ, N) where T&lt;:Union{UInt8, UInt16}\n    m, n = size(img)[2:3]\n    MM = typemax(T)\n    Y, X, V = rand(1:m, N), rand(1:n, N), round.(T, rand(truncated(Normal(μ, σ), 0, MM), N))\n    img2 = copy(img)\n    for (y, x, v) ∈ zip(Y, X, V)\n        img2[1, y, x] = v\n    end\n    return img2\nend\n\nfunction salt_pepper_noise(img::OpenCV.Mat{T}, N) where T&lt;:Union{UInt8, UInt16}\n    m, n = size(img)[2:3]\n    MM = typemax(T)\n    Y, X, V = rand(1:m, N), rand(1:n, N), sample([0, MM], N)\n    img2 = copy(img)\n    for (y, x, v) ∈ zip(Y, X, V)\n        img2[1, y, x] = v\n    end\n    return img2\nend\n아래 그림은 원본 이미지 testimage(\"cameraman.tif\") 에 대해 가우시안 노이즈와 소금-후추 노이즈를 생성시킨 결과이다.\nimg0= img2mat(testimage(\"cameraman.tif\"))\nimg_gn = gaussian_noise(img0, 100, 10, 10000)\nimg_sp = salt_pepper_noise(img0, 10000)\nimg2=arr2mat(cat([img_gn img_sp]; dims=2))\n\n\n\n\n\n\n그림 4: 가우시안 노이즈(왼쪽) 과 소금-후추 노이즈(오른쪽)\n\n\n\n두 이미지의 히스토그램은 다음과 같다. 가우시안 노이즈는 지정한 대로 픽셀값 100 근처에서 가우스 분포를 가지며, 소금-후추 노이즈는 최소값과 최대값에서 발생한다.\n\n\n\n\n\n\n그림 5: 가우시안 노이즈(왼쪽) 과 소금-후추 노이즈(오른쪽)",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 프로세싱의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_01.html#기하학적-변환과-보간법",
    "href": "src/image_processing/image_processing_01.html#기하학적-변환과-보간법",
    "title": "이미지 프로세싱의 기초",
    "section": "4 기하학적 변환과 보간법",
    "text": "4 기하학적 변환과 보간법\n영상에서 널리 이용되는 기하학적인 변환에는 아핀 변환(Affine transformation) 과 원근 변환(perspective transformation) 이 있다. 아핀 변환과 투시 변환은 모두 2차원 상의 직선이 직선으로 변환되는 공통점을 가지고 있다. 그러나 서로 평행한 두 직선이 변환 될 때 아핀 변환은 그 평행함이 유지되지만 투시 변환에서는 평행함이 유지되지 않는다.\n\n\n\n\n\n\n그림 6: 이미지의 기하학적 변환\n\n\n\n\n\n4.1 아핀 변환 (Affine transformation)\n대표적인 아핀 변환으로는 확대, 축소, 회전, 평행이동과 전단 변환등이 있으며, 아핀 변환의 합성도 아핀 변환이다. 아핀 변환은 \\(3\\times 3\\) 가역행렬 \\(\\boldsymbol{T}\\) 에 의해 정해지는 다음과 같은 변환으로 표현된다. 영상 변환에서는 \\(2\\) 차원 좌표로 \\(\\begin{bmatrix} x & y\\end{bmatrix}^T\\) 를 사용하지 않고 \\(\\begin{bmatrix}x & y & w \\end{bmatrix}^T\\) 로 표현되는 소위 homogeneous coordinate (동차 좌표) 를 사용한다. \\(w\\) 는 이미지의 스케일에 관련된 값으로 여기서는 우선 \\(1\\) 로 놓을 수 있다. 그리고 변환은 동차 좌표에 대해 수행하며 그 변환 행렬을 \\(\\boldsymbol{T}\\) 라고 하자. 그렇다면 다음과 같이 쓸 수 있다.\n\\[\n\\begin{bmatrix} x' \\\\ y' \\\\1\\end{bmatrix} = \\boldsymbol{T} \\begin{bmatrix} x \\\\ y \\\\ 1\\end{bmatrix}.\n\\]\n예를 들어 \\(\\theta\\) 만큼의 반시계 방향 회전 변환 \\(\\boldsymbol{T}_\\theta\\) 는\n\\[\n\\boldsymbol{T}_\\theta = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta & 0 \\\\ \\sin \\theta & \\cos \\theta & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\n\\]\n이며 \\(x\\) 방향으로 \\(d_x\\) \\(y\\) 방향으로 \\(d_y\\) 만큼의 이동 변환 \\(\\boldsymbol{T}_\\boldsymbol{d}\\) 는\n\\[\n\\boldsymbol{T}_\\boldsymbol{d} = \\begin{bmatrix}  1 & 0 & d_x \\\\ 0 & 1 & d_y \\\\ 0 & 0 & 1\\end{bmatrix}\n\\]\n이다. 또한 \\(x\\) 축 혹은 \\(y\\) 축 방향으로 기울이는 전단 변환(shear tranformation) \\(\\boldsymbol{T}_{Sx},\\, \\boldsymbol{T}_{Sy}\\) 은 각각\n\\[\n\\boldsymbol{T}_{Sx} = \\begin{bmatrix} 1 & s_x & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix},\\qquad \\boldsymbol{T}_{Sy} = \\begin{bmatrix} 1 & 0 & 0 \\\\ s_y & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\n\\]\n이다. 또한 \\(x\\) 축 방향으로 \\(a_x\\) 배 만큼, \\(y\\) 축 방향으로 \\(a_y\\) 배 만큼 키우거나 줄이는 변환 \\(\\boldsymbol{T}_c\\) 는\n\\[\n\\boldsymbol{T}_c = \\begin{bmatrix} a_x & 0 & 0 \\\\ 0 & a_y & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\n\\]\n이다.\n아핀 변환 행렬 \\(\\boldsymbol{T}\\) 는 \\(2 \\times 2\\) 행렬 \\(\\boldsymbol{R}\\) 과 \\(2 \\times 1\\) 행렬 \\(\\boldsymbol{t}\\) 에 대해\n\\[\n\\boldsymbol{T}=\\begin{bmatrix} \\boldsymbol{R} & \\boldsymbol{t} \\\\ \\boldsymbol{0}^T & 1\\end{bmatrix}\n\\]\n로 나타 낼 수 있다. \\(\\boldsymbol{R}\\) 은 회전이나 전단 변화 같은 기하학적 변형을 나타내는 행렬이고 \\(\\boldsymbol{t}\\) 는 평행이동을 나타내는 행렬이다. \\(\\boldsymbol{T}\\) 의 역행렬은\n\\[\n\\boldsymbol{T}^{-1} = \\begin{bmatrix}\\boldsymbol{R}^{-1} & -\\boldsymbol{R}^{-1}\\boldsymbol{t}\\\\ \\boldsymbol{0}^T & 1\\end{bmatrix}\n\\]\n이다. \\(\\boldsymbol{T}\\) 와 \\(\\boldsymbol{T}^{-1}\\) 에서 보듯이 \\(\\boldsymbol{T}\\) 를 이루는 \\(\\boldsymbol{R}\\) 과 \\(\\boldsymbol{t}\\) 만으로 아핀 변환과 역변환을 모두 나타낼 수 있으며, OpenCV 의 경우 \\(2\\times 3\\) 행렬 \\(\\begin{bmatrix} \\boldsymbol{R} & \\boldsymbol{t}\\end{bmatrix}\\) 만으로 2차원 아핀 변환을 나타낸다. 아핀 변환의 미지수는 \\(\\boldsymbol{R}\\) 의 행렬 성분 \\(4\\) 개와 \\(\\boldsymbol{t}\\) 의 행렬 성분 \\(2\\) 개이며 따라서 6개의 파라미터가 결정되어야 한다. 즉 서로 한 직선상에 위치하지 않은 좌표상의 세 점과 그 점들이 변환되는 점을 안다면 아핀 변환을 결정 할 수 있다.\n\n\n\n4.2 원근 변환 (Perspective transformation)\n\n\n\n\n\n\n그림 7: 원근 변환. 출처: https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html\n\n\n\n아핀 변환은 평행선이 평행선으로 유지되는 것과는 달리 원근 변환은 평행선이 평행선으로 유지되지 않는다. 원근 변환은 기본적으로 입체인 피사체를 2차원인 영상으로 변환시킬 때 발생하는 왜곡이다. 우리가 담는 피사체의 각 부분과 카메라 사이의 거리가 다르며, 피사체의 방향과 카메라의 방향에 따라 영상이 변형되기 때문이다. 원본 이미지 \\(f[i, j]=f(j, i)\\) 에 대해 원근 변환은 다음과 같은 변환행렬로 표현 할 수 있다.\n\\[\n\\begin{bmatrix} wx' \\\\ wy' \\\\ w'\\end{bmatrix} =\\boldsymbol{P}\\begin{bmatrix} x \\\\ y \\\\ 1\\end{bmatrix},\\qquad \\text{where }P_{33}=1\n\\]\n여기서 \\(w\\) 원본 이미지에 대해 변환된 이미지의 스케일을 정하는 값이다. 원근 변환은 행렬 \\(\\boldsymbol{P}\\) 에 의해 결정되며 \\(P_{33}=1\\) 이어야 하므로 모두 8개의 미지수가 존재한다. 따라서 네 점(물론 네 점의 가운데 아무 세점을 골라도 한 직선상이 있지 않아야 한다) 으로 원근 변환을 결정 할 수 있다. 변환된 이미지 \\(P[f][i', j'] = P[f][wy',\\, wx']\\) 이다.\n\n\n\n4.3 보간법\n아핀 변환 혹은 원근 변환 \\(T\\) 에 대해 이미지 \\(F\\) 를 변환시켜 \\(G=T[F]\\) 인 이미지를 얻고자 한다고 하자. 즉 정해진 \\(i,\\,j\\) 의 범위에 대해 \\(G[i,j]\\) 전체를 얻어야 한다. \\(T\\) 가 가역변환이므로 \\(F=T^{-1}[G]\\) 이므로 \\(F[i',\\,j'] = T^{-1}[G][i, j]\\) 를 통해 \\(F[i',\\,j']\\) 의 값으로 \\(g[i,\\,j]\\) 를 채울 수 있으면 좋겠지만 \\(i',\\,j'\\) 은 대부분 정수가 아니다. 이 때 보간법(interpolation) 을 사용하여, \\(T[G]^{-1}[i,\\,j]\\) 로 정해지는 \\((x,\\,y)\\) 근처의 \\(F[i,j]\\) 값을 이용하여 \\(G[i,\\,j]\\) 값을 구한다.\n\n\n최근접 이웃 보간\n\\((x,\\,y)\\) 에 가장 가까운 정수 값으로 보간하는 것이다. 즉 \\[\nF(x,\\,y) \\mapsto F(\\text{round}(x),\\, \\text{round}(y))\n\\tag{6}\\]\n를 사용한다. 가장 간단하면서도 빠르지만 변환된 이미지의 품질이 좋지 못하다.\n\n\n\n이중 선형 보간\n\\(x,\\,y\\) 에 대해\n\\[\ni\\le y &lt; i+1,\\, j\\le x&lt;j+1\n\\]\n인 정수 \\(i,\\,j\\) 를 찾아 \\(f[i,j]\\), \\(f[i,j+1]\\), \\(f[i+1, j]\\), \\(f[i+1, j+1]\\) 인 네 점을 이용한다.\n\\[\n\\begin{aligned}\nF(x,\\,y) &= (1-x+j)(1-y+i)F(j, i) + (1-x+j)(y-i)F(j,\\,i+1) \\\\[0.3em]\n&+ (x-j)(1-y+i)F(j+1, i) + (x-j)(y-i)F(j+1, i+1).\n\\end{aligned}\n\\tag{7}\\]\n앞서의 최근접 이웃 보간보다는 계산량이 많고 이미지 품질이 좋다.\n\n\n\n이중 큐빅 보간\n\\(x,\\,y\\) 에 대해 \\[\ni\\le y &lt; i+1,\\, j\\le x&lt;j+1\n\\]\n인 정수 \\(i,\\,j\\) 를 찾아 \\(f[i+k,j+m]\\), \\(k,\\,m = -1,\\,0,\\,1,\\,2\\) 인 16개의 점을 이용한다.\n\\[\nd(s) = \\left\\{\\begin{array}{ll} \\dfrac{3|s|^3}{2}-\\dfrac{5|s|^2}{2}+1, & 0 \\le |s| &lt; 1, \\\\ -\\dfrac{|s|^3}{2}+\\dfrac{5|s|^2}{2}-4|s|+2, \\qquad & 1\\le |s|&lt;2 , \\\\ 0 & |s|&gt;2 \\end{array}\\right.\n\\]\n에 대해 다음 함수를 이용하여 계산한다. \\[\nF(x,\\,y) = \\sum_{k=-1}^2 \\sum_{m=-1}^2 F(j+k, i+m)d(x- j-k)d(y - i-m)\n\\]\n\n아래 그림은 1차원 데이터의 보간법과 2차원 이미지의 보간법을 설명하는 그림이다.\n\n\n\n\n\n\n그림 8: 1차원 및 2차원 보간법(출처 : https://en.wikipedia.org/wiki/Bicubic_interpolation) By Cmglee - Own work, CC BY-SA 4.0, Link\n\n\n\n\n이제 변환 행렬과 보간법을 사용하여 기하학적으로 이미지를 변환시킬수도, 혹은 변환된 이미지를 복원시킬 수도 있다. 아핀 변환 행렬은 직접 입력할 수도 있고, 원래의 이미지와 변환되는 이미지에서 각 3개(아핀 변환의 경우) 혹은 4개(원근 변환의 경우) 를 선택하여 변환 행렬을 계산 할 수도 있다. 예를 들어 다음과 같이 변환하는 아핀 변환을 생각하자.\n\\[\n[1, 1] \\to [4, 5],\\qquad [2, 1] \\to [5, 6], \\qquad [3, 3] \\to [4, 2]\n\\]\n아핀 변환을 위해서는 변환된 이미지의 크기를 OpenCV.Size{UInt32} 형태의 객체로 전달해야 한다. 이것을 cvSize 함수로 구현하였다.\nfunction cvSize(w::T1, h::T2) where {T1&lt;:Integer, T2&lt;:Integer}\n    return cv.Size(Int32(w), Int32(h))\nend\n\np0 = Float32[1 1; 2 1; 3 3]\nq0 = Float32[4 5; 5 6; 4 2]\np1, q1 = arr2mat(p0), arr2mat(q0)\n\nH = cv.getAffineTransform(p1, q1)\nimg1 = cv.warpAffine(img0, H, cvSize(512, 512))\ncv.warpAffine(img0, H, cvSize(512, 512)) 는 이미지 img0 를 아핀 변환 행렬 H 를 이용하여폭 512, 높이 512 크기의 이미지로 변환하라는 명령이다.\n많이 사용되는 몇몇 변환에 대해서는 아핀 변환 행렬을 얻는 함수가 이미 존재한다. 예를 들어 (cX, cY) 를 중심으로 45도 만큼 회전시키며, 스케일을 0.7 배로 줄이는 변환에 대한 변환행렬은\nM=cv.getRotationMatrix2D(cvPoint(cX, cY), 45.0, 0.7)\n으로 얻을 수 있다. 여기서 cvPoint 함수는 2차원 혹은 3차원 상의 점을 OpenCV 의 OpenCv.Point(x, y) 객체로 반환하는 함수이다.\nfunction cvPoint(x::T1, y::T2) where {T1&lt;:Real, T2&lt;:Real}\n    T = promote_type(T1, T2)\n    return cv.Point{T}(T(x), T(y))\nend\n\nfunction cvPoint(x::T1, y::T2, z::T3) where {T1&lt;:Real, T2&lt;:Real, T3&lt;:Real}\n    T = promote_type(T1, T2, T3)\n    return cv.Point3{T}(T(x), T(y), T(z))\nend\n\n이를 이용하여 이미지를 45도 회전하고 크기를 0.7배로 줄이는 변환을 수행해보자. 두가지 보간법을 사용하였다.\nh, w = size(img0)[2:3]\ncX, cY = Float32(h/2), Float32(w/2)\nM = cv.getRotationMatrix2D(cvPoint(cX, cY), 45.0, 0.7)\nr1 = cv.warpAffine(img0, M, cvSize(w, h), flags=cv.INTER_NEAREST)\nr2 = cv.warpAffine(img0, M, cvSize(w, h), flags=cv.INTER_CUBIC)\nimg3 = arr2mat(cat([r1 r2]; dims=2))\n\n\n\n\n\n\n그림 9: 이미지 회전. (왼쪽) 최근접 보간, (오른쪽) 이중 큐빅 보간\n\n\n\n보간법에 따른 이미지가 큰 차이가 보이지 않을 수 도 있지만 이것은 아래에서 좀 더 정확히 보일 것이다.\nTestImages.jl 로 부터 \\(256\\times 256\\) 이미지 cameraman.tif 을 다운 받은 후 \\(100 \\times  100\\) 으로 크기를 줄였다. 그리고 그 이미지를 앞서 소개한 세가지 방법으로 확대하였으며 결과는 아래와 같다.\nimg0= cv.resize(img2mat(testimage(\"cameraman.tif\")), cv.Size{Int32}(100, 100))\nimg1 = cv.resize(img0, cv.Size(Int32(256), Int32(256));interpolation= cv.INTER_NEAREST)\nimg2 = cv.resize(img0, cv.Size(Int32(256), Int32(256));interpolation= cv.INTER_LINEAR)\nimg3 = cv.resize(img0, cv.Size(Int32(256), Int32(256));interpolation= cv.INTER_CUBIC)\ns = arr2mat(cat(img1, img2, img3; dims=2))\n\n\n\n왼쪽부터 최근접 이웃 보간, 이중 선형 보간, 이중 큐빅 보간",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 프로세싱의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_01.html#sec-ImageProcessing_spatial_image_filtering",
    "href": "src/image_processing/image_processing_01.html#sec-ImageProcessing_spatial_image_filtering",
    "title": "이미지 프로세싱의 기초",
    "section": "5 공간적 이미지 필터링",
    "text": "5 공간적 이미지 필터링\n공간적 필터링은 이미지를 2차원 공간으로 간주한다. 어떤 픽셀의 값을 그 주변값의 연산을 통해 바꿔서 원하는 목적을 달성하는 것을 공간적 이미지 필터링 이라고 한다. 많은 경우 2차원 배열을 사용하며 이 배열을 커널(kernel) 혹은 마스크(mask) 라고 한다. 즉 원본 이미지 \\(F\\) 의 픽셀 \\([i,\\,j]\\) 에 대해 부근의 픽셀의 집합 \\(N_{[i,j]}\\) 을 사용하는 어떤 연산 \\(H\\) 가 정의되어 연산된 이미지 \\(G\\) 가 만들어 진다면, 즉\n\\[\nG[i,j] = H\\left(N_{[i,j]}\\right)\n\\tag{8}\\]\n일 때 이 과정을 공간적 이미지 필터링 이라고 한다. 만약 \\(G[i,\\,j]\\) 가 \\(N_{[i, j]}\\) 의 성분들의 선형결합이라면 이 필터를 선형 필터(linear filter) 라고 하며, 그렇지 않다면 비선형 필터라고 한다. 대표적으로 주변 \\(n\\) 개의 픽셀의 평균값을 취하는 평균값 필터가 선형 필터이며, 중간값을 취하는 중간값 필터는 비선형 필터이다. 선형 필터는 다음에 나올 합성곱으로 표현 할 수 있다.\n\n\n5.1 가장저리 처리\n뒤에 나오겠지만 평균값 필터는 이미지의 어떤 픽셀과 그 주변의 선택된 픽셀값과의 평균으로 필셀값을 대체하는 필터이다. 예를 들어 이미지 \\(f\\) 의 \\([i, j]\\) 를 중심으로 하는 \\(3\\times 3\\) 정사각형에 속하는 9개의 픽셀값의 평균값으로 필터링 한다면\n\\[\nG[i,\\,j]=\\frac{1}{9} \\sum_{p=-1,\\,0,\\,1} \\sum_{q=-1,\\,0,\\,1} F[i-p,\\,j-q]\n\\]\n일 것이다. 노이즈 처리에 사용 될 수 있다. 이 경우 가장자리 라면, 예를 들어 \\(i=1\\) 이면 \\(i-1\\) 픽셀이 없으므로 문제가 된다. 일반적으로 필터링의 범위를 고려하여 필터링에 방해가 되지 않도록 이미지의 크기를 일시적으로 키우고 원본 이미지에 없던 부분을 적당한 값으로 채운 후(이 작업을 패딩(padding) 이라고 한다) 그 이미지 \\(\\overline{F}\\) 에 대해 필터링을 하고, 필터링이 모두 끝난 후 필터링된 이미지 \\(\\overline{g}\\) 를 원본 이미지에 맞게 크기를 줄이게 된다. 패딩을 할 때 특정한 값으로 채울 수도 있고(상수 패딩(constant padding)), 가장 가까운 원본 이미지 픽셀값을 복사할 수도 있으며(복제 패딩(replication padding)), 패딩되는 픽셀에 가장 가까운 원본 이미지 픽셀을 중심으로 대칭이 되도록 원본 픽셀 값을 복사할 수도 있다(반사 패딩(reflection padding)).\n\n\n\n\n\n\n\n그림 10: 이미지 패딩\n\n\n\n\n이후로는 이미지 필터를 다룰 때 사용자가 적절한 패딩을 선택했다고 가정한다.\n\n\n\n5.2 평균값 필터 (mean filter)\n\nhttps://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html 를 참고하라.\n\n픽셀 \\(F[i,j]\\) 주변의 선택된 픽섹에 대한 평균값으로 이미지를 변환시키는 것을 평균값 필터라고 한다. 노이즈를 줄이고 이미지를 부드럽게 하는 데 사용된다. 필터의 크기는 폭 w, 높이 h 에 대해 cv2.Size(w, h) (즉 여기서는 cvSize(w, h)) 로 결정한다.\ncv.blur(img0, 3)\n\n\n\n\n\n\n그림 11: 원본 이미지(왼쪽) 와 \\(5\\times 5\\) 평균값 필터로 처리된 이미지(오른쪽)\n\n\n\n\n\n\n5.3 중앙값 필터 (median filter)\n비선형 필터의 대표적인 필터는 중앙값 필터로 특히 소금-후추 노이즈를 제거하는데 많이 사용된다. 중앙값 필터는 원본 이미지 img 와 커널 크기 ksize 에 대해 OpenCV.medianBlur(img, ksize) 와 같이 사용한다. ksize==3 일 경우 \\([i, j]\\) 를 중심으로하는 \\(3 \\times 3\\) 크기의 부분 이미지에 대한 중간값으로 변환시킨다. 앞의 그림 5 의 소금-후추 노이즈 이미지(img_sp) 를 중간값 처리 한 결과는 아래와 같다.\ncv.medianBlur(img_sp, 3)\n\n\n\n\n\n\n그림 12: 소금 후추 노이즈 이미지(왼쪽) 과 중간값 필터로 처리된 이미지(오른쪽)\n\n\n\n\n\n\n5.4 표준편차 필터(standard deviation filter)\n표준편차 필터는 주변 픽셀값들의 표준편차로 픽셀값을 변환시킨다. 만약 한 픽셀 주변의 값이 서로 크게 바뀐다면 표준 편차 값이 클 것이다. 한 픽셀 주변 값이 서로 비슷하다면 표준 편차 값이 작을 것이다. 일단 이것을 구현해 보자. 우리는 표준편차가 제곱의 평균값에서 평균값의 제곱을 뺀 값의 양의 제곱근이라는 것을 안다.\nimgx = cv.Mat(convert.(Float32, img0))\nimgx_sq = cv.Mat(imgx .* imgx)\nimgx_mu = cv.blur(imgx, cvSize(3, 3))\nimgx_sqmu = cv.blur(imgx_sq, cvSize(3, 3))\nimgx_musq = cv.Mat(imgx_mu .* imgx_mu)\nimg_sdfiltered = cv.Mat(imgx_sqmu .- imgx_musq)\n\n\n\n\n\n\n그림 13: 표준편차 필터\n\n\n\n표준편차 필터는 위의 영상처럼 원본 영상 속 피사체의 윤곽을 도드러지게 보여준다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 프로세싱의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_01.html#sec-ImageProcessing_convolution_and_correlation",
    "href": "src/image_processing/image_processing_01.html#sec-ImageProcessing_convolution_and_correlation",
    "title": "이미지 프로세싱의 기초",
    "section": "6 합성곱 개념으로서의 이미지 필터링",
    "text": "6 합성곱 개념으로서의 이미지 필터링\n합성곱과 상관값은 공간적 이미지 필터링의 일종이다.\n\n6.1 합성곱과 상관값\n\n\n\n\n\n\n\n정의 1 (합성곱과 상관값) 원본 이미지 \\(F[i,\\,j]\\) 와 커널 \\(K[s,\\,t]\\) (혹은 마스크(mask) 라고도 한다) 을 생각하자. 이 때 커널은 \\((2a+1)\\times (2b+1)\\) 크기의 2차원 배열이며 커널 배열의 인덱스는\n\\[\n\\begin{aligned}\ns:& -a,\\, -(a-1),\\ldots,\\, -1,\\,0,\\,1,\\ldots,\\,a-1,\\,a, \\\\\nt:&-b,\\, -(b-1),\\ldots,\\, -1,\\,0,\\,1,\\ldots,\\,b-1,\\,b,\n\\end{aligned}\n\\]\n로 잡는다. 이미지와 커널 사이의 합성곱(convolution) \\(k \\ast I\\) 와 상관값(correlation) \\(k \\otimes I\\) 는 각각 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\n(K \\ast F)[i, j] &:= \\sum_{s=-a}^{a} \\sum_{t=-b}^b K[s,\\,t] \\,F[i-s,\\, j-t],\\\\[0.3em]\n(K \\otimes F)[i,j] &:= \\sum_{s=-a}^{a} \\sum_{t=-b}^b K[s,\\,t]\\, F[i+s,\\, j+t].\n\\end{aligned}\n\\tag{9}\\]\n\n\n\n\n\n우선 상관값을 살펴 보자. 커널이\n\\[\nK = \\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1\\end{bmatrix}\n\\]\n이고 원본 이미지의 \\(F[i, j]\\) 를 중심으로 한 \\(3\\times 3\\) 배열이\n\\[\n\\tilde{F}_{[i, j]}= F[i-1:i+1, j-1:j+1]= \\begin{bmatrix} 4 & 0 & 2 \\\\ 1 & 0 & 3 \\\\ 2 & 3 & 1\\end{bmatrix}\n\\]\n이라면 \\((K\\otimes F)[i, j]\\) 는 \\(K\\) 와 \\(\\tilde{F}_{[i, j]}\\) 의 성분별 곱의 합으로 정의된다. 즉\n\\[\n(k\\otimes I)[i, j] = 1\\times 4 + (-1)* 2 + 1 * 1 + (-1)\\times 3 + 1 \\times 2 + (-1)\\times 1 = 1\n\\]\n이다. 그러나 합성곱은 식 9 에서 알 수 있듯이 \\(K\\) 의 성분과 대각 위치에 있는 \\(\\tilde{F}_{[i, j]}\\) 의 성분의 모든 곱의 합이다. 즉, \\[\n(K \\ast F)[i, j] = 1\\times 2 + (-1)* 4 + 1 * 3 + (-1)\\times 1 + 1 \\times 1 + (-1)\\times 2 = -1\n\\]\n이다. 혹은 합성곱은 커널을 180 도 회전시킨 행렬과의 상관값이라고 생각 할 수도 있다. 또 커널과 합성곱의 중요한 차이는 아래 표와 같다.\n\n\n\n표 1: 합성곱과 상관값의 수학적 성질\n\n\n\n\n\n\n\n\n\n\n성질\n합성곱\n상관값\n\n\n\n\ncommtative\n\\(F\\ast G = G \\ast F\\)\n\\(-\\)\n\n\nassociative\n\\(F\\ast(G \\ast H) = (F \\ast G)\\ast H\\)\n\\(-\\)\n\n\ndistributive\n\\(F\\ast(G+H)= F\\ast G + F \\ast H\\)\n\\(F\\otimes (G + H) = F\\otimes G + F \\otimes H\\)\n\n\n\n\n\n\n\n일반적으로 상관값이 계산량이 적고 직관적인데 비해 합성곱이 수학적으로 좋은 성질을 많이 가지고 있다. 또한 커널이 중심에 대해 대칭이라면 합성곱과 상관값은 동일하다.\n\n\n\n예제 1 정의 1 의 커널 인덱스 표기법을 사용하자. 커널이 \\(K[-s,\\,-t] = K[s,\\,t]\\) 라면 \\(K\\ast F =  K \\otimes F\\) 임을 보일 수 있다.\n\\[\n\\begin{aligned}\n(K \\ast F)[i,\\,j] &= \\sum_{s=-a}^{a} \\sum_{t=-b}^b K[s,\\,t] \\,F[i-s,\\, j-t] &&; s\\to -s,\\, t\\to -t \\\\\n&=\\sum_{s=-a}^{a} \\sum_{t=-b}^b K[-s,\\, -t] F[i+s,\\, j+t] &&;K[-s,\\,-t] = K[s,\\,t]\\\\\n&= \\sum_{s=-a}^{a} \\sum_{t=-b}^b K[s,\\, t] F[i+s,\\, j+t] \\\\\n&= (K\\otimes F)[i, j]\n\\end{aligned}\n\\]\n\n\n\n가끔 합성곱과 상관값이 서로 혼용되기도 한다. 예를 들어 신경망의 합성곱 신경망(convolutional neural network, CNN) 에서는 실제로 이름에 나온 합성곱 연산이 아닌 상관 연산을 사용한다. 그러나 역사적, 관례적 이유로 합성곱 신경망이라는 이름을 사용한다.\n\n\n\n6.2 분리 가능한 필터 커널\n\n\n\n\n\n\n\n정의 2 (분리 가능한 커널) 열벡터 \\(\\boldsymbol{v},\\, \\boldsymbol{w}\\) 에 대해 커널 행렬 \\(\\boldsymbol{K}\\) 가\n\\[\n\\boldsymbol{K}= \\boldsymbol{v}\\boldsymbol{w}^T\n\\]\n의 형태로 쓸 수 있을 때 이 커널을 분리 가능한 커널이라고 한다.\n\n\n\n\n\n\n\n명제 1 \\(m \\times n\\) 행렬 \\(\\boldsymbol{K}\\) 가 정의 2 와 같이 분리가능하다면 \\(\\boldsymbol{K} = \\boldsymbol{v}\\ast \\boldsymbol{w}^T\\) 이다. 즉 두 벡터의 합성곱이다.\n\n\n\n\n(증명). \\[\nK[i,j] = v_i w_j = v[i, 0] (w^T)[0, j]=\\sum_{s=0} \\sum_{t=0} v[s, t] w^T[0+s, j+t] = v \\ast w^T\n\\]\n이므로 \\(\\boldsymbol{K}\\) 는 \\(\\boldsymbol{v}\\) 와 \\(\\boldsymbol{w}^T\\) 의 합성곱이다. \\(\\square\\)\n\n\n\\(K= v \\ast w\\) 라면\n\\[\nK \\ast f = (v \\ast w) \\ast f = v \\ast (w \\ast f)\n\\]\n이다. 즉 분리 가능한 커널을 순차적으로 적용시킨 것과 같다. \\(M \\times N\\) 이미지 \\(f\\) 에 대해 \\(m\\times n\\) 커널 \\(K\\) 를 이용하여 필터링했다면 한 픽셀당 \\(mn\\) 번의 곱셈과 \\(mn-1\\) 번의 덧셈, 그리고 1번의 할당을 수행하며, 이것을 \\(MN\\) 번 수행하므로 모두 \\(MN(2mn)\\) 번의 연산을 수행한다. \\((w\\ast f)\\) 연산에는 \\(MN(2n)\\) 번의 연산을 수행하며 이후 \\(v\\) 를 적용하면 \\(MN(2m)\\) 번의 연산을 수행하므로 모두 \\(MN(2m+2n)\\) 번의 연산을 수행한다. 즉 두가지 동일한 결과를 낳는 연산의 연산 횟수 비가\n\\[\n\\text{ratio}=\\dfrac{Mn(2mn)}{Mn(2m+2n)}= \\dfrac{mn}{m+n}\n\\]\n이다. \\(m=n=5\\) 일 경우 분리 가능한 커널을 분리하여 사용하는 것이 연산 횟수가 2.5배 줄며 \\(m=n=11\\) 일 경우 5.5 배 줄게 된다. 물론 연산 횟수의 감소가 그만큼의 시간의 절약을 정확하게 의미하지는 않지만 커널이 클 때 분리가능하다면 이 방법을 사용하는 것은 큰 장점이 된다.\n\n\n\n명제 2 영행렬이 아닌 \\(m \\times n\\) 행렬 \\(\\boldsymbol{K}\\) 가 정의 2 와 같이 분리가능한 것과 \\(\\operatorname{rank}(\\boldsymbol{K})=1\\) 은 동치이다.\n\n\n\n\n(증명). \\(\\boldsymbol{K}=\\boldsymbol{v}\\boldsymbol{w}^T\\) 이고 \\(\\boldsymbol{v}\\), \\(\\boldsymbol{w}\\) 가 모두 열벡터 이므로 두 행렬의 \\(\\operatorname{rank}\\) 는 \\(0\\) 혹은 \\(1\\) 이다. 둘 중 하나가 \\(\\operatorname{rank}0\\) 이면 \\(\\boldsymbol{K}=\\boldsymbol{0}\\) 이므로 가정에 모순된다. 따라서 두 열벡터는 모두 \\(\\operatorname{rank}1\\) 열벡터 이며 따라서 \\(\\operatorname{rank}(\\boldsymbol{K})=1\\) 이다.\n이제 \\(\\operatorname{rank}(\\boldsymbol{K})=1\\) 이면 \\(\\boldsymbol{K}\\) 에는 \\(\\boldsymbol{0}^T\\) 가 아닌 행 \\(\\boldsymbol{k}^T\\) 가 존재하며 나머지 행은 \\(\\boldsymbol{k}^T\\) 의 (0 을 포함한)스칼라 곱이다. 첫번째 행 \\(\\boldsymbol{k}^T\\) 이 \\(\\boldsymbol{0}^T\\) 가 아니라고 하자.\n\\[\n\\boldsymbol{K} = \\begin{bmatrix} \\boldsymbol{k}^T \\\\ a_2 \\boldsymbol{k}^T \\\\ \\vdots \\\\ a_m \\boldsymbol{k}^T\\end{bmatrix} = \\begin{bmatrix} 1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{bmatrix} \\boldsymbol{k}^T\n\\]\n이다. 즉 \\(\\operatorname{rank}(\\boldsymbol{K})=1\\) 이면 \\(\\boldsymbol{K}\\) 는 분리가능하다. \\(\\square\\)\n\n\n합성곱은 cv.filter2D 함수를 이용한다. OpenCV 관련 페이지 를 참고하라. 뒤에 나올 소벨 커널(식 16) 의 \\(G_x\\) 를 적용하면\nK1 = arr2mat(Int32[1 0 -1; 2 0 -2; 1 0 -1])\ncv.filter2D(img0, -1, K1)\n\n\n\n\n\n\n그림 14: 2D 필터\n\n\n\ncv.filter2D 함수의 두번째 인자 -1 은 입력 이미지 img0 와 같은 성분의 타입을 사용하라는 의미이다.\n\n\n\n6.3 가우시안 필터\n가우시안 블러링 이라고도 한다. 평균값 필터와 비슷한 역할로 노이즈와 이미지의 급격환 변화를 줄이는 역할을 한다. \\(m \\times n\\) 커널에 대해 커널의 중심 \\(c_x,\\,c_y\\) 와 표준편차 \\(\\sigma_x,\\, \\sigma_y\\) 에 의한 2차원 가우스 분포를 갖는 커널에 의한 변환이다.\n\\[\nG[i,j] = \\dfrac{1}{a} \\exp \\left[-\\dfrac{(c_x-j)^2}{2\\sigma_x^2} - \\dfrac{(c_y-i)^2}{2\\sigma_x^2}\\right]\n\\]\n여기서 \\(a\\) 는 전체 커널의 성분의 합이 \\(1\\) 이 되도록 하는 상수이다. OpenCV 에서 가우시안 필터링은 다음과 같이 수행한다.\ncv.GaussianBlur(img0, cvSize(5, 5), 1.3)\n여기서 img0 는 원본 이미지 이며, cvSize(5, 5) 는 가우시안 커널 사이즈의 높이와 폭을 의미한다. 그 다름은 가우시안 커널의 \\(x\\) 방향 표준편차이며, 이후 \\(y\\) 방향 표준편차를 정해주지 않는다면 \\(x\\) 방향 표준편차와 동일하게 잡는다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 프로세싱의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_01.html#빈도-필터",
    "href": "src/image_processing/image_processing_01.html#빈도-필터",
    "title": "이미지 프로세싱의 기초",
    "section": "7 빈도 필터",
    "text": "7 빈도 필터\n\n7.1 이미지에 있어 ‘빈도 (frequency)’\n이미지의 빈도 혹은 주파수(frequency) 는 매우 많이 등장하는 개념이지만 혼동되기 쉽다. 이미지에서 어떤 픽셀과 바로 이웃한 픽셀의 값이 매우 큰 차이를 보일 때 그 픽셀을 중심으로 픽셀 값의 ’변화’는 매우 크다고 할 수 있다. 이렇게 픽셀 값의 변화가 매우 짧은 픽셀 간격에서 나타날 때, 그 픽셀은 높은 주파수 성분, 반대의 경우를 낮은 주파수 성분이라고 부른다.\n\n\n하이패스 필터 (high-pass filter), 로우패스 필터 (low-pass filter)\n\n필터가 낮은 빈도를 나타내는 성분만 제거하거나 픽셀 값을 줄이는 기능을 갖는다면 하이패스(high-pass) 필터라 하고, 그 반대의 경우를 로우패스(low-pass) 필터라 한다.\n사실 하이패스 필터는 이미지에서 가장자리 (엣지, edge)를 찾아 내거나 가장자리 신호를 선택적으로 강화하는 기능으로 자주 사용된다.\n로우 패스 필터는 반대로 픽셀 값의 변화를 줄이는 역할을 한다. 일반적으로 노이즈 감소 필터는 로우 패스 필터이다. 앞서 살펴 본 평균값 필터, 중간값 필터, 가우시안 필터 등이 로우패스 필터이다. 로우 필터 패스를 사용하면 영상의 차이가 흐려지는 역할을 하기 때문에 블러링(blurring) 이라고 불리기도 한다.\n\n\n\n\n\n7.2 Unsharp masking & highboost filtering\n\n\nUnsharp mask\n\n원본 이미지를 일단 평균값 필터링 같은 로우패스 필터로 블러링 (blurring) 처리한 후, 원본 이미지에서 처리된 이미지를 빼서 차이 값으로 이루어진 행렬을 ‘Unsharp mask’ 라 한다. 원본이미지를 \\(F[i,j]\\), 블러링 된 이미지를 \\(\\overline{F}[i,j]\\) 라 하면 unsharp mask \\(M[i,j]\\) 는 다음과 같다.\n\n\\[\nM[i,j]=F[i,j]-\\overline{F}[i,j]\n\\tag{10}\\]\n\n\n\nHighboost filtering\n\n이 때 어떤 1보다 큰 양의 실수 \\(c\\) 를 \\(M[i,j]\\) 에 곱해서 원본에 더해준다면 가장자리신호가 더 현저해지며 이것을 highboost filtering 이라 한다. 즉\n\n\\[\nG[i,j]=F[i,j]+c\\,M[i,j] = F[i,j]+c(F[i,j]-\\overline{F}[i,j])\n\\tag{11}\\]\n\n\n\n\n7.3 경계 검출을 위한 하이 패스 필터 커널\n경계 검출을 위해서는 일반적으로 2차원 공간에서의 미분, 즉 gradient 를 사용한다. 영상의 특정 픽셀에서의 미분을 구한다고 하자.\n\\[\n\\nabla_F F[i, j] = (F[i, j+1] - F[i, j],\\, F[i+1, j]-F[i, j])\n\\tag{12}\\]\n로 구할 수 있다. 이를 전방 미분, 혹은 전방 차분이라고 한다. 다른 방법으로\n\\[\n\\nabla_B F[i, j] = (F[i, j] - F[i, j-1],\\, F[i+1, j]-F[i, j+1])\n\\tag{13}\\]\n로 구할 수 있다. 이를 후방 미분, 혹은 후방 차분이라고 한다. 경계 검출에는 가로 방향의 하이패스 필터와 세로 방향의 하이패스 필터를 모두 고려해야 하며 보통\n\\[\nG[i, j] = \\| \\nabla F[i,\\,j]\\| \\qquad \\text{or} \\qquad \\| \\nabla F[i,\\,j]\\|^2\n\\]\n를 사용한다. 단순히 미분만을 고려한 위의 방법은 노이즈에 취약하기 때문에 잘 사용되지 않으며 아래의 방법이 많이 사용된다.\n\n\n로버츠 커널 (Roberts kernel)\n커널 \\[\nK_1 = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{bmatrix}, \\quad K_2=\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & -1 \\\\0&1&0 \\end{bmatrix}\n\\tag{14}\\]\n을 로버츠 커널이라고 한다. \\(K_1,\\,K_2\\) 는 사선 방향의 차분임을 알 수 있다. 이 커널에 대해 \\(J_1 = K_1 \\ast F,\\, J_2 = K_2 \\ast F\\) 를 얻고\n\\[\nJ[i,j] = \\sqrt{J_1[i, j]^2 + J_2[i,j]^2}\n\\]\n으로 변환된 이미지를 얻으면 경계에 대한 정보를 얻을 수 있다. 단순 차분 커널보다는 낳지만 역시 노이즈에 취약하며 아래에 설명할 커널들보다 검출 성능이 좋지 않다.\n\n\n\n프리윗 커널(Prewitt kernel)\n전방 차분과 후방 차분의 평균을 구하되, 차분을 구하는 방향에 대해 수직한 이웃 픽셀을 포함하여 구한다고 하자. 예를 들어\n\\[\n\\partial_x F[i, j] = \\dfrac{(F[i+1, j]- F[i, j]) + (F[i, j]- F[i, j-1])}{2}= \\dfrac{F[i+1, j]- F[i-1, j]}{2}\n\\]\n이다. 이것을 \\(j-1,\\,j,\\,j+1\\) 에 대해 모두 구하여 더한다고 하자. 여기서는 변하는 비율이 중요하기 때문에 가장 간단하게 하는 비례상수를 곱하면 아래의 식 식 15 의 \\(P_x\\) 가 나온다. \\(y\\) 방향 편미분은 \\(P_y\\) 이다. 이렇게 구성된 커널 \\(P_x,\\,P_y\\) 를 프레윗 커널(Prewitt kernel) 이라고 한다.\n\\[\nP_x = \\begin{bmatrix}+1&0&-1\\\\+1&0&-1\\\\+1&0&-1\\end{bmatrix}, \\qquad\nP_y = \\begin{bmatrix}+1&+1&+1\\\\0&0&0\\\\-1&-1&-1\\end{bmatrix}\n\\tag{15}\\]\n역시 이미지 \\(F\\) 에 대해 프리윗 커널로 변환시킨 이미지는 \\(G=\\sqrt{(P_x \\ast F)^2 + (P_y \\ast F)^2}\\) 이다.\n\n\n\n소벨 커널(Sobel kernel)\n프리윗 커널은 차분을 구하는 픽셀 과 그 주변 픽셀 2개의 차분을 더하지만 같은 중요도로 더한다. 여기에 변환시키는 픽셀에 가중치를 조금 더 준것이 소벨 커널이다. 다음을 보라.\n\\[\nS_x = \\begin{bmatrix}+1&0&-1\\\\+2&0&-2\\\\+1&0&-1\\end{bmatrix},\\qquad\nS_y = \\begin{bmatrix}+1&+2&+1\\\\0&0&0\\\\-1&-2&-1\\end{bmatrix}\n\\tag{16}\\]\n\n\n\n\n7.4 Laplaician filter\n1차 미분 필터들은 한 번만 차분 값을 계산하기 때문에, 차분의 크기도 작고, 이로 인해서 경계가 확실한 부분만 추출할 수 있는 반면, 노이즈가 있거나 엣지 (edge)의 강도가 약한 부분에는 불완전한 추출을 보이는 한계가 있다. 이를 극복하기 위해 개발된 필터가 2차 미분형, 즉, 라플라시안 필터 (Laplacian filter) 이다. 라플라시안 필터는 말 그대로, 연속 함수 \\(f(x,y)\\)의 이계 도함수를 의미하는 라플라시안으로부터 유래한 것으로, 디지털 이미지에 대해, 아래와 같은 근사 공식을 활용하여 만들 수 있다.\n라플라스 연산자 \\(\\nabla^2\\) 는 다음과 같이 정의된다.\n\\[\n\\nabla^2 F= \\partial_x^2 F + \\partial_y^2 F\n\\]\n우리는 앞서 전진 미분과 후진 미분에 대해 알아보았다. 이미지 처리에서 \\(\\partial^2\\) 는 전진 미분에서 후진미분을 뺀 것이다. 따라서,\n\\[\n\\begin{aligned}\n\\nabla^2 F &= (\\nabla_F F - \\nabla_B F)_x +  (\\nabla_F F - \\nabla_B F)_y \\\\[0.3em]\n&=(F[i+1, j] + F[i-1, j] - 2F[i, j]) + (F[i, j+1] + F[i, j-1]-2F[i, j]) \\\\[0.3em]\n&= F[i+1, j] + F[i-1, j] + F[i, j+1] + F[i, j-1] - 4F[i, j].\n\\end{aligned}\n\\]\n이다. 따라서 \\(3 \\times 3\\) 행렬로 커널을 구성할 수 있으며 라플라시안 커널 \\(L\\) 은 아래와 같다.\n\\[\nL = \\begin {bmatrix} 0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}\n\\tag{17}\\]\nimg0= img2mat(testimage(\"cameraman.tif\"));\nL=arr2mat(Int32[0 1 0 ; 1 -4 1 ;0  1 0])\ncv.filter2D(img0, -1, L)\n\n\n\n\n\n\n그림 15: 라플라시안 필터\n\n\n\n\n\n7.5 Laplacian of Gaussian Filter (LoG filter)\n원본 이미지에 가우시안 필터링 \\(G\\) 을 한 후 라플라시안 필터 \\(L\\) 를 적용하는 것을 Laplacian of gaussian filter, 즉 Log 필터 라고 한다. 합성곱의 associative 특성에 의해 \\(G\\ast (L \\ast F) = (G \\ast L)\\ast F\\) 이므로 \\(G \\ast L\\) 을 LoG 필터라고 부를 수 있다. 이 때 LoG filter 는 다음과 같은 꼴을 띈다.\n\\[\n(G \\ast L)[i, j] := \\dfrac{(c_x-j)^2+(c_y-i)^2-2\\sigma^2}{\\sigma^4} \\exp \\left(- \\dfrac{(c_x-j)^2+(c_y-i)^2}{2\\sigma^2}\\right)\n\\tag{18}\\]\n라플라시안 필터와 유사하게, LoG 필터도 이미지에서 엣지 부분의 정보를 효과적으로 추출할 수 있다. 특히, 라플라시안 필터보다 더 넓은 범위에서 이미지 합성 곱을 하기 때문에, 이미지 추출 과정에서 왜곡 가능성을 줄일 수 있다는 장점이 있다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 프로세싱의 기초"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_04.html",
    "href": "src/image_processing/image_processing_04.html",
    "title": "허프 변환 및 거리 변환 (Hough & Distance Transform)",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "허프 변환 및 거리 변환 (Hough & Distance Transform)"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_06.html",
    "href": "src/image_processing/image_processing_06.html",
    "title": "이미지 분할",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 분할"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_06.html#이미지-분할",
    "href": "src/image_processing/image_processing_06.html#이미지-분할",
    "title": "이미지 분할",
    "section": "1 이미지 분할",
    "text": "1 이미지 분할\n이미지의 어떤 영역 \\(R\\) 을 서로 겹치지 않으며 그 합집합이 영역 \\(R\\) 이 되는 부분집합 \\(R_1,\\ldots,\\,R_n\\) 으로 나누는 것. 즉, 1. \\(R_i \\ne \\varnothing\\) for all \\(i=1,\\ldots,\\,n\\) 2. \\(\\displaystyle \\bigcup_{i=1}^n = R\\) 3. 각각의 \\(R_i\\) 는 연결된 집합. 4. \\(i \\ne j \\implies R_i \\cap R_j = \\varnothing\\)",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 분할"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_06.html#이미지-이진화",
    "href": "src/image_processing/image_processing_06.html#이미지-이진화",
    "title": "이미지 분할",
    "section": "2 이미지 이진화",
    "text": "2 이미지 이진화",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 분할"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_06.html#오츠-방법으로-처리된-이진화-thresholding-by-otsus-algorithm",
    "href": "src/image_processing/image_processing_06.html#오츠-방법으로-처리된-이진화-thresholding-by-otsus-algorithm",
    "title": "이미지 분할",
    "section": "3 오츠 방법으로 처리된 이진화 (Thresholding by Otsu’s algorithm)",
    "text": "3 오츠 방법으로 처리된 이진화 (Thresholding by Otsu’s algorithm)",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 분할"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_06.html#이미지-영역-찾기",
    "href": "src/image_processing/image_processing_06.html#이미지-영역-찾기",
    "title": "이미지 분할",
    "section": "4 이미지 영역 찾기",
    "text": "4 이미지 영역 찾기",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 분할"
    ]
  },
  {
    "objectID": "src/image_processing/image_processing_06.html#미분기를-활용한-이미지-분할-방법",
    "href": "src/image_processing/image_processing_06.html#미분기를-활용한-이미지-분할-방법",
    "title": "이미지 분할",
    "section": "5 미분기를 활용한 이미지 분할 방법",
    "text": "5 미분기를 활용한 이미지 분할 방법",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "이미지 분할"
    ]
  },
  {
    "objectID": "src/image_processing/index.html",
    "href": "src/image_processing/index.html",
    "title": "영상 처리",
    "section": "",
    "text": "영상 처리\n\n참고자료\n\n\nGonzalez, R. C., and R. E. Woods. 2018. Digital Image Processing. Pearson. https://books.google.co.kr/books?id=0F05vgAACAAJ.\n\n\n권석준. 2021. “이미지 분석 & 처리 개론.” unpublished.",
    "crumbs": [
      "영상 처리/토모그래피",
      "영상 처리 기초",
      "영상 처리"
    ]
  },
  {
    "objectID": "src/image_processing/tomography.html",
    "href": "src/image_processing/tomography.html",
    "title": "토모그래피",
    "section": "",
    "text": "다음의 그림을 보자. 이미지가 \\(0\\) 에서 \\(1\\) 사이의 값을 가지며 검은색이 \\(0\\), 흰색이 \\(1\\) 이라고 하자. 이 그림은 실제로 회색조 \\(1024 \\times 1024\\) 크기의 이미지이다. 이 이미지는 f 라는 배열의 변수로 저장되었다고 하자. \\(f(x, y)\\) 라는 함수 나 f[i, j] 라는 형식의 배열로 이해해도 상관 없다.\n\n\n\n\n\n\n\n그림 1: object\n\n\n\n\n이 이미지를 \\(\\theta\\) 만큼 회전 시킨 이미지를 \\(\\mathfrak{R}_\\theta [f]\\) 라고 하자. 아래 그림은 \\(\\mathfrak{R}_{\\pi/4}[f]\\) 이다. 회전방향은 시계방향이다.\n\n\n\n\n\n\n\n그림 2: rotated object\n\n\n\n\n\n\n\n아래와 같이 정의된 \\(S(x,\\,\\theta)\\) 를 \\(f\\) 에 대한 sinogram 이라고 한다.\n\\[\nS(x, \\theta) =  \\int \\mathfrak{R}_\\theta [f](x, y) \\, dy\n\\]\n즉 sinogram 은 2 차원 이미지, 혹은 함수를 회전시켜가며 한 방향(여기서는 \\(y\\) 방향) 에 대한 선적분을 구하였을때 나오는 다른 방향과 회전각도에 대한 2차원 함수를 의미한다. 위의 그림에 대한 Sinogram 은 다음과 같다.\n\n\n\n\n\n\n\n그림 3: sinogram\n\n\n\n\n이 때 고정된 \\(\\theta\\) 에 대한 \\(S(x, \\theta)\\) 를 projection 이라고 하고 \\(p_\\theta (t)\\) 라고 표기한다. 즉 \\(p_\\theta(t) =  S(t, \\theta)\\) 이다. 이미지로부터 sinogram 을 얻는 것을 라돈 변환 (Randon transformation) 이라고 한다. Radon 은 오스트리아의 수학자 Johann Karl August Radon 을 의미한다.\n\n\n\n\nReconstruction 은 sinogram 으로부터 원래의 이미지를 구성하는 것을 말한다. Fourier slice theorem (projection slice theorem 혹은 central slice theorem) 은 수학적으로 simogram 으로부터 원래의 이미지를 구성할 수 있다는 것을 보장한다. 이미지로부터 sinogram 을 얻는 것을 라돈 변환이라고 하듯이 sinogram 으로부터 이미지를 얻는 것을 역 라돈 변환 (inverse Radon transformation) 이라고 한다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "주제별 처리",
      "토모그래피"
    ]
  },
  {
    "objectID": "src/image_processing/tomography.html#라돈-변환",
    "href": "src/image_processing/tomography.html#라돈-변환",
    "title": "토모그래피",
    "section": "",
    "text": "다음의 그림을 보자. 이미지가 \\(0\\) 에서 \\(1\\) 사이의 값을 가지며 검은색이 \\(0\\), 흰색이 \\(1\\) 이라고 하자. 이 그림은 실제로 회색조 \\(1024 \\times 1024\\) 크기의 이미지이다. 이 이미지는 f 라는 배열의 변수로 저장되었다고 하자. \\(f(x, y)\\) 라는 함수 나 f[i, j] 라는 형식의 배열로 이해해도 상관 없다.\n\n\n\n\n\n\n\n그림 1: object\n\n\n\n\n이 이미지를 \\(\\theta\\) 만큼 회전 시킨 이미지를 \\(\\mathfrak{R}_\\theta [f]\\) 라고 하자. 아래 그림은 \\(\\mathfrak{R}_{\\pi/4}[f]\\) 이다. 회전방향은 시계방향이다.\n\n\n\n\n\n\n\n그림 2: rotated object\n\n\n\n\n\n\n\n아래와 같이 정의된 \\(S(x,\\,\\theta)\\) 를 \\(f\\) 에 대한 sinogram 이라고 한다.\n\\[\nS(x, \\theta) =  \\int \\mathfrak{R}_\\theta [f](x, y) \\, dy\n\\]\n즉 sinogram 은 2 차원 이미지, 혹은 함수를 회전시켜가며 한 방향(여기서는 \\(y\\) 방향) 에 대한 선적분을 구하였을때 나오는 다른 방향과 회전각도에 대한 2차원 함수를 의미한다. 위의 그림에 대한 Sinogram 은 다음과 같다.\n\n\n\n\n\n\n\n그림 3: sinogram\n\n\n\n\n이 때 고정된 \\(\\theta\\) 에 대한 \\(S(x, \\theta)\\) 를 projection 이라고 하고 \\(p_\\theta (t)\\) 라고 표기한다. 즉 \\(p_\\theta(t) =  S(t, \\theta)\\) 이다. 이미지로부터 sinogram 을 얻는 것을 라돈 변환 (Randon transformation) 이라고 한다. Radon 은 오스트리아의 수학자 Johann Karl August Radon 을 의미한다.\n\n\n\n\nReconstruction 은 sinogram 으로부터 원래의 이미지를 구성하는 것을 말한다. Fourier slice theorem (projection slice theorem 혹은 central slice theorem) 은 수학적으로 simogram 으로부터 원래의 이미지를 구성할 수 있다는 것을 보장한다. 이미지로부터 sinogram 을 얻는 것을 라돈 변환이라고 하듯이 sinogram 으로부터 이미지를 얻는 것을 역 라돈 변환 (inverse Radon transformation) 이라고 한다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "주제별 처리",
      "토모그래피"
    ]
  },
  {
    "objectID": "src/image_processing/tomography.html#토모그래피",
    "href": "src/image_processing/tomography.html#토모그래피",
    "title": "토모그래피",
    "section": "2 토모그래피",
    "text": "2 토모그래피\n토모그래피의 기본적인 개념은 아래 그림과 같다. 중성자나 X 선과 같이 물질을 투과하는 성질이 강한 입자를 물질에 쐬어 주면 대부분의 입자가 물질을 투과하여 검출기에 검출된다. 하지만 모든 입자가 투과하지는 못하며 물질의 성질에 따라 일부 입자가 산란되거나 흡수되며 검출기에서는 산란되거나 흡수되지 않고 투과되는 빔의 강도를 검출기 위치별로 측정한다. 검출기는 1차원일수도 있고 2차원 일 수도 있다.\n\n\n\n\n\n\n\n그림 4: Tomography\n\n\n\n\n비에 따라 (1) 검출기와 선원이 고정된 상태에서 측정 대상이 회전중심을 기준으로 회전할 수도 있고, (2) 측정 대상이 고정된 상태에서 검출기와 선원(radiation source) 이 회전할 수도 있다. 우리는 앞서 라돈 변환을 설명할 때 (1) 의 경우에 대대 설명했지만, 수학적인 설명은 (2) 의 경우를 기준으로 설명하겠다. (이 분야의 대표적인 교과서들이 (2) 를 기준으로 설명하였으므로…) 물리적으로는 (1) 과 (2) 는 서로 회전 방향이 반대라면 동일하다. 즉 (1) 의 경우 시계방향으로 \\(\\theta\\) 만큼의 회전은 (2) 의 경우에서 반시계방향으로 \\(\\theta\\) 만큼의 회전과 완전히 동일하다.\n물질의 위치에 따른 선형 감쇄 계수가 \\(\\mu (x,\\,y)\\) 로 주어졌다고 하자. 감쇄 계수는 \\(I_0\\) 의 강도로 입사된 빔이 \\(ds\\) 만큼의 경로를 진행했을 때 감소되는 빔의 강도이다. 즉,\n\\[\n\\mu = -\\dfrac{1}{I}\\dfrac{dI}{ds}\n\\]\n이다. 선원에서 \\(I_0\\) 의 빔의 강도를 가진 빔이 \\(S\\) 의 경로를 따라 갔을 때의 빔의 강도는\n\\[\nI = I_0 \\exp \\left[-\\int_S \\mu (x, y)\\, ds\\right]\n\\tag{1}\\]\n가 된다. 중성자나 X-선의 경우 공기중의 선형 감쇠 계수는 매우 작기 때문에 우리가 관심이 있는 영역에서의 경로적분만을 생각하면 된다. 또한 빔이 \\(x\\) 값이 고정된 직선 경로를 따른다면,\n\\[\nI(x) = I_0 \\exp \\left[-\\int_{y_i}^{y_f} \\mu (x, y)\\, dy\\right]\n\\tag{2}\\]\n이다. 만약 물질이 없는 경로에서의 측정값이 있다면 \\(I(x) = I_0\\) 가 될 것이다.\n우리가 검출기에서 측정한 값이 \\(\\overline{I}(x)\\) 이며 물질이 없는 경로에서의 측정값 \\(\\overline{I_0}\\) 을 가지고 있다면,\n\\[\n-\\ln \\left(\\dfrac{\\overline{I}(x)}{\\overline{I_0}}\\right) = \\int_{y_i}^{y_f} \\mu (x, y)\\, dy\n\\tag{3}\\]\n가 될 것이다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "주제별 처리",
      "토모그래피"
    ]
  },
  {
    "objectID": "src/image_processing/tomography.html#projection",
    "href": "src/image_processing/tomography.html#projection",
    "title": "토모그래피",
    "section": "3 Projection",
    "text": "3 Projection\n토모그래피에 있어 물질은 \\(\\mu (x, y)\\) 를 의미한다. \\(\\mu\\) 는 선원, 물질의 종류 및 밀도에 따라 달라지는 값이며, 토모그래피는 projection들 을 모아 \\(\\mu(x, y)\\) (3차원 토모그래피의 경우는 \\(\\mu(x, y, z)\\)) 를 재구성(reconstruction) 하는 방법이다.\n이제 선원과 검출기를 반시계방향으로 \\(\\theta\\) 만큼 돌려서 찍었다고 하자. 이 데이터를 \\(I_\\theta (x)\\) 라고 한다. 이것은 앞서 라돈 변환에서의 projection \\(p_\\theta(t)\\) 와 같다.\n\n\n\n\n\n\n\n그림 5: Projection\n\n\n\n\n 이제 라돈 변환을 잠시 다른 모양의 동등한 식으로 바꿔 보자.\n\\[\np_\\theta(t) = \\int_{-\\infty}^{\\infty} \\mu(x,\\,y)\\, \\delta (x\\cos \\theta + y \\sin \\theta -t )\\, dx\\,dy\n\\tag{4}\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "주제별 처리",
      "토모그래피"
    ]
  },
  {
    "objectID": "src/image_processing/tomography.html#fourier-slice-theorem",
    "href": "src/image_processing/tomography.html#fourier-slice-theorem",
    "title": "토모그래피",
    "section": "4 Fourier Slice Theorem",
    "text": "4 Fourier Slice Theorem\n\n4.1 푸리에 변환과 역변환\n이차원 함수 \\(f(x, y)\\) 을 2차원 푸리에변환 한 함수 \\(\\mathfrak{F}[f](u, v) = F(u, v)\\) 는 다음과 같다.\n\\[\nF(u, v) = \\mathfrak{F}[f](u, v) = \\iint f(x, y) \\, e^{-2\\pi i ( ux+yv)}\\, dx\\, dy\n\\]\n우리는 \\(F(u, v)\\) 에 대한 푸리에 역변환으로부터 \\(f(x, y) = \\mathfrak{F}^{-1}[F](x, y)\\) 를 얻을 수 있다는 것을 알고 있다. 즉,\n\\[\nf(x, y) = \\mathfrak{F}^{-1}[F](x, y) = \\iint F(u, v) \\, e^{2\\pi i (ux +yv)}\\, du\\, dv\n\\]\n임을 안다. 즉 \\(f(x, y)\\) 를 안다는 것과 \\(F(u, v)\\) 를 안다는 것은 정확히 같은 것이다.\n\n\n\n4.2 Projection 과 푸리에변환\nProjection \\(p_\\theta (s)\\) 에 대한 1차원 푸리에 변환을 \\(S_\\theta(\\omega)\\) 라 하면,\n\\[\nS_\\theta(\\rho) = \\int p_\\theta(t) \\,e^{-2\\pi i\\rho t}\\, dt\n\\tag{5}\\]\n이다.\n아래의 그림처럼 \\(\\theta\\) 만큼 회전시킨 \\(t-s\\) 좌표계를 생각하자. 그렇다면, \\((x, y)\\) 와 \\((t, x)\\) 의 관계는 아래와 같다.\n\\[\n\\begin{bmatrix} t \\\\ s \\end{bmatrix} =  \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\ - \\sin \\theta & \\cos \\theta\\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n그림 6: New coordinate system\n\n\n\n\n라돈 변환을 생각하면,\n\\[\np_\\theta (t) = \\int f(t, s)\\, ds\n\\]\n이며 식 5 로 부터\n\\[\n\\begin{aligned}\nS_\\theta (\\rho) &= \\int p_\\theta (t) e^{-2\\pi i \\rho t}\\, dt \\\\\n&=\\int \\left[\\int f(t, s) \\, ds\\right] \\, e^{-2\\pi i \\rho t}\\, dt \\\\\n&= \\iint f(x, y) e^{-2 \\pi i \\rho ( x \\cos\\theta + y\\sin \\theta) } \\, dx\\, dy\n\\end{aligned}\n\\tag{6}\\]\n임을 안다. 여기서 \\(u = \\rho \\cos \\theta,\\, v = \\rho \\sin \\theta\\) 라고 하면,\n\\[\nS_\\theta (\\rho) = \\iint f(x,  y) e^{-2\\pi i (ux + vy)}\\, dx\\, dy = \\mathfrak{F}[f](u, v) = F(\\rho \\cos \\theta, \\rho \\sin \\theta)\n\\tag{7}\\]\n이다. 이것이 바로 Fourier Slice Theorem 이다. Fourier slice theorem 은 sinogram 으로부터 \\(f(x, y)\\) 를 구할 수 있음을, 즉 역 라돈 변환이 가능함을 보장한다.\n\n\n\n\n\n\n\n그림 7: New coordinate system",
    "crumbs": [
      "영상 처리/토모그래피",
      "주제별 처리",
      "토모그래피"
    ]
  },
  {
    "objectID": "src/image_processing/tomography.html#filtered-back-projeciton",
    "href": "src/image_processing/tomography.html#filtered-back-projeciton",
    "title": "토모그래피",
    "section": "5 Filtered back projeciton",
    "text": "5 Filtered back projeciton\n\n\n\n\n\n\n지금까지의 정리\n\n\n\n\n\\(f(x, y)\\) 가 함수라고 하자. 그렇다면 우리는 임의의 projeciton \\(p_\\theta(t)\\) 를 얻을 수 있으며, 이를 통해 \\(\\mathfrak{F}[f](u, v)\\) 를 구할 수 있다.\n우리가 토모그래피를 통해 측정하는 것은 \\(\\theta = \\theta_1,\\, \\theta_2,\\,\\theta_3, \\ldots\\) 에 대한 \\(p_\\theta (t)\\) 이다. 그리고 푸리에 변환을 통해 \\(S_\\theta (\\omega)\\) 를 얻을 수 있다.\n\\(S_\\theta (\\omega) = F(\\omega \\cos\\theta, \\omega \\sin \\theta)\\) 와 같다.\n\n\n\n\n\n5.1 Filtered projection\n\\[\n\\begin{aligned}\nf(x, y) &= \\iint F(u, v) e^{2\\pi i (ux + yv)}\\, du\\, dv \\\\\n&= \\int_0^{2\\pi}\\int_0^{\\infty} F(\\rho \\cos \\theta, \\rho \\sin \\theta)\\, e^{2\\pi i (\\rho x \\cos \\theta + \\rho y \\sin \\theta)} \\, \\rho\\,   d\\rho \\, d\\theta\n\\end{aligned}\n\\tag{8}\\]\n이며 \\(\\theta\\) 에 대한 적분을 \\([0, \\pi)\\), \\([\\pi, 2\\pi)\\) 구간으로 나누어 생각해 보자.\n\nProjection \\(p_\\theta (t)\\) 를 생각해보자. \\(\\theta + \\pi\\) 만큼 회전시킨 projection 은 \\(\\theta\\) 에 대한 projection 과 같은 선상의 적분이며 단지 \\(t\\) 값이 \\(-t\\) 로 바뀔 뿐이다. 따라서 \\(p_{\\theta + \\pi} (t) = p_\\theta(-t)\\) 이다.\n식 (\\(6\\)) 으로부터, \\(S_{\\theta + \\pi}(\\rho) = S_\\theta (-\\rho)\\) 임을 안다. 따라서 다음이 성립한다. \\[\nF(\\rho \\cos (\\theta + \\pi), \\rho \\sin (\\theta + \\pi)) = F(-\\rho \\cos \\theta, -\\rho \\sin\\theta)\n\\tag{9}\\]\n\n\n그렇다면, \\([\\pi, 2\\pi)\\) 구간에서 \\(\\phi = \\theta - \\pi\\) 로 놓으면, 그리고 중간에 \\(\\rho \\to -\\rho\\) 변환을 사용하면,\n\\[\n\\begin{aligned}\n\\int_\\pi^{2\\pi}\\int_0^\\infty &F(\\rho \\cos \\theta, \\rho \\sin \\theta)\\,  e^{2\\pi i (\\rho x \\cos \\theta + \\rho y \\sin \\theta)} \\, \\rho\\,   d\\rho \\, d\\theta\\\\\n&=\\int_0^\\pi \\int_0^\\infty F(-\\rho\\cos \\phi, -\\rho \\sin \\phi) e^{-2\\pi i(\\rho x \\cos \\phi +\\rho y \\sin\\phi)} \\, \\rho\\, d\\rho \\, d\\phi \\\\\n& = \\int_0^\\pi \\int_{-\\infty}^0 F(\\rho \\cos \\phi,  \\rho \\sin \\phi) e^{2\\pi i (\\rho x \\cos \\theta + \\rho y \\sin \\theta)} (-\\rho)\\, d\\rho \\,d\\phi\n\\end{aligned}\n\\tag{10}\\]\n이다. 식 7 과 식 10 로부터, 그리고 Fourier slice theorem 으로부터,\n\\[\n\\begin{aligned}\nf(x, y) &=\\int_0^{ \\pi} \\int_{-\\infty}^\\infty F(\\rho \\cos\\theta, \\rho \\sin \\theta) e^{2\\pi i \\rho(x \\cos \\theta + y \\sin \\theta)}|\\rho| \\, d\\rho \\, d\\theta \\\\\n&= \\int_0^\\pi \\left[ \\int_{0}^\\infty S_\\theta (\\rho) |\\rho| e^{2\\pi i \\rho (x\\cos \\theta + y \\sin \\theta)}\\, d\\rho \\right] \\, d\\theta\n\\end{aligned}\n\\tag{11}\\]\n를 얻는다. 우리는 \\(t=x \\cos \\theta + y \\sin \\theta\\) 가 \\(\\theta\\) 만큼 회전시켰을때의 projection 이 독립변수임을(혹은 좌표값임을) 알고 있다. 따라서\n\\[\nQ_\\theta (t) = \\int_{0}^\\infty S_\\theta(\\rho) |\\rho| e^{2\\pi i \\rho t} \\, d\\rho\n\\tag{12}\\]\n라고 정의하자. 그렇다면\n\\[\nf(x, y) = \\int_0 ^\\pi  Q_\\theta (t = x \\cos \\theta + y \\sin \\theta) \\,d\\theta\n\\tag{13}\\]\n가 된다. 이 때의 \\(Q_\\theta (t)\\) 를 filtered projection 이라고 하며 filtered projection 을 통해 \\(f(x, y)\\) 를 얻는 과정을 filtered back projection 이라고 한다. 즉 filtered back projection 은 다음의 과정을 통해 원본 이미지를 reconstructin 한다.\n\\[\np_\\theta (t) \\longrightarrow S_\\theta (\\rho) \\longrightarrow Q_\\theta(t) \\longrightarrow f(x, y)\n\\]\n우리가 image 를 reconstruction 할 때는 영역을 선택한다. 이를 reconstruction region 이라고 하자. 이 영역에 대해 \\(f(x,\\,y)\\) 를 구해야 한다. 다음 그림을 보자\n\n\n\n\n\n\n\n그림 8: Filtered projection\n\n\n\n\n우리는 \\(\\theta = \\theta_1,\\,\\theta_2,\\ldots\\) 에 대한 slices \\(p_\\theta (t)\\) 를 얻었으며 이를 통해 각 slices 에 대한 \\(Q_\\theta (t)\\) 를 얻을 수 있다. \\(Q_\\theta(t)\\) 를 얻었다면 식 (\\(11\\)) 로 부터 \\(f(x,\\,y)\\) 를 얻을 수 있는데, 실제로 reconstruction 을 할 때는 적분을 합으로 바꾸게 된다. 즉,\n\\[\nf(x, y) = C \\cdot \\left(\\sum_\\theta Q_\\theta (t=x \\cos \\theta + y \\sin \\theta)\\right)\n\\]\n이다.\n\n\n\n5.2 Low frequency window\n식 12 의 \\(|\\rho|\\) 는 high-pass filter 역할을 하며(이를 Lamp 필터 혹은 Ram-Lack 필터 라고 한다) 이로인해 high frequency 영역에서의 노이즈에 매우 취약하게 된다. 이로인해 \\(Q_\\theta (t)\\) 를 계산할 때 window function 이라고 불리우는 high pass filter 를 곱하여 푸리에 변환을 수행한다. 각 필터의 모양은 아래 그림 9 와 같다.\n\n\n\n\n\n\n그림 9: Window functions",
    "crumbs": [
      "영상 처리/토모그래피",
      "주제별 처리",
      "토모그래피"
    ]
  },
  {
    "objectID": "src/image_processing/tomography.html#대수적인-reconstruction",
    "href": "src/image_processing/tomography.html#대수적인-reconstruction",
    "title": "토모그래피",
    "section": "6 대수적인 Reconstruction",
    "text": "6 대수적인 Reconstruction\n\n\n\n\n\n\n그림 10: 대수적인 reconstruction\n\n\n\n토모그래피를 수행하는 전체 영역이 \\(N_1 \\times N_2\\) 의 그리드라고 하자. 우리는 이 때 그리드 내의 선형 감쇄 계수 \\(\\mu(x,\\,y)\\) 는 동일하다고 가정한다. 그리드마다 인덱스를 부여하여 \\(\\mu_j\\) \\((j=1,\\,2,\\ldots,\\,N_1 \\times N_2)\\) 라고 하자. 중성자나 X 선원에서 발생하는 방사선이 물질을 투과하여 그 결과로 감소하여 검출기 픽셀 \\(i\\) 번째 pixel 에 검출되었다고 하자. 식 3 에 따라 \\(i\\) 번째 pixel 에 검출된 카운트 수는\n\\[\n-\\ln \\left(\\dfrac{I_i}{I_0}\\right) = \\sum_{j} w_{ij} \\mu_j\n\\]\n가 된다. \\(d_i = -\\ln \\left(\\dfrac{I_i}{I_0}\\right)\\) 라고 하면, \\(d_i = \\sum_{j} w_{ij} \\mu_j\\) 이다. 토모그래피에서 slice 를 \\(\\theta = \\theta_1,\\ldots,\\,\\theta_m\\) 만큼 얻었다고 하자. 이제 인덱스 \\(i\\) 를 검출기 뿐만 아니라 slice 에 대한 인덱스로 하자. 즉 검출기 픽셀이 \\(N_d\\) 일 때 \\(d_1,\\ldots,\\,d_{N_d}\\) 는 \\(\\theta_1\\) slice 에 대한 검출기 픽셀 값을 처리한(\\(-\\ln (I_i/I_0)\\)) 값이고 \\(d_{N_d+1},\\ldots,\\, d_{N_d\\times 2}\\) 는 \\(\\theta_2\\) 에 대한 값이라고 하자. 그렇다면\n\\[\n\\boldsymbol{d} = \\begin{bmatrix} d_1 & d_2 & \\cdots & d_{N_d\\times m}\\end{bmatrix}^T\n\\]\n를 정의 할 수 있다. 또한 \\(w_{ij}\\) 의 index \\(i\\) 도 검출기 픽셀과 slice 회전에 대한 인덱스에 대해 \\(\\mu_j\\) 가 \\(d_j\\) 에 기여하는 정도 라고 볼 수 있다. 그렇다면 우리는 아래와 같은 선형방정식을 얻는다.\n\\[\n\\boldsymbol{\\Omega \\mu} = \\boldsymbol{d}.\n\\tag{14}\\]\n즉 reconstruction 문제가 선형방정식 문제가 된다. 이 때 \\(\\omega\\) 는 [검출기 픽셀 갯수 \\(\\times\\) slice 수] 만큼의 행과 [Recustruct 되는 이미지의 그리드 수] 만큼의 열을 갖는 행렬이다. 보통 이미지의 그리드 수는 검출기 픽셀 수의 제곱이며 검출기 픽셀 수는 수백 ~ 수천이며 , slice 수 역시 수백 ~ 수천 정도 이므로, \\(\\boldsymbol{\\Omega}\\) 는 수만 \\(\\times\\) 수만 에서 수백만 \\(\\times\\) 수백만 정도의 크기를 갖는 매우 큰 행렬이다. 물론 대부분의 성분이 \\(0\\) 인 희소행력(spase matrix) 이다.\n식 14 을 보자. 이것이 유일한 해를 갖는 경우는 \\(\\boldsymbol{A}\\) 가 정사각 행렬이며 \\(\\det (\\boldsymbol{A}) \\neq 0\\) 일 때 뿐이다. 그러나 일반적인 경우에 \\(\\boldsymbol{A}\\) 는 정사각 행렬이 아니다. 이 경우 일반화된 역행렬 방법이 사용된다.\n\n\n6.1 ART (Algebraic reconstruction technique)\n\\[\n\\boldsymbol{x}^{(k+1)} = \\boldsymbol{x}^{(k)} + \\lambda_m \\dfrac{1}{\\boldsymbol{A}_{1:}^T \\boldsymbol{A}_{1:}} (\\boldsymbol{b}_i - \\boldsymbol{A}_{i:}\\boldsymbol{x}^{(k)}) \\boldsymbol{A}_{i:}\n\\]\nㅋ \n\n\n6.2 SART (Simultaneous ART)\n\\[\n\\boldsymbol{x}^{(k+1)} = \\boldsymbol{x}^{(k)} + \\lambda_m \\boldsymbol{V}^{-1} \\boldsymbol{A}^T \\boldsymbol{W} (\\boldsymbol{b}-\\boldsymbol{Ax}^{(k)})\n\\]\n이 때 \\(\\boldsymbol{V}\\) 와 \\(\\boldsymbol{W}\\) 는 대각행렬로 각각의 대각성분은 다음과 같다.\n\\[\n\\boldsymbol{V}_{jj} = \\sum_{i=1}^M |\\boldsymbol{A}_{ij}|, \\qquad \\boldsymbol{W}_{ii} = \\dfrac{1}{\\sum_{j=1}^N |\\boldsymbol{A}_{ij}|}\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "주제별 처리",
      "토모그래피"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/fourier_transform.html",
    "href": "src/image_processing/wavelet/fourier_transform.html",
    "title": "푸리에 변환",
    "section": "",
    "text": "여기의 상당수 내용은 일반적인 학부 2학년 수준의 선형대수학, 해석학 범위를 넘어선 지식을 요구한다. 이런 내용은 증명하지 않고 언급하고 넘어가거나 사용할 수 있다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "푸리에 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/fourier_transform.html#l1mathbbr-에서의-푸리에-변환",
    "href": "src/image_processing/wavelet/fourier_transform.html#l1mathbbr-에서의-푸리에-변환",
    "title": "푸리에 변환",
    "section": "1 \\(L^1(\\mathbb{R})\\) 에서의 푸리에 변환",
    "text": "1 \\(L^1(\\mathbb{R})\\) 에서의 푸리에 변환\n\n1.1 \\(L^p\\) 공간 과 \\(L^1\\) 공간에서의 푸리에 변환\n\n\n\n\n\n\n\n정의 1 (\\(L^p\\)-노름 과 \\(L^p\\) 공간) \\(f:\\mathbb{R} \\to \\mathbb{C}\\) 와 \\(1\\le p &lt; \\infty\\) 인 \\(p\\) 에 대해\n\\[\n\\|f\\|_p = \\left[\\int_{-\\infty}^\\infty |f(t)|^p\\, dt\\,\\right]^{1/p} &lt;\\infty\n\\]\n를 \\(L^p\\)-노름 이라고 한다. \\(L^p\\) 노름이 정의된 함수의 집합을 \\(L^p\\)-공간 이라고 하며 \\(L^p\\) 혹은 \\(L^p(\\mathbb{R})\\) 로 쓴다.\n\n\n\n\n행렬과 벡터의 노름에서처럼 \\(p=1,\\,2,\\infty\\) 일 때 가장 중요하다. 또한 \\(L^p\\) 노름이 정의되는 함수의 집합은 벡터공간이다. \\(p=1\\) 일 때 \\(L^1\\) 노름이 정의되는 함수들을 Lebesgue 적분 가능 함수 라고 한다. 이제 \\(f\\in L^1(\\mathbb{R})\\) 이라고 하자. \\(e^{-i\\omega t}\\) 는 유계인 연속 함수이므로 \\(e^{-i \\omega t}f(t)\\) 는 모든 \\(\\omega \\in \\mathbb{R}\\) 에서 국소적으로 적분 가능한 함수(locally integrable function) 이다. 이 경우 아래와 같이 \\(L^1\\) 공간에서의 푸리에 변환을 정의 할 수 있다. \\(L^\\infty(\\mathbb{R})\\) 는 measure zero 인 집합을 제외하면 유계인 함수의 집합이다.\n\n\n\n\n\n\n\n\n정의 2 (\\(L^1(\\mathbb{R})\\) 에서의 푸리에 변환) \\(f\\in L^1(\\mathbb{R})\\) 에 대한 푸리에 변환 \\(\\hat{f}(\\omega)\\) 를 다음과 같이 정의 할 수 있다.\n\\[\n\\hat{f}(\\omega)= \\mathfrak{F}[f(t)] = \\int_{-\\infty}^\\infty e^{-i\\omega t}f(t)\\, dt.\n\\tag{1}\\]\n\n\n\n\n\n원래 함수 \\(f(t)\\) 는 실함수이더라도 \\(\\hat{f}(\\omega)\\) 는 실변수 \\(\\omega\\) 에 대한 복소함수이다. 복소수를 극좌표계에서 \\(x+iy=re^{i\\theta}\\), \\(r=\\sqrt{x^2+y^2}\\), \\(\\theta = \\arctan(y/x)\\) 로 표현하는 것처럼 \\(R(\\omega) = \\text{Re}[\\hat{f}(\\omega)]\\) ,\\(X(\\omega)= \\text{Im}[\\hat{f}(\\omega)]\\) 에 대해\n\\[\n\\hat{f}(\\omega)= R(\\omega)+ iX(\\omega)= A(\\omega) e^{i\\theta(\\omega)}\n\\]\n로 표현할 수 있으며 이 때 \\(A(\\omega)=\\left|\\hat{f}(\\omega)\\right|\\) 를 amplitude spectrum, \\(\\theta(\\omega)\\) 을 phase spectrum 이라고 한다.\n\n\n\n예제 1 (가우시안 함수의 푸리에 변환) 가우시안 함수 \\(f(t) = e^{-t^2/2\\sigma^2}\\) 에 대한 푸리에 변환 역시 가우시안이다.\n\\[\n\\begin{aligned}\n\\hat{f}(\\omega) = \\int_{-\\infty}^\\infty e^{-(i\\omega t + t^2/2\\sigma^2)}\\,dt = \\int_{-\\infty}^\\infty \\exp\\left(-\\dfrac{1}{2\\sigma^2}(t+i\\sigma^2\\omega)^2-\\dfrac{\\omega^2\\sigma^2}{2}\\right) = \\sqrt{2\\pi}\\sigma e^{-\\sigma^2 \\omega^2/2}\n\\end{aligned}\n\\]\n푸리에 변환에 의해 원래 함수의 표준편차 \\(\\sigma\\) 가 \\(1/\\sigma\\) 로 변화한다.\n\n\n\n\n\n예제 2 (직사각 함수의 푸리에 변환) \\[\nf(t) = \\left\\{\\begin{array}{ll} 1, \\qquad & |t|&lt; a \\\\ 0&\\text{otherwise}\\end{array}\\right. .\n\\]\n의 푸리에 변환은 다음과 같다.\n\\[\n\\begin{aligned}\n\\hat{f}(\\omega) = \\int_{-a}^a e^{-i\\omega t}\\, dt= \\left(\\dfrac{2}{\\omega}\\right) \\sin (a\\omega) = 2a\\,  \\text{sinc} (a\\omega)\n\\end{aligned}\n\\]\n이 때 \\(\\textrm{sinc}(x) := \\dfrac{\\sin x}{x}\\) 로 정의되는 함수이다. 기본적인 신호 를 참고하라. 실제로 \\(\\operatorname{sinc}\\) 는 \\(L^1(\\mathbb{R})\\) 함수가 아니다. 즉 \\(f\\in L^1(\\mathbb{R})\\) 이더라도 \\(\\hat{f}\\not\\in L^1(\\mathbb{R})\\) 일 수 있다.\n\n\n\n\n\n예제 3 기본적인 신호 함수들 의 Heaviside 계단함수 \\(\\text{step}(x)\\) 가 사용된 아래와 같은 함수를 생각하자.\n\\[\nf(t) = \\left(1-\\dfrac{|t|}{a}\\right) \\text{step}\\left( 1-\\dfrac{|t|}{a}\\right)\n\\]\n이 함수의 푸리에 변환은 다음과 같다.\n\\[\n\\hat{f}(\\omega)= a \\cdot \\dfrac{\\sin^2 \\left(\\dfrac{a\\omega}{2}\\right)}{\\left(\\dfrac{a\\omega}{2}\\right)^2}\n\\]\n\n\n\n함수의 푸리에 변환\n\n\n\n\n\n\n\n예제 4 \\(f(t) = e^{-a|t|},\\, (a&gt;0)\\) 의 푸리에 변환은 다음과 같다.\n\\[\n\\begin{aligned}\n\\hat{f}(\\omega) = \\mathfrak{F}\\left[ e^{-a|t|} \\right] &= \\int_{-\\infty}^0 e^{(a-i\\omega)t}\\, dt + \\int_0^\\infty e^{-(a+i\\omega)t}\\, dt = \\dfrac{2a}{\\omega^2+a^2}\n\\end{aligned}\n\\]\n\n\n\n\\(e^{-a|t|}\\) 함수의 푸리에 변환\n\n\n\n\n\n\n\n1.2 푸리에 변환의 수학적 성질\n\n\n\n\n\n\n\n정의 3 (연산자들) 앞으로 많이 사용될 연산자들을 정의하기로 한다.\n  Translation : \\(T_a f(t) := f(t-a)\\),\n  Modulation : \\(M_bf(t) := e^{ibt}f(t)\\),\n  Dilation : \\(D_c f(t) := \\dfrac{1}{\\sqrt{|c|}}f\\left(\\dfrac{1}{c}\\right)\\).\n  Parity : \\(Pf(t) := f(-t)\\).\n\n\n\n\n다음은 푸리에 변환의 정의에 따라 쉽게 증명 할 수 있다.\n\n명제 1 (푸리에 변환의 수학적 성질) \\(f(t),\\,g(t)\\in L^1(\\mathbb{R})\\), \\(a,\\,b\\in \\mathbb{C}\\) 이고 \\(\\hat{f}(\\omega) = \\mathfrak{F}[f(t)]\\), \\(\\hat{g}(t) = \\mathfrak{F}[g(t)]\\) 일 때 다음이 성립한다.\n  (\\(1\\)) (Linearity) \\(\\mathfrak{F}[af(t)+bg(t)] = a \\mathfrak{F}[f(t)] + b\\mathfrak{F}[g(t)]\\).\n  (\\(2\\)) (Shifting) \\(\\mathfrak{F}[T_a f(t)] = \\mathfrak{F}[f(t-a)] = e^{-ia\\omega}\\hat{f}(\\omega)\\).\n  (\\(3\\)) (Scaling) \\(\\mathfrak{F}\\left[D_{1/a} f(t)\\right] = \\mathfrak{F}[\\sqrt{|a|}f(at)] =  \\dfrac{1}{\\sqrt{|a|}}\\hat{f}\\left(\\dfrac{\\omega}{a} \\right) = D_a \\hat{f}(\\omega)\\).\n  (\\(4\\)) (Conjugation) \\(\\mathfrak{F}[\\overline{D_{-1}f(t)}] = \\mathfrak{F}[\\overline{f(-t)}] = \\overline{\\hat{f}(\\omega)}\\).\n  (\\(5\\)) (Modulation) \\(\\mathfrak{F}[M_b f(t) = \\mathfrak{F}[e^{ibt}f(t)] = T_b \\hat{f}(\\omega) = \\hat{f}(\\omega - b)\\).\n  (\\(6\\)) (Continuity) \\(\\hat{f}(\\omega)\\) 는 연속이다.\n  (\\(7\\)) (Differentiation) \\(\\dfrac{d^n \\hat{f}(\\omega)}{d\\omega^n} = (-i)^n \\mathfrak{F}\\left[t^n f(t)\\right]\\)\n\n\n(증명). (\\(7\\)) 만 수학적 귀납법으로 증명한다. 우선 \\(n=1\\) 경우.\n\\[\n\\begin{aligned}\n\\dfrac{d\\hat{f}}{d\\omega} &= \\lim_{h \\to 0}\\dfrac{\\hat{f}(\\omega + h) - \\hat{f}(\\omega)}{h} = \\lim_{h \\to 0} \\int_{-\\infty}^\\infty f(t) e^{-i\\omega t}\\left(\\dfrac{e^{-iht}-1}{h}\\right)\\, dt \\\\\n&= \\int_{-\\infty}^\\infty (-it)f(t)e{-i\\omega t}\\, dt = -i \\mathfrak{F}[tf(t)].\n\\end{aligned}\n\\]\n이제 \\(n\\) 에 대해 성립함을 가정하자.\n\\[\n\\begin{aligned}\n\\dfrac{d^{n+1}\\hat{f}}{d\\omega^{n+1}}&= \\dfrac{d}{d\\omega}\\left((-i)^n\\mathfrak{F}[t^nf(t)]\\right) = (-i)^n \\dfrac{d}{d\\omega}\\mathfrak{F}[t^n f(t)] = (-i)^{n+1}\\mathfrak{F}[t^{n+1}f(t)]\n\\end{aligned}\n\\]\n이다. \\(\\square\\)\n\n\n\n정리 1 (리만-르벡 따름정리) \\(f\\in L^1(\\mathbb{R})\\) 일 때\n\\[\n\\lim_{|\\omega|\\to \\infty} \\left|\\hat{f}(\\omega)\\right| = 0\n\\]\n이다.\n\n\n(증명). \\(e^{-i\\omega t} = - e^{-i\\omega (t+\\pi/\\omega)}\\) 를 이용하면,\n\\[\n\\begin{aligned}\n\\hat{f}(\\omega) & = - \\int_{-\\infty}^\\infty e^{-i\\omega (t+\\pi/\\omega)} f(t) = - \\int_{-\\infty}^\\infty e^{-i \\omega x} f\\left(x-\\dfrac{\\pi}{\\omega}\\right)\\, dx \\\\\n\\end{aligned}\n\\]\n이다. 따라서,\n\\[\n\\begin{aligned}\n\\hat{f}(\\omega) &=  \\dfrac{1}{2}\\left[\\int_{-\\infty}^\\infty e^{-i\\omega t} f(t) \\, dt - \\int_{-\\infty}^\\infty e^{-i\\omega t} f\\left(t- \\dfrac{\\pi}{\\omega}\\right)\\, dt\\right] \\\\[0.5em]\n&= \\dfrac{1}{2} \\int_{-\\infty}^\\infty e^{-i\\omega t}\\left[f(t) - f\\left(t-\\dfrac{\\pi}{\\omega}\\right)\\right]\\, dt \\\\[0.5em]\n\\end{aligned}\n\\]\n이다. 이로부터\n\\[\n\\lim_{|\\omega|\\to \\infty} \\left|\\hat{f}(\\omega) \\right| \\le \\lim_{|\\omega|\\to \\infty}\\dfrac{1}{2} \\int_{-\\infty}^\\infty \\left| f(t) - f\\left(t-\\dfrac{\\pi}{\\omega}\\right)\\right|\\, dt = 0\n\\]\n임을 안다. \\(\\square\\)\n\n\n\\(C_0 (\\mathbb{R})\\) 은 \\(f:\\mathbb{R}\\to \\mathbb{C}\\) 함수 가운데 모든 \\(\\mathbb{R}\\) 에서 연속이며 \\(\\displaystyle \\lim_{|t|\\to \\infty} f(t) = 0\\) 인 함수의 집합이다. \\(C_0 (\\mathbb{R})\\) 에 대해 노름을\n\\[\n\\|f\\| := \\sup_{t\\in \\mathbb{R}} |f(t)|\n\\]\n로 정의 할 수 있으며, 따라서 \\(C_0 (\\mathbb{R})\\) 은 노름 공간이다. 정리 1 로부터 푸리에 변환은 \\(L^1(\\mathbb{R})\\) 에서 \\(C_0(\\mathbb{R})\\) 로의 선형 변환이라는 것을 알 수 있다.\n\n\n정리 2 (\\(n\\) 계 도함수의 푸리에 변환) \\(f(t)\\) 가 \\(C^1(\\mathbb{R})\\) 함수이고 \\(\\displaystyle \\lim_{|t|\\to\\infty} f(t) = 0\\) 이며 \\(f,\\,f'\\in L^1(\\mathbb{R})\\) 이면\n\\[\n\\mathfrak{F}[f'(t)] = (i\\omega) \\mathfrak{F}[f(t)]\n\\tag{2}\\]\n이 성립한다. 또한 \\(f\\in C^n(\\mathbb{R})\\) 이며 \\(f,\\,f',\\ldots,\\,f^{(n)}\\in L^1(\\mathbb{R})\\) 이고 \\(k=1,\\ldots,\\,n-1\\) 에 대해 \\(\\displaystyle \\lim_{|t|\\to \\infty} f^{(k)}(t) = 0\\) 이라면 다음이 성립한다.\n\\[\n\\mathfrak{F}\\left[f^{(n)}(t)\\right] = (i\\omega)^n \\mathfrak{F}[f(t)].\n\\tag{3}\\]\n\n\n(증명). 푸리에 변환의 정의로부터\n\\[\n\\mathfrak{F}[f'(t)] = \\int_{-\\infty}^\\infty e^{-i\\omega t}f'(t)\\, dt = \\left[e^{-i\\omega t} f(t)\\right]_{-\\infty}^{\\infty} +i\\omega \\int_{\\infty}^\\infty e^{i\\omega t}f(t)\\, dt = (i\\omega)\\mathfrak{F}[f(t)]\n\\]\n이다. \\(1,\\,2,\\ldots,\\,(n-1)\\)-계 도함수에 대해 식 3 이 성립한다고 가정하면\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f^{(n)}(t)] &=(i\\omega)\\mathfrak{F}[f^{(n-1)}(t)] = (i\\omega)^n \\mathfrak{F}[f(t)]\n\\end{aligned}\n\\]\n이다. \\(\\square\\)\n\n\n곡선의 매끄러운 정도(smoothness) 는 그 곡선이 어떤 \\(C^n\\) 급 함수인지를 의미한다. 만약 \\(f^{(n)}\\in L^1(\\mathbb{R})\\) 이며 \\(\\displaystyle \\lim_{|t|\\to\\infty}f^{(n)}(t)=0\\) 이라면 리만 르벡 정리 에 따라 \\(\\displaystyle \\lim_{|\\omega|\\to\\infty} (i\\omega)^n \\mathfrak{F}[f(t)]=0\\) 이다. 즉 \\(f\\) 가 더 매끄러울 수록, 즉 \\(f\\in C^n\\) 에서 \\(n\\) 값이 클 수록 \\(\\mathfrak{F}[f(t)]\\) 는 \\(|\\omega| \\to \\infty\\) 극한에서 더 빨리 \\(0\\) 으로 수렴한다. 뒤에 나오겠지만 그 역도 마찬가지이다. \\(f\\in C^n(\\mathbb{R})\\) 은 함수의 전역적 성질이며 \\(\\displaystyle \\lim_{|\\omega|\\to\\infty}\\mathfrak{F}[f(t)] = 0\\) 은 국소적 성질이다. 함수의 전역적 성질이 그 푸리에 변환의 국소적 성질을 규정한다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "푸리에 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/fourier_transform.html#합성곱과-푸리에-변환",
    "href": "src/image_processing/wavelet/fourier_transform.html#합성곱과-푸리에-변환",
    "title": "푸리에 변환",
    "section": "2 합성곱과 푸리에 변환",
    "text": "2 합성곱과 푸리에 변환\n\n\n\n\n\n\n\n정의 4 (합성곱) \\(f,\\,g\\in L^1(\\mathbb{R})\\) 에 대해 두 함수의 합성곱 \\((f \\ast g)(t)\\) 는 다음과 같이 정의된다.\n\\[\n(f \\ast g)(t) := \\int_{-\\infty}^\\infty f(t-\\tau)\\,g(\\tau)\\, d\\tau\n\\tag{4}\\]\n\n\n\n\n\n\n명제 2 정의 4 에서의 \\((f\\ast g)\\in L^1(\\mathbb{R})\\) 이다.\n\n\n(증명). \\(\\|f\\ast g\\|_1 \\le \\|g\\|_1\\|f\\|_1\\) 임을 보이자. \\[\n\\begin{aligned}\n\\|f\\ast g\\|_1 = \\int_{-\\infty}^\\infty |(f\\ast g)(t)|\\, dt & = \\int_{-\\infty}^\\infty \\left|\\int_{-\\infty}^\\infty f(t-\\tau)\\, g(\\tau)\\, d\\tau \\right|\\, dt \\\\\n&\\le \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty |f(t-\\tau)\\, g(\\tau)| \\, d\\tau\\, dt \\\\\n&= \\int_{-\\infty}^\\infty |g(\\tau)|\\,d\\tau\\, \\int_{-\\infty}^\\infty |f(t)|\\,dt = \\|g\\|_1 \\|f\\|_1. \\qquad \\square\n\\end{aligned}\n\\]\n\n\n\n명제 3 \\(f,\\,g,\\,h \\in L^1(\\mathbb{R})\\) 에 대해 다음이 성립한다.\n  (\\(1\\)) \\(f \\ast g= g \\ast f\\).\n  (\\(2\\)) \\(f\\ast (g \\ast h)= (f \\ast g)\\ast h\\).\n  (\\(3\\)) \\(f \\ast (g + h) = f\\ast g + f \\ast h\\).\n\n\n(증명). (\\(1\\)) \\(\\displaystyle (f\\ast g)(t) = \\int_{-\\infty}^\\infty f(t-\\tau)g(\\tau) \\, d\\tau \\stackrel{s=t-\\tau}{=} \\int_{-\\infty}^\\infty f(s)\\, g(t-s)\\, ds = (g\\ast f)(t).\\)\n(\\(2\\))\n\\[\n\\begin{aligned}\n\\left[f\\ast (g \\ast h)\\right](t)&= \\int_{-\\infty}^\\infty f(t-\\tau) \\left[\\int_{-\\infty}^{\\infty} g(\\tau - s)\\, h (s)\\, ds\\right]\\, d\\tau \\\\\n&=\\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty f(t-\\tau) \\,g(\\tau-s) \\, d\\tau \\right] h(s)\\, ds \\\\\n&\\stackrel{x=\\tau-s}{=} \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty f((t-s)-x))\\, g(x)\\, dx\\right] h(s)\\, ds\\\\\n&= \\int_{-\\infty}^\\infty (f \\ast g)(t-s)\\, h(s)\\,ds \\\\\n&= (f \\ast g)\\ast h (t).\n\\end{aligned}\n\\]\n(\\(3\\)) trivial. \\(\\square\\)\n\n\n아래의 두 명제는 증명 없이 받아들인다.\n\n명제 4 \\(f\\in L^1(\\mathbb{R})\\) 이고 \\(g\\in L^\\infty(\\mathbb{R})\\) 이면 \\(f\\ast g\\) 는 \\(\\mathbb{R}\\) 에서 연속이다.\n\n\n\n명제 5 양수 \\(p,\\,q,\\,s\\) 가 \\(\\dfrac{1}{s} = \\dfrac{1}{p} + \\dfrac{1}{q} -1\\) 일 때 다음이 성립한다.\n\\[\n\\|f\\ast g\\|_s \\le \\|f\\|_p \\|g\\|_q\n\\]\n\n\n\n정리 3 (Convolution theorem) \\(f,\\,g\\in L^1(\\mathbb{R})\\) 일 때 다음이 성립한다.\n\\[\n\\mathfrak{F}[(f \\ast g)(t)] = \\mathfrak{F}[f(t)]\\mathfrak{F}[g(t)].\n\\]\n\n\n(증명). \\[\n\\begin{aligned}\n\\mathfrak{F}[(f \\ast g)(t)] &= \\int_{-\\infty}^\\infty e^{-i \\omega t} \\int_{-\\infty}^\\infty f(t-\\tau)\\, g(\\tau)\\, d\\tau\\, dt \\\\\n&= \\int_{-\\infty}^\\infty\\left[ \\int_{-\\infty}^\\infty e^{-i\\omega (t-\\tau)} f(t-\\tau) \\, dt\\right] e^{-i \\omega \\tau}g(\\tau)\\, d\\tau \\\\\n&= \\int_{-\\infty}^\\infty \\mathfrak{F}[f(t)] \\, e^{-i \\omega \\tau} g(\\tau)\\, d\\tau\\\\\n&= \\mathfrak{F}[f(t)]\\mathfrak{F}[g(t)]. \\qquad \\square\n\\end{aligned}\n\\]\n\n위의 정리는 합성곱의 푸리에 변환은 각 함수의 푸리에 변환의 곱과 같다는 의미이다.\n\n\n따름정리 1 \\(f,\\,g,\\,h\\in L^1(\\mathbb{R})\\) 이며\n\\[\nh(x) = \\int_{-\\infty}^\\infty g(\\omega)\\, e^{i\\omega x}\\, d\\omega\n\\]\n일 때 다음이 성립한다.\n\\[\n(f\\ast h)(x) = \\int_{-\\infty}^\\infty g(\\omega)\\, \\hat{f}(\\omega) \\, e^{i\\omega x}\\, d\\omega.\n\\]\n\n\n(증명). \\[\n\\begin{aligned}\n(f\\ast h)(x) &= \\int_{-\\infty}^\\infty f(x-\\tau) h(\\tau)\\, d\\tau = \\int_{-\\infty}^\\infty f(x-\\tau) \\left[\\int_{-\\infty}^\\infty g(\\omega)\\, e^{i\\omega \\tau}\\, d\\omega\\right]\\, d\\tau \\\\\n&= \\int_{-\\infty}^\\infty g(\\omega) \\left[\\int_{-\\infty}^\\infty f(x-\\tau) e^{i\\omega \\tau} \\, d\\tau\\right] \\, d\\omega\\\\\n&=\\int_{-\\infty}^\\infty g(\\omega) \\left[\\int_{-\\infty}^\\infty f(x-\\tau) e^{-i\\omega (x-\\tau)} \\, d\\tau\\right] \\, e^{i\\omega x} d\\omega\\\\\n&= \\int_{-\\infty}^\\infty g(\\omega) \\hat{f}(\\omega) e^{i\\omega x}\\, d\\omega, \\qquad \\square\n\\end{aligned}\n\\]\n\n\n\n2.1 역 푸리에 변환 문제\n우리는 \\(f \\in L^1(\\mathbb{R})\\) 에 대한 푸리에 변환을 알아보았다. 이제 그 역변환, 즉 어떤 frequency domain 에서의 함수 \\(\\hat{f}(\\omega)\\) 로 부터 원함수 \\(f(t)\\) 를 어떤 조건에서 구할 수 있는지 알아보기로 하자.\n\n정리 4 (푸리에 역변환) \\(f\\in L^1(\\mathbb{R})\\) 이고 \\(\\hat{f}=\\mathfrak{F}[f(t)]\\in L^1(\\mathbb{R})\\) 일 때\n\\[\n\\dfrac{1}{2\\pi}\\int_{-\\infty}^\\infty \\hat{f}(\\omega) e^{i\\omega t}\\, d\\omega = f(t)\n\\tag{5}\\]\n이다.\n\n\n(증명). \\[\n\\begin{aligned}\n\\dfrac{1}{2\\pi} \\int_{-\\infty}^\\infty \\hat{f}(\\omega)e^{i\\omega t}\\, d\\omega &= \\dfrac{1}{2\\pi} \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty f(t')e^{-i\\omega t'}\\, dt'\\right] e^{i\\omega t}\\, d\\omega \\\\\n&= \\dfrac{1}{2\\pi} \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty e^{i\\omega (t-t')}\\, d\\omega\\right] \\, f(t')\\, dt'\n\\end{aligned}\n\\]\n여기서 \\(\\int_\\mathbb{R} e^{i\\omega t}\\, d\\omega = 2\\pi \\delta(t)\\) 이므로\n\\[\n\\dfrac{1}{2\\pi} \\int_{-\\infty}^\\infty \\hat{f}(\\omega)e^{i\\omega t}\\, d\\omega = \\int_{-\\infty}^\\infty f'(t) \\delta(t-t')\\, dt' = f(t)\n\\]\n이다. \\(\\square\\)\n\n그러나 예제 2 에서 보았듯이 \\(f\\in L^1(\\mathbb{R})\\) 이더라도 \\(f\\not\\in L^1(\\mathbb{R})\\) 일 수 있다. 여기에 어떤 함수 \\(K_\\lambda(\\omega)\\) 를 도입하여 대부분의 \\(t\\in \\mathbb{R}\\) 에 대해 다음이 성립할 수 있도록 할 수 있음을 보이고자 한다.\n\\[\n\\lim_{\\lambda\\to \\infty} \\int_{-\\lambda}^\\lambda \\hat{f}(\\omega)K_\\lambda (\\omega)e^{i\\omega t}\\, dt = f(t).\n\\]\n이 때 \\(K_\\lambda(\\omega)\\) 를 summability kernel 이라고 한다.\n\n\n\n\n\n\n\n\n정의 5 (Summability kernel) \\(\\mathbb{R}\\) 에서의 summability kernel 은 아래의 조건을 만족하는 연속함수 \\(K_\\lambda :\\mathbb{R} \\to \\mathbb{C}\\) 의 집합 이다.\n  (\\(1\\)) \\(\\forall \\lambda &gt; 0,\\, \\displaystyle \\int_\\mathbb{R} K_\\lambda (x)\\, dx = 1\\).\n  (\\(2\\)) \\(\\exists M&gt;0,\\forall \\lambda &gt;0\\,\\, \\displaystyle \\|K_\\lambda\\|_1=\\int_{\\mathbb{R}} |K_\\lambda(x)|\\, dx \\le M\\).\n  (\\(3\\)) \\(\\forall \\delta &gt;0,\\, \\displaystyle \\lim_{\\lambda \\to \\infty} \\int_{|x|&gt;\\delta} |K_\\lambda(x)| \\, dx = 0\\).\n\n\n\n\n\n\n정리 5 \\(f\\in L^{1}(\\mathbb{R})\\) 이며 \\(\\{K_\\lambda\\}\\) 가 summability kernel 의 집합일 때 다음이 성립한다.\n\\[\n\\lim_{\\lambda \\to \\infty} \\|(f\\ast K_\\lambda)- f\\|_1 = 0.\n\\tag{6}\\]\n\n\n(증명). \\[\n\\begin{aligned}\n\\left|(f\\ast K_\\lambda) (t)- f(t)\\right| &= \\left|\\int_{-\\infty}^\\infty f(t-\\tau)\\, K_\\lambda(\\tau)\\, d\\tau - \\int_{-\\infty}^\\infty K_\\lambda(\\tau)f(t)\\, d\\tau\\right| & &;\\text{정의 ($1$)}\\\\\n&=\\left|\\int_{-\\infty}^\\infty \\left(f(t-\\tau) - f(t)\\right)K_\\lambda (\\tau)\\, d\\tau\\right| \\\\\n&\\le \\int_{-\\infty}^\\infty |K_\\lambda(\\tau) |f(t-\\tau)-f(t)|\\, d\\tau\n\\end{aligned}\n\\]\n이다. 이로부터\n\\[\n\\begin{aligned}\n\\|(f\\ast K_\\lambda) (t) - f(t)\\|_1 &= \\int_{-\\infty}^\\infty |(f\\ast K_\\lambda)(t) - f(t)|\\, dt \\\\\n&\\le\\int_{-\\infty}^\\infty dt \\int_{-\\infty}^\\infty |K_\\lambda(\\tau)| |f(t-\\tau) - f(t)|\\, d\\tau \\\\\n&= \\int_{-\\infty}^\\infty \\left[\\int_{-\\infty}^\\infty |f(t-\\tau) - f(t)| \\, dt\\right] \\,|K_\\lambda(\\tau)|\\, d\\tau \\\\\n&= \\int_{-\\infty}^\\infty \\|f(t-\\tau)-f(t)\\|_1 \\, |K_\\lambda (\\tau)| \\, d\\tau\n\\end{aligned}\n\\]\n정의 (\\(2\\)) 로부터 \\(\\|K_\\lambda\\|_1 \\le M\\) 인 \\(M&gt;0\\) 이 존재함을 안다. \\(f\\in L^1(\\mathbb{R})\\) 일 때 \\(\\int_{-\\infty}^\\infty |f(t-\\tau)- f(t)|\\, dt\\) 는 \\(\\tau\\) 에 대해 연속이므로 모든 \\(\\varepsilon&gt;0\\) 에 대해 어떤 \\(\\delta&gt;0\\) 이 존재하여 \\(|\\tau|&lt;\\delta\\) 이면 \\(\\|f(t-\\tau)-f(t)\\|_1&lt;\\dfrac{\\varepsilon}{M}\\) 이 되도록 할 수 있다. 또한 \\(\\|f(t-\\tau) - f(t)\\|_1\\) 는 유계이므로 \\(\\|f(t-\\tau) - f(t)\\|_1&lt;C\\) 인 \\(C&gt;0\\) 이 존재한다. 따라서\n\\[\n\\begin{aligned}\n\\lim_{\\lambda \\to \\infty}\\|(f\\ast K_\\lambda) (t) - f(t)\\|_1 &\\le \\lim_{\\lambda \\to \\infty} \\left[\\left(\\int_{-\\infty}^{-\\delta} + \\int_{-\\delta}^\\delta + \\int_{\\delta}^\\infty \\right)\\|f(t-\\tau)-f(t)\\|_1 \\, |K_\\lambda (\\tau)| \\, d\\tau\\right] \\\\\n&&lt; \\lim_{\\lambda \\to \\infty} \\left[C\\int_{|\\tau|&gt;\\delta} |K_\\lambda(\\tau)|\\, d\\tau+ \\int_{|\\tau|&lt;\\delta} \\dfrac{\\varepsilon}{M}|K_\\lambda (\\tau)| \\, d\\tau \\right] &lt;\\varepsilon\n\\end{aligned}\n\\]\n이므로 식 6 이 성립한다. \\(\\square\\)\n\n\n\n정리 6 (General modulation) \\(\\hat{f}=\\mathfrak{F}[f(t)],\\, \\hat{g}=\\mathfrak{F}[g(t)]\\) 이며 \\(\\hat{f},\\, \\hat{g}\\in L^1(\\mathbb{R})\\) 일 때 다음이 성립한다.\n\\[\n\\mathfrak{F}[f(t)\\,g(t)]= \\dfrac{1}{2\\pi}\\left(\\hat{f} \\ast \\hat{g}\\right).\n\\tag{7}\\]\n\n\n(증명). \\(\\hat{f},\\,\\hat{g}\\in L^1(\\mathbb{R})\\) 이므로 역변환이 가능하다. 정리 4 을 이용한다.\n\\[\\begin{aligned}\n\\mathfrak{F}[f(t)\\,g(t)](\\omega) &=\\int_{-\\infty}^\\infty e^{-i\\omega t} f(t)\\, g(t)\\, dt= \\dfrac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-i \\omega t}g(t)\\, dt \\int_{-\\infty}^\\infty \\hat{f}(\\omega') e^{i\\omega' t}d\\omega' \\\\\n&= \\dfrac{1}{2\\pi}\\int_{-\\infty}^\\infty \\hat{f}(\\omega') \\left[\\int_{-\\infty}^\\infty g(t)\\,e^{-i(\\omega-\\omega')t}\\, dt\\right]\\, d\\omega' \\\\\n&= \\dfrac{1}{2\\pi}\\int_{-\\infty}^\\infty \\hat{f}(\\omega') \\hat{g}(\\omega-\\omega')\\, d\\omega' = \\dfrac{1}{2\\pi} (\\hat{f} \\ast \\hat{g})(\\omega),\\qquad \\square\n\\end{aligned}\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "푸리에 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/fourier_transform.html#l2mathbbr-에서의-푸리에-변환",
    "href": "src/image_processing/wavelet/fourier_transform.html#l2mathbbr-에서의-푸리에-변환",
    "title": "푸리에 변환",
    "section": "3 \\(L^2(\\mathbb{R})\\) 에서의 푸리에 변환",
    "text": "3 \\(L^2(\\mathbb{R})\\) 에서의 푸리에 변환\n\\(L^2\\) 공간에서의 노름 \\(\\|\\cdot\\|_2\\) 은 다음과 같이 정의된다.\n\\[\n\\|f\\|_2 := \\left(\\int_{-\\infty}^\\infty |f(t)|^2\\, dt\\right)^{1/2}\n\\]\n\\(f\\in L^2(\\mathbb{R})\\) 일 때 \\(\\left\\|\\hat{f}\\right\\|_2 = \\sqrt{2\\pi}\\|f\\|_2\\) 이다. \\(\\sqrt{2\\pi}\\) factor 를 안보이게 하기 위해 푸리에 변환을 다음과 같이 재정의 한다.\n\\[\n\\hat{f}(\\omega):= \\mathfrak{F}[f(t)]=\\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty e^{-i\\omega t}\\, f(t)\\, dt\n\\tag{8}\\]\n\n\n정리 7 \\(f\\in C_{\\mathbb{R}}\\) 이며 어떤 유계인 구간 밖에서 \\(0\\) 값을 갖는다고 하자. 그렇다면 \\(\\hat{f}=\\mathfrak{F}[f(t)]\\in L^2(\\mathbb{R})\\) 이며\n\\[\n\\|f\\|_2 = \\left\\|\\hat{f}\\right\\|_2\n\\]\n이다.\n\n\n(증명). 일반성을 잃지 않고 \\(t\\not\\in [-\\pi,\\,\\pi]\\) 일 때 \\(f(t)=0\\) 이라고 할 수 있다. 아래의 \\(\\{\\phi_n(t)\\}\\) 는 \\(L^2(\\mathbb{R})\\) 의 정규직교기저이다.\n\\[\n\\left\\{\\phi_n(t)=\\dfrac{1}{\\sqrt{2\\pi}} e^{int}: n\\in \\mathbb{Z}\\right\\}.\n\\]\n따라서,\n\\[\nf(t) = \\sum_{n\\in \\mathbb{Z}}f_n e^{int},\\qquad \\text{where}\\; f_n = \\int \\overline{\\phi_n(t)} f(t)\\, dt = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}  ^\\infty e^{-int}f(t)\\,dt = \\hat{f}(n)\n\\]\n이며,\n\\[\n{\\|f\\|_2}^2 = \\sum_{n\\in \\mathbb{Z}} |\\hat{f}(n)|^2\n\\]\n이다. \\(g(t) = e^{i\\mu t}f(t)\\) 에 대해 \\(|g(t)|=|f(t)|\\) 이므로 \\(\\|g\\|_2 = \\|f\\|_2\\) 이다. 또한 \\(\\displaystyle g(t) = \\sum_{n\\in \\mathbb{Z}}f_n e^{i(n+\\mu)t}\\) 이므로\n\\[\n{\\|f\\|_2}^2 =\\sum_{n\\in \\mathbb{Z}} \\left|\\hat{f}(n+\\mu)\\right|^2\n\\]\n이다. 위 식의 양 변을 \\(\\mu\\) 에 대해 \\([0,\\,1]\\) 구간에서 적분하면\n\\[\n{\\|f\\|_2}^2 = \\sum_{n\\in \\mathbb{Z}}\\int_0^1 \\left|\\hat{f}(n+\\mu)\\right|^2 \\, d\\mu = \\int_{-\\infty}^\\infty \\left|\\hat{f}(\\mu)\\right|^2\\, d\\mu = {\\left\\|\\hat{f}\\right\\|_2}^2\n\\]\n이다. \\(\\square\\)\n\n\n정리 7 은 \\(f\\in L^2(\\mathbb{R}) \\iff\\hat{f}\\in L^2(\\mathbb{R})\\) 를 의미한다. \\(f\\in L^1(\\mathbb{R})\\) 일 경우에는 이것이 성립하지 않았음을 생각하라(예제 2).\n\n\n\n\n\n\n\n\n정의 6 (콤팩트 지지 함수) 위상공간 \\(X\\) 에서 정의된 \\(f:X \\to \\mathbb{R}\\) 에 대해 \\(S=\\{x\\in X:  f(x)\\ne 0\\}\\) 의 closure \\(\\overline{S}\\) 를 \\(f\\) 에 대한 support 라고 하고 \\(\\textrm{supp}(f)\\) 라고 쓴다. \\(\\textrm{supp}(f)\\) 가 옹골집합(compact set) 일 때 \\(f\\) 를 콤팩트 지지 함수 (compactly supported function) 라고 한다.\n\n\n\n\n\\(\\mathbb{R}\\) 에서의 옹골집합은 유계인 닫힌 집합과 동치이다. 즉 \\(\\mathbb{R}\\) 에서의 콤팩트 지지함수는 support 가 유계인 닫힌 집합인 함수이다. 그렇다면 \\(f\\) 가 \\(L^2(\\mathbb{R})\\) 에서의 콤팩트 지지함수라면 정리 7 에 따라 푸리에 변환이 존재하며 \\(\\|f\\|_2 = \\left\\|\\hat{f}\\right\\|_2\\) 이다.\n또 한가지 사실은 \\(\\mathbb{R}\\) 에서 정의된 모든 컴팩트 지지함수의 집합은 \\(L^2(\\mathbb{R})\\) 에서 조밀하다는 것이다(이것은 증명하지 않고 받아들이기로 하자). 즉 \\(f\\in L^2(\\mathbb{R})\\) 라면 임의의 \\(\\varepsilon&gt;0\\) 에 대해 \\(\\|f-g\\|_2&lt;\\varepsilon\\) 인 콤팩트 지지함수 \\(g\\in L^2(\\mathbb{R})\\) 을 찾을 수 있다는 의미이다. 그런 의미에서 푸리에 변환을 다음과 같이 정의 할 수 있다.\n\n\n\n\n\n\n\n\n정의 7 (\\(L^2(\\mathbb{R})\\) 에서의 푸리에 변환) \\(f\\in L^2(\\mathbb{R})\\) 이며 \\(\\{f_n : f_n\\in L^2(\\mathbb{R})\\}\\) 을 \\(\\displaystyle \\lim_{n \\to \\infty}f_n = f\\) 인 콤팩트 지지함수의 함수열이라고 하자. 이 때 \\(f\\) 의 푸리에 변환은 다음과 같이 정의된다.\n\\[\n\\hat{f} = \\lim_{n \\to \\infty} \\mathfrak{F}[f_n]\n\\]\n\n\n\n\n\\(f_n\\) 이 콤팩트 지지함수이므로 정리 7 에 따라 \\(\\|\\hat{f}_n\\|_2\\) 가 존재하며 그 극한이 \\(f\\) 의 푸리에 변환이다. 그렇다면 이 정의는 우리가 \\(L^1(\\mathbb{R})\\) 에서 했던 정의와 동일할까?\n\\(f\\in L^1(\\mathbb{R}) \\cap L^2(\\mathbb{R})\\)\n\n정리 8 (Parseval’s Identity) \\(f\\in L^2(\\mathbb{R})\\) 이면 \\(\\|f\\|_2 = \\left\\|\\hat{f}\\right\\|_2\\) 이다.\n\n\n(증명). 정의 7 과 정리 7 로부터,\n\\[\n\\left\\| \\hat{f}\\right\\|_2 = \\lim_{n \\to \\infty} \\left\\|\\mathfrak{F}[f_n]\\right\\|_2 = \\lim_{n \\to \\infty} \\|f_n\\|_2 = \\|f\\|_2.\\qquad \\square\n\\]\n\n\n\n정리 9 \\(f\\in L_2(\\mathbb{R})\\) 일 때\n\\[\n\\hat{f}(\\omega)= \\lim_{n \\to \\infty} \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-n}^n e^{-i\\omega t}f(t)\\, dt\n\\]\n이다.\n\n\n(증명). \\(n\\in \\mathbb{Z}_+\\) 에 대해 \\(f_n(t)\\) 를 아래와 같이 정의한다.\n\\[\nf_n (t) = \\left\\{\\begin{array}{ll} f(t), \\qquad &\\text{for }|t|&lt;n, \\\\ 0&\\text{otherwise}.\\end{array}\\right.\n\\tag{9}\\]\n\\(\\displaystyle \\lim_{n \\to \\infty} \\|f-f_n\\|_2=0\\) 이며, 따라서 \\(\\displaystyle \\lim_{n \\to \\infty}\\|\\hat{f} - \\hat{f}_n\\|_2 = 0\\) 이다. \\(\\square\\)\n\n\n\n정리 10 \\(f,\\,g\\in L^2(\\mathbb{R})\\) 일 때 다음이 성립한다.\n\\[\n\\left\\langle f,\\, \\overline{\\hat{g}}\\right\\rangle = \\int_{-\\infty}^\\infty f(t) \\hat{g(t)}\\, dt = \\int_{-\\infty} \\hat{f}(t)\\, g(t)\\, dt = \\left\\langle \\hat{f},\\, \\overline{g}\\right\\rangle.\n\\]\n\n\n(증명). 힘수열 \\(f_n,\\,g_n\\) 을 식 9 에 따라 정의 한다.\n\\[\n\\hat{f}_m (t) = \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty e^{-ist} f_m(s)\\, ds\n\\]\n이므로,\n\\[\n\\begin{aligned}\n\\int_{-\\infty}^\\infty \\hat {f}_m (t) g_n (t)\\, dt &= \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty g_n(t) \\left[\\int_{-\\infty}^\\infty e^{-ist}\\, f_m (s)\\, ds\\right]\\, dt \\\\\n&= \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty f_m (s) \\left[\\int_{-\\infty}^\\infty g_n(t)\\, e^{-ist}\\, dt\\right]\\, ds \\\\\n&=  \\int_{-\\infty}^\\infty f_m (s) \\,\\hat{g}_n (s)\\, ds\n\\end{aligned}\n\\]\n이다. \\(\\displaystyle \\lim_{n \\to \\infty} \\|g-g_n\\|_2 = 0\\), \\(\\displaystyle \\lim_{n \\to \\infty} \\|\\hat{g}-\\hat{g} _n\\|_2 = 0\\) 임을 안다. 이로부터\n\\[\n\\int_{-\\infty}^\\infty \\hat{f}_m(t) \\, g(t) \\, dt = \\int_{-\\infty}^\\infty f_m(t) \\, \\hat{g}(t)\\, dt\n\\]\n이다. 또한 \\(\\displaystyle \\lim_{m \\to \\infty} \\|f-f_n\\|_2 = 0\\), \\(\\displaystyle \\lim_{m \\to \\infty} \\|\\hat{f}-\\hat{f} _f\\|_2 = 0\\) 임 이므로 다음이 성립한다.\n\\[\n\\int_{-\\infty} \\hat{f}(t)\\, g(t)\\, dt  = \\int_{-\\infty}^\\infty f(t) \\hat{g(t)}\\, dt.  \\qquad \\square\n\\]\n\n\n\n보조정리 1 \\(f\\in L^2(\\mathbb{R})\\) 이고 \\(g=\\overline{\\hat{f}}\\) 이면 \\(f=\\overline{\\hat{g}}\\) 이다.\n\n\n(증명). 다음을 보일 수 있다.\n\\[\n\\left\\langle f,\\, \\overline{\\hat{g}}\\right\\rangle = \\left\\langle \\hat{f},\\, \\overline{g}\\right\\rangle = \\left\\langle \\hat{f},\\, \\hat{f}\\right\\rangle = {\\left\\|\\hat{f}\\right\\|_2}^2 = {\\|f\\|_2}^2.\n\\]\n또한\n\\[\n\\overline{\\langle f,\\, \\overline{\\hat{g}}\\rangle} = \\overline{\\langle \\hat{f},\\,\\hat{f}\\rangle} = \\|f\\|_2^2\n\\]\n이다. 마지막으로 Parseval’s relation 으로부터,\n\\[\n\\left\\|\\hat{g}\\right\\|_2^2 = \\|g\\|_2^2 = \\left\\|\\hat{f}\\right\\|_2^2 = \\|f\\|_2^2\n\\]\n임을 안다. 위의 세 식을 이용하면,\n\\[\n{\\left\\|f-\\overline{\\hat{g}}\\right\\|_2}^2 = \\left\\langle f-\\overline{\\hat{g}},\\,f-\\overline{\\hat{g}}\\right\\rangle = {\\|f\\|_2}^2 - \\langle f,\\, \\overline{\\hat{g}}\\rangle - \\overline{\\langle f,\\, \\overline{\\hat{g}}\\rangle} + \\left\\|\\hat{g}\\right\\|_2^2 = 0.\n\\]\n이다. \\(\\square\\)\n\n\n\n3.1 \\(L^2(\\mathbb{R})\\) 에서의 역푸리에 변환과 일반화된 Parserval’s Relation\n\n정리 11 \\(f\\in L^2(\\mathbb{R})\\) 에 대해\n\\[\n\\lim_{n\\to \\infty}\\dfrac{1}{\\sqrt{2\\pi}} \\int_{-n}^n e^{i\\omega t}\\hat{f}(\\omega)\\, d\\omega = f(t)\n\\tag{10}\\]\n이다.\n\n\n(증명). \\(f\\in L^2(\\mathbb{R})\\) 이며 \\(g=\\overline{\\hat{f}}\\) 라고 하자. 보조정리 1 에 의해\n\\[\n\\begin{aligned}\nf(t) = \\overline{\\hat{g}(t)} &= \\lim_{n \\to \\infty} \\dfrac{1}{\\sqrt{2\\pi}} \\overline{\\int_{-n}^n e^{-i\\omega t}\\, g(\\omega)\\, d\\omega} \\\\\n&= \\lim_{n \\to \\infty} \\dfrac{1}{\\sqrt{2\\pi}} \\int_{-n}^n e^{i\\omega t} \\overline{g(\\omega)}\\, d\\omega  = \\lim_{n \\to \\infty}\\dfrac{1}{\\sqrt{2\\pi}} \\int_{-n}^n e^{i\\omega t}\\, \\hat{f}(\\omega)\\, d\\omega. \\qquad \\square\n\\end{aligned}\n\\]\n\n여기서 식 10 를 푸리에 역변환 (inverse Fourier transform) 이라고 하고 \\(\\mathfrak{F}^{-1}\\left[\\hat{f}\\right]\\) 라고도 쓴다.\n\n\n정리 12 (일반화된 Parseval’s relation) \\(f,\\,g\\in L^2(\\mathbb{R})\\) 일 때 다음이 성립한다.\n\\[\n\\langle f,\\,g \\rangle = \\int_{-\\infty}^\\infty f(t) \\overline{g(t)}\\, dt = \\int_{-\\infty}^\\infty \\hat{f}(\\omega)\\overline{\\hat{g}(\\omega)}\\, d\\omega = \\left\\langle \\hat{f},\\, \\hat{g}\\right\\rangle.\n\\tag{11}\\]\n\n\n(증명). 정리 8 로부터 \\(\\|f+g\\|_2^2 = \\left\\|\\hat{f}+\\hat{g}\\right\\|_2^2\\) 임을 안다. 이로부터,\n\\[\n\\|f\\|_2^2 + \\|g\\|_2^2 + \\int \\left[\\overline{f(t)}\\,g(t) + f(t)\\,\\overline{g(t)}\\right]\\, dt = \\left\\|\\hat{f}\\right\\|_2^2 + \\left\\|\\hat{g}\\right\\|_2^2 + \\int \\left[\\overline{\\hat{f}(\\omega)}\\,\\hat{g}(\\omega) + \\hat{f}(\\omega)\\, \\overline{\\hat{g}(\\omega)}\\right]\\, d\\omega\n\\]\n이다. \\(\\|f\\|_2=\\left\\|\\hat{f}\\right\\|_2\\), \\(\\|g\\|_2 = \\left\\|\\hat{g}\\right\\|_2\\) 이므로,\n\\[\n\\int \\left[\\overline{f(t)}\\,g(t) + f(t)\\,\\overline{g(t)}\\right]\\, dt = \\int \\left[\\overline{\\hat{f}(\\omega)}\\,\\hat{g}(\\omega) + \\hat{f}(\\omega)\\, \\overline{\\hat{g}(\\omega)}\\right]\\, d\\omega\n\\tag{12}\\]\n이다. \\(g,\\, \\hat{g}\\) 를 각각 \\(ig,\\, i\\hat{g}\\) 로 변경시켜도 식 12 이 성립하므로,\n\\[\n\\int \\left[\\overline{f(t)}\\,g(t) - f(t)\\,\\overline{g(t)}\\right]\\, dt = \\int \\left[\\overline{\\hat{f}(\\omega)}\\,\\hat{g}(\\omega) - \\hat{f}(\\omega)\\, \\overline{\\hat{g}(\\omega)}\\right]\\, d\\omega\n\\tag{13}\\]\n이다. 식 12 에서 식 12 를 빼면\n\\[\n\\int f(t)\\,\\overline{g(t)}\\, dt = \\int  \\hat{f}(\\omega)\\, \\overline{\\hat{g}(\\omega)}\\, d\\omega\n\\]\n를 얻는다. 즉 \\(\\langle f,\\,g\\rangle = \\left\\langle \\hat{f},\\, \\hat{g}\\right\\rangle\\) 이다. \\(\\square\\)",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "푸리에 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/fourier_transform.html#이산-푸리에-변환-discrete-fourier-transformation-dft",
    "href": "src/image_processing/wavelet/fourier_transform.html#이산-푸리에-변환-discrete-fourier-transformation-dft",
    "title": "푸리에 변환",
    "section": "4 이산 푸리에 변환 (Discrete Fourier Transformation, DFT)",
    "text": "4 이산 푸리에 변환 (Discrete Fourier Transformation, DFT)\n\n4.1 이산 푸리에 변환과 역변환\n\\([a,\\,b]\\) 구간에서 정의된 1차원 공간상의 함수 \\(f\\) 에 대해 구간 내의 \\(N\\) 개의 점을 아래와 같이 잡을 수 있다.\n\\[\nt_1=a &lt; t_2 &lt; \\cdots &lt; t_{N-1} &lt; t_{N} = b.\n\\]\n계산을 간단하게 하기 위해 동일한 간격을 갖는 \\(t_k = a + (k-1)\\dfrac{b-a}{N-1}\\) 라고 할 수 있다. \\(f_k: = f(t_k)\\) 라고 하면 수열 \\(\\langle f \\rangle = (f_1,\\ldots,\\, f_{N})\\) 를 정의 할 수 있다.\n우리는 여기서 한가지 가정을 하게 되는데 \\(f\\) 이 주기 \\(N\\) 을 갖는다는 것이다. 정수 \\(m\\) 에 대해 \\(f_{k+mN}=f_k\\) 이다. 예를 들어 \\((1, 3, 2, 4)\\) 에 대한 이산 푸리에 변환은\n\\[\n\\ldots , \\underline{1, 3, 2 , 4}, 1, 3,2,4, \\underline{1, 3, 2,4},\\ldots\n\\]\n에 대한 푸리에 변환이라는 의미이다 (주기적으로 반복되는 것을 잘 보이게 하기 위해 밑줄로 표현하였다).\n\n\n\n\n\n\n\n정의 8 (이산 푸리에 변환) \\(\\langle f\\rangle = (f_1,\\ldots,\\,f_{N})\\) 에 대해 아래와 같이 정의된 \\(\\mathfrak{F}[f]:=\\langle F\\rangle = (F_1,\\ldots,\\,F_{N})\\) 을 \\(\\langle f\\rangle\\) 에 대한 이산 푸리에 변환(descrete Fourier transform, DFT) 이라고 한다.\n\\[\nF_u = \\mathfrak{F}[f]_u : =\\dfrac{1}{N} \\sum_{k=1}^{N} f_k \\exp\\left(-\\dfrac{i2\\pi (u-1)(k-1) }{N}  \\right).\n\\tag{14}\\]\n\n\n\n\n\n\n정리 13 (이산 푸리에 변환의 역변환) \\((f_k)\\) 에 대한 이산 푸리에 변환 \\((F_u)\\) 에 대해 다음이 성립한다.\n\\[\nf_k = \\sum_{u=1}^{N} F_u \\exp \\left(\\dfrac{i2\\pi (u-1)(k-1)}{N}\\right)\n\\tag{15}\\]\n\n\n(증명). \\[\n\\begin{aligned}\n\\sum_{u=1}^{N} F_u &\\exp \\left(\\dfrac{i2\\pi (k-1)(u-1)}{N}  \\right) \\\\\n&= \\dfrac{1}{N}\\sum_{u=1}^{N} \\left[\\sum_{k'=1}^{N} f_{k'} \\exp \\left(\\dfrac{-i2\\pi (k'-1)(u-1)}{N}  \\right)  \\right]\\exp \\left(\\dfrac{i2\\pi (k-1)(u-1)}{N} \\right)  \\\\\n&= \\dfrac{1}{N}\\sum_{k'=1}^{N} f_k \\sum_{u=1}^{N} \\exp\\left( \\dfrac{2\\pi i (k-k')(u-1)}{N}  \\right) \\\\\n\\end{aligned}\n\\tag{16}\\]\n우리는 여기서 \\(k'=k\\) 이면 \\(\\sum_{u=1}^{N} \\exp (\\cdots )= N\\) 임을 안다. \\(k'\\ne k\\) 라면 \\(|k'-k|\\le N-1\\) 이므로\n\\[\n\\sum_{u=1}^{N} \\exp\\left( \\dfrac{2\\pi i (k-k')(u-1)}{N}  \\right) = \\dfrac{{1-\\exp \\left( \\dfrac{2\\pi i (k-k') N}{N}\\right)}}{1-\\exp \\left( \\dfrac{2\\pi i (k-k')}{N}\\right)} = 0\n\\]\n이다. 따라서, 식 16 의 마지막 줄의 \\(\\sum_{u=1}^{N}\\exp (\\cdots)  = N\\delta_{k,k'}\\) 이므로\n\\[\n\\sum_{u=1}^{N} F_u \\exp \\left(\\dfrac{i2\\pi (k-1)(u-1)}{N}  \\right) = \\dfrac{1}{N} \\sum_{k=1}^{N} f_{k'} N\\delta_{k,k'} = f_k\n\\]\n이다. \\(\\square\\)\n\n이로부터 \\(\\langle F\\rangle = \\mathfrak{F}[f]\\) 에 대해 \\(\\langle f\\rangle = \\mathfrak{F}^{-1}[F]\\) 을 얻었다. 즉 DFT 로도 푸리에 변환과 역변환이 똑같이 성립한다.\n\n우리는 위의 식으로 부터 다음을 알 수 있다.\n\n푸리에 변환으로 얻는 \\(F_u = \\mathfrak{F}[f_k]\\) 의 변수 \\(u\\) 는 각진동수(angular frequency) 이다. 따라서 이 신호를 진동수(frequency) 에 대한 신호로 바꾸고 싶다면 변수를 \\(2\\pi u\\) 로 변경시켜야 한다.\n입력되는 신호 \\(f\\) 가 실수 신호더라도 푸리에 변환 신호는 복소수가 된다. 각진동수에 대한 분포를 알고싶다면 \\(|F_u|\\) 를 보아야 한다. Julia 에서는 abs 함수가 복소수에 대한 절대값을 반환한다.\n\n\n\n\n4.2 DFT 의 행렬 형태와 성질\n수열 \\((f)\\) 와 그 DFT \\((F)\\) 를 열벡터 형식으로 쓸 수 있다. 즉\n\\[\nf=\\begin{bmatrix} f_1 \\\\ \\vdots \\\\ f_N\\end{bmatrix},\\qquad F=\\begin{bmatrix} F_1 \\\\ \\vdots \\\\ F_N\\end{bmatrix}\n\\]\n이다. 이 때 식 14 와 식 15 에서 볼 수 있듯이 DFT 와 역변환은 선형 변환이다. 따라서 행렬 \\(W\\) 에 대해 \\(f=WF\\) 와 같은 꼴로 쓸 수 있다. 여기서\n\\[\nW_{jk} = \\dfrac{1}{N} \\exp \\left(-\\dfrac{2 \\pi i (j-1)(k-1)}{N}\\right)\n\\]\n이다.\n\n\n보조정리 2 수열 \\((f_k),\\,k=1,\\ldots,\\,N\\) 과 정수 \\(m\\in \\mathbb{Z}\\) 에 대해 다음이 성립한다.\n\\[\n\\sum_{k=1}^{N} f_{k-m} e^{-2\\pi i (k-m-1)(u-1)/N} = \\mathfrak{F}[f]_u\n\\]\n\n\n(증명). \\(f_k\\) 와 \\(e^{-2\\pi i (k-1)(u-1)/N}\\) 모두 \\(N\\) 주기 함수이며 따라서 위의 식은 \\(\\displaystyle \\sum_{k=1} f_k e^{-2\\pi i (k-1)(u-1)/N} = \\mathfrak{F}[f]_u\\) 와 같다. \\(\\square\\)\n\n\n\n정리 14 (DFT 의 성질) \\(\\hat{f} = \\mathfrak{F}[f]\\) 일 때 다음이 성립한다.\n  (\\(1\\)) \\(g_k = f_{k-m}\\) 인 수열 \\((g_k)\\) 에 대해 \\(\\mathfrak{F}[g]_{u} = \\mathfrak{F}[f]_u e^{-2\\pi i m(u-1)/N}\\) 이다.\n  (\\(2\\)) \\(g_k = f_k e^{2\\pi i (k-1)m/N}\\) 인 수열 \\((g_k)\\) 에 대해 \\(\\mathfrak{F}[g]_u = \\mathfrak{F}[f]_{u-m}\\) 이다.\n  (\\(3\\)) \\(g_k = f_k \\cos \\left(\\dfrac{2\\pi m(k-1)}{N}\\right)\\) 인 수열 \\((g_k)\\) 에 대해 \\(\\hat{g}_u = \\dfrac{1}{2}\\left(\\hat{f}_{u-m} + \\hat{f}_{u+m}\\right)\\) 이다.\n\n\n(증명). (\\(1\\)) 보조정리 2 을 이용하면\n\\[\n\\begin{aligned}\n\\hat{g}_u &= \\dfrac{1}{N}\\sum_{k=1}^N g_k e^{-2\\pi i (k-1)(u-1)/N } = \\dfrac{1}{N} \\sum_{k=1}^N f_{k-m} e^{-2\\pi i (k-m-1)(u-1)/N} e^{-2 \\pi i m(u-1)/N} \\\\\n&= \\hat{f}_u e^{-2\\pi i m(u-1)/N}.\n\\end{aligned}\n\\]\n(\\(2\\))\n\\[\n\\begin{aligned}\n\\mathfrak{F}[g]_u &= \\dfrac{1}{N}\\sum_{k=1}^N f_k e^{2\\pi i (k-1)m/N} e^{-2\\pi i (k-1)(u-1)} = \\dfrac{1}{N} \\sum_{k=1}^N f_k e^{-2\\pi i (k-1)(u-m-1)/N} = \\mathfrak{F}[f]_{u-m}.\n\\end{aligned}\n\\]\n(\\(3\\)) \\(\\cos \\theta = (e^{i\\theta} + e^{-i\\theta})/2\\) 와 (\\(2\\)) 를 이용하면\n\\[\n\\begin{aligned}\n\\mathfrak{F}[g]_u &= \\dfrac{1}{N}\\sum_{k=1}^N f_k \\dfrac{e^{2\\pi i m(k-1)/N} + e^{-2\\pi i m(k-1)/N}}{2}e^{-2\\pi i (k-1)(u-m-1)/N} \\\\\n&= \\dfrac{1}{2} \\left(\\mathfrak{F}[f]_{u-m} + \\mathfrak{F}[f]_{u+m}\\right). \\qquad \\square\n\\end{aligned}\n\\]",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "푸리에 변환"
    ]
  },
  {
    "objectID": "src/image_processing/wavelet/fourier_transform.html#고속-푸리에-변환-fft",
    "href": "src/image_processing/wavelet/fourier_transform.html#고속-푸리에-변환-fft",
    "title": "푸리에 변환",
    "section": "5 고속 푸리에 변환 (FFT)",
    "text": "5 고속 푸리에 변환 (FFT)\n고속 푸리에 변환(Fast Fourier Transform, FFT)은 DFT 과 그 역변환을 빠르게 수행하는 효율적인 알고리즘이다. 위의 이산 푸리에 변환은 \\(N^2\\) 번의 곱셉과 \\(N(N-1)\\) 번의 덧셈 연산, 즉 대략 \\(2N^2\\) 번의 연산이 필요하지만 FFT를 이용하면 대략 \\(\\mathcal{O}(N \\log_2 N)\\) 의 연산만으로 가능하다. 기본적인 아이디어는 가우스가 생각했었으나 잊혀졌고 이후 Cooley 와 Tukey 가 1965년도에 고속 푸리에 변환 알고리즘을 발표하였으며 이후 전 세계적으로 광범위하게 사용되었다.\n\n\n5.1 기본적인 아이디어\n우선 \\(N=2^n\\) 일 경우에 대해 생각하자. 그리고 우리는 \\(f_k\\) 의 인덱스 \\(k\\) 를 \\(1\\le k \\le N\\) 에서 생각했지만 여기서는 \\(0\\le k \\le N-1\\) 로 생각하자. 이 경우,\n\\[\n\\mathfrak{F}[f]_u=\\dfrac{1}{N}\\sum_{k=0}^{N-1} f_k e^{-2 \\pi i ku /N},\\qquad u=0,\\ldots,N-1\n\\] 이다. \\(f_k,\\, k=0,\\ldots,\\,N-1\\) 에 대해 \\((f_k)\\) 의 짝수 인덱스만 모아 수열 \\((a_k)\\) 를 만들고 홀수 인덱스만 모아 수열 \\((b_k)\\) 를 만든다.\n\\[\n\\begin{aligned}\na_k &= f_{2k}, \\\\\nb_k &= f_{2k+1}\n\\end{aligned}\n\\]\n이렇게 하면\n\\[\n(f_k) = (a_0,\\,b_0,\\,a_1,\\,b_1,\\ldots,\\, a_{N/2-2},\\, b_{N/2-2},\\, a_{N/2-1},\\, b_{N/2-1})\n\\]\n이다. \\(w_N := e^{-2\\pi i /N},\\, M=N/2=2^{n-1}\\) 이라 하면 \\({w_N}^2 = w_M = e^{-2\\pi i /M} = e^{-2\\pi i /(N/2)}\\) 이며, 이를 이용하여 \\(\\mathfrak{F}_u\\) 를 계산하면 아래와 같다.\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f]_u &= \\sum_{k=0}^{N-1} f_k e^{-2\\pi i uk/N} =  \\sum_{k=0}^{N-1}f_k {w_N}^{uk} \\\\\n&= \\sum_{k=0}^{M-1}a_k {w_N}^{2uk} + \\sum_{k=0}^{M-1} b_k {w_N}^{(2k+1)u}\\\\\n&= \\sum_{k=0}^{M-1} a_k (w_M)^{uk} + \\left({w_N}^u\\right) \\sum_{k=0}^{M-1} b_k (w_M)^{uk} \\\\\n\\end{aligned}\n\\tag{17}\\]\n이다. \\(M-1\\) 개의 점에 대한 \\((a_k)\\) 와 \\((b_k)\\) 의 푸리에 변환 \\(\\mathfrak{F}[a]_u,\\, \\mathfrak{F}[b]_u\\) 를 생각하자. \\(0 \\le u \\le M-1\\) 일 경우\n\\[\n\\mathfrak{F}[f]_{u} = \\mathfrak{F}[a]_u + ({w_N}^u) \\mathfrak{F}[b]_u\n\\tag{18}\\]\n이며, \\(M\\le u \\le N-1\\) 일 경우 \\(v=u-M\\) 으로 놓고 \\({w_N}^{-M} = (e^{-2\\pi i /N})^{-M} = e^{i\\pi} = -1\\) 과 \\({w_M}^M=1\\) 을 이용하면,\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f]_{M \\le u \\le N-1} &= \\sum_{k=0}^{M-1} a_k (w_M)^{uk} + ({w_N}^u) \\sum_{k=0}^{M-1} b_k (w_M)^{uk} \\\\\n&= \\sum_{k=0}^{M-1}a_k (w_M)^{vk} (w_M)^{Mk} + (w_N)^{v}(w_N)^M \\sum_{k=0}^{M-1}b_k (w_M)(w_M)^{vk} (w_M)^{Mk} \\\\\n&= \\sum_{k=0}^{M-1} a_k (w_M)^{vk} - ({w_N}^v) \\sum_{k=0}^{M-1} b_k (w_M)^{vk} \\\\\n&= \\mathfrak{F}[a]_{u-M} - ({w_N}^{u-M}) \\mathfrak{F}[b]_{u-M}\n\\end{aligned}\n\\tag{19}\\]\n이다. 즉 \\(1\\le u\\le M\\) 에 대해\n\\[\n\\begin{aligned}\n\\mathfrak{F}[f]_u &= \\mathfrak{F}[a]_u + ({w_N}^u) \\mathfrak{F}[b]_u, \\\\\n\\mathfrak{F}[f]_{u+M} &= \\mathfrak{F}[a]_u - ({w_N}^u) \\mathfrak{F}[b]_u,\n\\end{aligned}\n\\tag{20}\\]\n이다. 우리는 \\(N=2^n\\) 개의 점에 대한 DFT 가 \\(M=N/2\\) 인 두 푸리에 변환의 선형결합과 같다는 것을 알게 되었다. 만약 \\(N\\) 개의 점에 대한 DFT 의 연산 횟수를 \\(X(N)\\) 이라고 하자. \\(N=2^n\\) 일 때 \\(N/2=2^{n-1}\\) 개의 점에 대한 두번의 DFT 와 \\((w_N^m)\\) 과 \\(\\mathfrak{F}[b]_u\\) 와의 \\(N/2\\) 번의 곱셈 연산이 필요하므로\n\\[\n\\begin{aligned}\nX(N=2^n) &= 2X(2^{n-1}) + 2^{n-1} \\\\\n&= 2(2X(2^{n-2}) + 2^{n-2}) + 2^{n-1}\\\\\n&\\qquad \\vdots \\\\\n&= 2^{n-1} X(2^1) + (n-1)2^{n-1}\n\\end{aligned}\n\\]\n\\(N=2\\) 일 때 \\(X(2^1)=X(2)=4\\) 이므로\n\\[\nX(2^n) = 2^{n+1} + (n-1)2^{n-1} \\approx n 2^{n} = 2^n \\log_2 (2^n) = N \\log_2(N)\n\\]\n이다. 즉 \\(N=2^n\\) 개의 점에 대해 대략 \\(N \\log_2 (N)\\) 번의 연산을 통해 DFT 를 수행 할 수 있다.",
    "crumbs": [
      "영상 처리/토모그래피",
      "푸리에 변환과 웨이블릿 변환",
      "푸리에 변환"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html",
    "href": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html",
    "title": "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]\n과겨의 수치 해석은 구하고자 하는 식의 값을 최대한 바르고 정확하게 구하는 것이었다면, 현대의 수치 해석은 어떤 수학적인 문제를 컴퓨터를 이용하여 계산하는 것이다. 단순한 방정식의 해를 구하는 것 부터 아주 큰 행렬을 이용한 계산이나 복잡한 편미분방정식을 푸는 것까지 사용되고 있다. 즉 수치 해석은 복잡하거나, 계산의 양이 많거나, 정확한 해를 구하기가 힘들어 근사적으로 구해야 하는 등 사람이 직접 수행하기 힘든 수학적 작업을 컴퓨터를 사용하면 인간이 수행하는 것보다 훨씬 빠르고 정확하게 얻을 수 있기 때문이다.\n그러나 컴퓨터를 사용하기 때문에 한계도 존재한다 우선 수의 표현의 한계부터 알아보기로 하자.",
    "crumbs": [
      "수치해석 I",
      "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html#컴퓨터에서의-정수와-실수real-number-의-표현",
    "href": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html#컴퓨터에서의-정수와-실수real-number-의-표현",
    "title": "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)",
    "section": "1 컴퓨터에서의 정수와 실수(Real number) 의 표현",
    "text": "1 컴퓨터에서의 정수와 실수(Real number) 의 표현\n현대의 컴퓨터는 2진수로 모든것을 표현한다. 하나의 2진 단위를 비트 (bit) 라 하며 보통 8개의 비트 하나로 묶은 단위를 바이트 (Byte) 라 한다. 우리가 다루는 컴퓨터는 정해진 수의 바이트 수에 특정한 규칙에 따라 수를 표현한다. 예를 들어 8비트, 즉 1바이트에 정수를 표현한다고 하자. 이것으로 표현 할 수 있는 정수는 28=256 개가 전부이다. 이것보다 더 많은 정수를 표현하기 위해서는 더 큰 비트수를 차지하는 정수형을 사용해야 한다.\n\n\n1.1 Julia 에서의 정수의 표현\nIntroductio to Julia 의 기본적인 수 타입 과 정수형 타입 을 참고하라.\n\n많은 언어들이 정수에 대해 1, 2, 4, 8, 16 바이트 정수형을 제공한다. 예를 들어 8바이트, 즉 64비트 정수형의 경우는 264=18_446_744_073_709_551_616 개 즉, 대략 \\(1.8 \\times 10^{19}\\) 개의 정수를 표현 할 수 있다. 정수는 각 바이트에 대해 부호 있는 정수와 부호 없는 정수의 자료형이 있다. 예를 들어 2바이트 부호 없는 정수의 경우 \\(0\\) 부터 \\(2^{16}=65536\\) 까지의 정수를 다룰 수 있다. Julia 의 경우 2바이트 부호 없는 정수를 위한 자료형은 UInt16 이다. 부호 있는 정수의 경우에는 전체 바이트중 하나를 부호 즉 음수와 앙수를 구별하는데 사용한다. 기본적으로 부호 있는 정수에서 모든 비트가 \\(0\\) 이면 정수 \\(0\\) 으로 간주한다. 2바이트 부호 있는 정수의 경우 \\(-32758\\) 에서 \\(+32757\\) 까지의 정수를 다룰 수 있다.\n\n\n\n1.2 실수(Real Number) 의 표현\n빛의 속도 \\(c\\) 는 299792458 m/s = 0.299792458 x 109 m/s 를 0.299792458 과 109 로 나누어 보자. 여기서 0.299794568 부분을 가수(mentissa) 라고 하고 109 를 지수(exponent)라 한다. 실수를 이렇게 가수와 지수로 나누어 표현하는 것을 부동소수점 표현이라고 하며 실수(real number)를 컴퓨터에서 표현 할 때 부동소숫점 표현(floating point representation) 이 기본적으로 사용된다. 이에 대비되는 것이 고정 소숫점 표현(fixed point representation) 이다.\n\n\n1.3 고정소숫점 표현\n기본적인 원리는 정해진 데이터 비트에서 정수부분과 소수부분의 비트가 나누어져 있다. 예를 들자면 16비트 고정소수표현에서 맨 앞의 한 비트는 음수와 양수를 구분하는데, 그 다음의 9비트는 정수부에, 마지막 6비트는 소수부를 다루는데 사용 할 수 있다. 고정소숫점 표현은 정수의 연산을 거의 그대로 사용 할 수 있기 때문에 아래의 부동소숫점 표현보다 빠르게 계산 할 수 있다. 그러나 표현 할 수 있는 숫자의 범위가 매우 제한되어 있어 아래의 부동소숫점 표현보다는 사용도가 적다. Julia 의 대표적인 이미지 처리 패키지인 Images.jl 에서는 픽셀의 색상 정보에 고정소숫점 표현을 사용한다.\n\n\n\n1.4 부동소숫점표현\nJulia 는 부동소숫점 표현에서 The IEEE 754 기준을 따른다. 이 기준에 따르면, 64비트 실수 (이후 Float64 라 한다)의 경우 음수/양수 를 나타내는 1 비트, 지수를 나타내는 UInt 형식의 11비트, 가수를 나타내는 52비트로 이루어진다. 각각의 비트를 \\(b_0,\\,b_1,\\ldots,\\,b_{63}\\) 이라 하자. \\(0\\) 부터 시작하는 인덱스에 익숙하지 않은 독자를 위해 설명하자면 C, C++, Python 등 많은 언어는 배열의 인덱스를 \\(0\\) 부터 시작하도록 한다. Fortran 이나 Julia 는 \\(1\\) 시작하지만, 메모리 비트를 표현할때는 보통 \\(0\\) 부터 시작하는계 관례이므로 여기서는 \\(0\\) 부터 시작하도록 하자. 64 비트 실수형이므로 인덱스는 \\(0\\) 부터 \\(63\\) 까지 이다.실제로는 대부분의 컴퓨팅 연산의 기본이 IEEE 754 이다\n이 때 마지막 비트, 즉 \\(b_{63}\\) 을 최상위 비트라 하며, 양수인지, 음수인지를 나타내는 비트이다. 이 값을 \\(p\\) 라 하자. \\(p=0\\) 이면 양수, \\(p=1\\) 이면 음수이다. \\(b_{52}\\) 부터 \\(b_{62}\\) 까지는 지수 정보를 저장한다. 지수정보는 UInt 형식으로 저장되며 지수를 \\(e\\) 라 하면,\n\\[\ne= \\sum_{i=0}^{10} b_{52+i} 2^i\n\\]\n이다. \\(e\\) 는 \\(0\\) 부터 \\(2047\\) 까지의 값을 가질 수 있지만 실제 지수로서 기능하는 것은 \\(1\\) 부터 \\(2046\\) 까지이다. \\(e=0\\) 일 때와, \\(e=2047\\) 일 때는 뒤에 기술하는 특별한 방법을 사용한다(\\(e=2047\\) 에 대해서는 절 1.4.1 을 보라.) .\n가수는 \\(b_{0}\\) 부터 \\(b_{51}\\) 까지의 비트에 저장된다. 가수 \\(m\\) 이 의미하는 값은 \\(e\\) 값에 따라 아래와 같이 결정된다.\n\\[\nm= \\left\\{ \\begin{array}{ll} 1.0+ \\displaystyle{\\sum_{i=1}^{52}} \\, b_{52-i}2^{-i} \\qquad & \\text{if }e&gt;0\\,,\\\\ \\displaystyle{\\sum_{i=1}^{52}}\\, b_{52-i}2^{-i} & \\text{if }e = 0\\,. \\end{array} \\right.\n\\]\n가수 \\(m\\)의 범위는 \\(e&gt;0\\) 일 때는 \\(1\\le m &lt; 2\\) 이고 \\(e=0\\) 일 때는 \\(0\\le m &lt; 1\\) 이다. \\(e&gt;0\\) 일 경우, 52 비트로 1.0 이하의 유효숫자를 표현하고 거기에 에 1을 더해주기때문에 실제적으로는 가수는 53 bit 로 저장된다고 볼 수 있다. \\(e=0\\) 일 경우는 52비트로 저장된다고 볼 수 있다. \\(2^{-53}  \\approx 1.1 \\times 10^{-16},\\, 2^{-52} \\approx 2.2 \\times 10^{-16}\\) 이므로 Float64 형식의 실수는 소수점 아래 17자리 이하의 가수의 차이는 구별 할 수 없다. 즉 \\(1.0 + 1.0 \\times 10^{-17}\\) 은 \\(1.0\\) 과 구별되지 않는다.\n이제 부호 \\(p\\) 와 지수 \\(e\\), 가수 \\(m\\) 을 이용해 실수 \\(r\\) 을 표현하면,\n\\[\nr=\\left\\{ \\begin{array}{ll} (-1)^p \\times m \\times 2^{e-1023}  \\qquad & \\text{if }e&gt;0\\,, \\\\ (-1)^p \\times m \\times e^{-1022} & \\text{if } e=0 \\,.\\end{array}\\right.\n\\]\n이다.\n한가지 언급하자면, \\(e=0,\\,m=0.0\\) 일 경우 \\(p\\) 값에 관계 없이 \\(0.0\\) 이 된다. 즉, 실수 \\(0.0\\) 을 Float64로 표현하는 방법은 2가지로 \\(+0.0\\) 과 \\(-0.0\\) 인데, 대부분의 언어는 특별한 경우를 제외하면 이 둘을 동일하게 처리하도록 되어 있다.\n\n\nNaN, \\(\\pm \\infty\\)\n앞서 지수 \\(e\\) 가 2047일 경우는 특별히 다룬다고 언급하였다.\nIEEE 754 표준에서는 NaN 과 양/음의 무한대 표현을 정의한다. 지수비트가 모두 \\(1\\) 이고 가수비트가 모두 \\(0\\) 이면 무한대를 의미한다. 부호비트가 \\(0\\) 이면 \\(+\\infty\\), 부호비트가 \\(1\\) 이면 \\(-\\infty\\) 이다.\n\\(e=2047\\) 이고 양과 음의 무한대를 표현하는 경우를 제외하면 모두 NaN 으로 처리한다. NaN 은 Not a Number 의 약자로, 연산 과정 등에서의 오류를 표현한다. NaN 도 종류에 따라 구분하는 경우가 있지만 여기서는 다루지 않는다.\n아래 표는 이런 특별한 표현을 정리한 것이다.\n\n\n\n\n\n\n\n\n\n\\(b_{63}\\)\n\\(b_{62}\\) ~ \\(b_{52}\\)\n\\(b_{51}\\) ~ \\(b_{0}\\)\n표현\n\n\n\n\n0\n00000000000\n0000000000000000000000000000000000000000000000000000\n+0.0\n\n\n1\n00000000000\n0000000000000000000000000000000000000000000000000000\n−0.0\n\n\n0\n11111111111\n0000000000000000000000000000000000000000000000000000\n\\(+\\infty\\)\n\n\n1\n11111111111\n0000000000000000000000000000000000000000000000000000\n\\(-\\infty\\)\n\n\n0\n11111111111\n0000000000000000000000000000000000000000000000000001\nNaN\n\n\n0\n11111111111\n1000000000000000000000000000000000000000000000000001\nNaN\n\n\n0\n11111111111\n1111111111111111111111111111111111111111111111111111\nNaN\n\n\n\n\n혹시나 관심있는 사람을 위해 bitstring 함수를 소개한다. 이 함수는 값을 비트표현의 문자열로 바꾸어준다.\nIn [9]: bitstring(0.0)\nOut[9]: \"0000000000000000000000000000000000000000000000000000000000000000\"\n\nIn [10]: bitstring(Inf)\nOut[10]: \"0111111111110000000000000000000000000000000000000000000000000000\"\n\nIn [11]: bitstring(-Inf)\nOut[11]: \"1111111111110000000000000000000000000000000000000000000000000000\"\n\nIn [12]: bitstring(UInt8(255))\nOut[12]: \"11111111\"\n\nIn [13]: bitstring(UInt8(1))\nOut[13]: \"00000001\"",
    "crumbs": [
      "수치해석 I",
      "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html#trunctation-error-round-off-error",
    "href": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html#trunctation-error-round-off-error",
    "title": "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)",
    "section": "2 Trunctation error, Round-off error",
    "text": "2 Trunctation error, Round-off error\n컴퓨터로 수치해석을 하는 경우 오류가 있을 수 있다. 프로그래밍적으로 오류가 발생하여 실행되지 않을 수도 있고, 프로그래머의 오류로 (예를 들면 더해야 하는데 곱한다던가) 틀린 답을 낼 수도 있다. 이런 인적 오류 이외에 발생할 수 있는 오류로는 컴퓨터를 이용한 계산이기 때문에 어쩔수 없이 발생가게 되는 에러-truncation error 와 round-off error 가 있다.\n\n\n2.1 Truncation error\nTruncation error 는 수학적 근사에서 기인하는 오차이다. 예를 들어 \\(e^x\\) 를 테일러 전개를 이용하여 계산한다면,\n\\[\ne^x \\approx 1+x+\\dfrac{x^2}{2} + \\dfrac{x^3}{3!} + \\cdots + \\dfrac{x^n}{n!}+ \\cdots\n\\]\n임을 이용한다. 그런데 컴퓨터는 무한번 계산을 할 수 없으므로 어느 정도에 끊어야 하며, 나머지 항에 의한 값이 오차가 될 것이다. 이러한 오차를 truncation error 라 한다. 혹은 앞으로 많이 사용하겠지만 어떤 함수나 수열의 미분을 계산함에 있어\n\\[\nf'(x_0) \\approx \\dfrac{f(x_0 + h)-f(x_0)}{h}\n\\]\n와 같은 근사(approximation)를 사용하는데 어떤 특정한 작은 \\(h\\) 값을 사용하게 됨으로서 발생하는 오차도 truncation error 라고 할 수 있다.\n\n\n\n2.2 Round-off Error\nRound-off error 는 앞서 알아본 실수의 컴퓨터 표현에 의한 오차이다. 실수의 갯수는 무한대이며, 임의의 서로 다른 두 실수 사이에도 무한개의 실수가 존재한다. 그러나 64비트로 표현 가능한 실수는 최대한 \\(2^{64} \\approx 1.8\\times 10^{19}\\) 정도이다. 또한 앞서 알아봤듯이 유효숫자의 갯수는 10진수 표현으로 대략 16개 이며, IEEE 754 표준을 따르면 \\(3.6\\) 과 \\(3.6+1.0 \\times 10^{-16}\\) 은 구별되지 않는다. Julia 언어로 다음과 같이 계산해보자.\nIn [1]: 3.6-2.4-1.2\nOut[1]: 2.220446049250313e-16\n\nIn [2]: 3.6-1.2-2.4\nOut[2]: 4.440892098500626e-16\n다음과 같은 결과를 얻는다. 우리는 \\(3.6-2.4-1.2=0.0\\) 임을 알고 있지만, 컴퓨터로 계산한 결과는 다르다. 게다가 \\(3.6-2.4-1.2\\) 와 \\(3.6-1.2-2.4\\) 의 결과가 다르다. 이것은 불가피하며, 인간적인 관점에서는 다소 무작위적이기 때문에 계산을 하는 사람이 항상 주의해야 할 수 밖에 없다.\n또하나의 예를 들면,\nIn [3]: (3.4/1.6)*1.6\nOut[3]: 3.4000000000000004\n이다. 우리는 이 값이 3.4 이어야 함을 알고 있지만 ronnd-off 에러에 의해 결과가 \\(3.4+4.0\\times 10^{-16}\\) 으로 나왔다. 이는 Float64 에서 다루는 유효숫자의 갯수가 10진법으로 대략 16개 정도이기 때문에 발생한다.",
    "crumbs": [
      "수치해석 I",
      "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html#계산의-분석",
    "href": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html#계산의-분석",
    "title": "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)",
    "section": "3 계산의 분석",
    "text": "3 계산의 분석\n보통 알고리즘을 실행 속도 측면에서 분석할 때는 수행 시간이 문제의 크기를 표현하는 변수 \\(n\\) 에 대해 점근적으로 어떻게 행동하는지 예측한다. 여기서 \\(n\\) 은 벡터의 차원, 행렬의 크기, 다항식의 차수(degree) 등을 의미할 수 있다. 보통 \\(n\\) 이 커짐에 따라 계산이 복잡하거나 계산량이 많아지는 것을 의미한다. 보통 \\(n\\) 이 커짐에 따라 수행시간이 어떻게 되는지에 관심이 있다. 때문에 \\(n\\to \\infty\\) 극한에서 생각하는데, 알고리즘의 계산 복잡도 (computational complexity) 를 분석하는 방법으로 많이 사용되는 것 중 하나가 대문자 \\(O\\) 표기법이다.\n\n\n3.1 FLOP\n수행 시간을 계산할 때 대상의 크기에 대한 변수 \\(n\\) 에 대해 몇번의 기본연산을 수행하는지를 따진다. 많이 사용되는 FLOP(floating-point operations) 는 계산에 수행되는 모든 스칼라 사칙연산과 제곱근 계산을 1 flop 단위로 삼아 \\(n\\) 에 대한 함수 \\(T(n)\\) 으로 표현한다. 예를 들어 \\(n\\) 차원 공간에서의 두 벡터 u, v 의 내적은\nudotv = 0.0\nfor i = 1:n\n    udotv = udotv + u[i] * v[i]\nend\n를 통해 계산 할 수 있으며, \\(n\\) 번의 스칼라곱과 \\(n-1\\) 번의 스칼라합 계산이 수행되므로 내적에 대해서는 \\(T(n) = 2n-1\\) 이다. \\(n \\times n\\) 두 행렬 A, B 의 곱 C 는\nfor i = 1:n\n    for j = 1:n\n        for k = 1:n\n            C[i, j] = C[i, j] + (A[i, k] * B[k, j])\n        end\n    end\nend\n로 계산 할 수 있다. C[i,j] 를 계산하는데 \\(n\\) 번의 곱하기와 \\(n-1\\) 번의 더하기가 수행된다. C 가 \\(n \\times n\\) 행렬이므로 \\(n^3\\) 번의 곱하기와 \\(n^3-n^2\\) 번의 더하기가 수행되기 때문에 \\(T(n) = 2n^3-n^2\\) 이다.\n\n\n\n3.2 대문자 \\(O\\) 표기법\n예를 들어 \\(T(n) =  n^3+n\\) 이라고 하자. \\(n\\) 이 매우 클 경우 \\(T(n)\\) 의 값에 대한 기여는 대부분 \\(n^3\\) 에서 오며 \\(n\\) 이 기여하는 바는 미미하다. 이 때 함수를 가장 크게 기여하는 부분만을 표현하는 방법이 대문자 \\(O\\) 표기법이다.\n\n대문자 \\(O\\) 표기법의 정의\n자연수 \\(n\\) 에 대한 양의 함수 \\(f(n)\\), \\(g(n)\\) 에 대해 \\(f(n) = O(g(n))\\) 은\n\\[\n\\lim_{n \\to \\infty}\\dfrac{f(n)}{g(n)} &lt;\\infty\n\\]\n이 됨을 의미한다. 또한 \\(f(n) \\sim O(g(n))\\) 은\n\\[\n\\lim_{n \\to \\infty}\\dfrac{f(n)}{g(n)} =1\n\\]\n을 의미한다.\n\n보통 \\(O(f(n))\\) 에서 \\(f(n)\\) 은 가능한 함수 가운데 가장 간단한 함수를 사용한다. 예를 들어,\n\\[\n\\begin{aligned}\n2n^3-3n+1 &= O(n^2)\\\\\nn \\ln(n) + 3n &= O(n \\ln (n))\n\\end{aligned}\n\\]\n과 같다. 즉, \\(2n-1 = O(n)\\) 이다. \\(n \\times n\\) 행렬 \\(\\boldsymbol{A},\\,\\boldsymbol{B}\\) 의 곱 \\(\\boldsymbol{C}=\\boldsymbol{AB}\\) 를 생각하자. \\(C_{ij} = \\boldsymbol{A}_{i:}\\cdot \\boldsymbol{B}_{:j}\\) 이므로 \\(T(n) = 2n^3-n^2 = O(n^3)\\) 이다.",
    "crumbs": [
      "수치해석 I",
      "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html#알고리즘과-루프-불변성",
    "href": "src/numerical_analysis_using_julia/01_numerical_analysis_and_algorithm.html#알고리즘과-루프-불변성",
    "title": "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)",
    "section": "4 알고리즘과 루프 불변성",
    "text": "4 알고리즘과 루프 불변성\n알고리즘은 어떤 문제가 주어졌을 때 이 문제를 풀기 위한 구체적인 방법으로 유한번의 절차 이후 종료되어야 한다. 수치해석에서 대부분의 중요한 알고리즘은 for ... end 나 while ... end 같은 루프문 과 if ... elseif ... else... end 같은 분기문으로 이루어져 있다. 루프로 이루어지는 알고리즘이 가져야 할 가장 중요한 성질이 루프 불변성 (loop invariance) 이다. 알고리즘상의 어떤 명제 혹은 식 \\(P\\)가 아래와 같은 성질을 만족하면 루프 불변이라고 한다.\n\n초기조건 : 루프가 첫 번째 반복을 시작하기 전에 \\(P\\)가 참이어야 한다\n유지조건 : 루프의 \\(N\\) 회차 반복이 시작되기 전에 \\(P\\) 가 참이었다면 \\(N+1\\) 회차 반복이 시작되기 전까지도 계속 참이어야 한다\n종료조건 : 루프가 종료될 때 \\(P\\)이 알고리즘의 타당성을 보이는 데 도움이 될 유용한 특성을 가져야 한다\n\n초기조건과 유지조건은 수학적 귀납법이 성립하는 조건과 동일하다는 것을 쉽게 알 수 있을 것이다. 세번째 종료조건이 여기에 고유한 것으로 어쨌든 알고리즘은 원하는 결과를 얻게 하며 종료되어야 한다.\n가장 간단한 문제로 두 자연수의 최대공약수를 구하는 유클리드 호제법을 생각해보자. 두 자연수 \\(n_1,\\,n_2\\) 에 대해 \\(n_1\\) 을 \\(n_2\\) 로 나눈 나머지를 \\(\\text{rem} (n_1,\\,n_2)\\) 라 하자. 두 자연수 \\(a\\), \\(b\\) 에 대한 최대공약수는 다음과 같은 절차로 얻을 수 있다.\n[과정 1] \\(a,\\,b\\) 중 큰 수를 \\(a_1\\), 작은 수를 \\(b_1\\) 이라 하자. \\(a=b\\) 이면 당연히 최대공약수는 \\(a(=b)\\) 이다.\n[과정 2] \\(n=2,\\,3,\\ldots,\\) 에 대해 \\(a_n\\), \\(b_n\\) 이 양의 정수이며 \\(a_n&gt;b_n\\) 일 때 \\(a_{n+1}=b_n,\\, b_{n+1}=\\text{rem}( a_n,\\,b_n)\\) 이라 한다. 이 과정을 \\(b_n=0\\) 이 될 때 까지 수행한다.\n[과정 3] \\(b_{n}=0\\) 일 때 \\(\\gcd(a,\\,b) = a_{n}\\) 이다.\n유클리드 호제법에 대해 다음 명제가 루프불변성을 가짐을 보이자.\n\n\\(a_n &gt; b_n&gt;0\\)\n\\(a_{n+1} &lt; a_n,\\, b_{n+1} &lt; b_n\\)\n\n첫번째 반복이 시작하기 전에 \\(a_1 = \\max \\{a,\\,b\\},\\,b_1 = \\min \\{a,\\,b\\}\\) 정의했으므로 1번을 만족한다. 2 번은 첫번째 루프에서 만족시킬 필요가 없다.이제 \\(N\\) 번 반복을 수행했을 때 위의 두 명제를 만족한다고 하자. \\(N+1\\) 번 수행한 \\(a_{n+1},\\,b_{n+1}\\) 이 이 조건을 만족하는것은 자명하다. 루프가 종료되는 조건은 \\(a_n=b_n\\) 일 때이다. \\(0&lt;a_{n+1}&lt;a_n\\) 이며 \\(a_n\\) 은 항상 자연수이므로 이 알고리즘은 필연적으로 \\(a_n=b_n\\) 이 되어 종료된다. 이때의 값이 두 수 \\(a,\\,b\\) 의 최대공약수이다.\nJulia 의 minmax(x, y) 함수는 x, y 중 작은값, 큰값을 튜플로 리턴한다. 즉 minmax(9, 7)=(9, 7) 이다. 이 함수를 이용하면 간단히 구현 할 수 있다. gcd 는 julia 내에 정의된 함수이므로 이것과 구별하기 위해 mygcd 함수로 구현해보자.\nfunction mygcd(a::T, b::T) where T&lt;:Integer \n    a, b = minmax(a, b)\n    while(b ≠ 0)\n        a, b = b, rem(a, b)\n    end\n    return a\nend",
    "crumbs": [
      "수치해석 I",
      "수치해석 (Numerical Analysis) 과 알고리즘 (Algorithm)"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/03_linear_system.html",
    "href": "src/numerical_analysis_using_julia/03_linear_system.html",
    "title": "수치해석 입문 : 선형시스템과 다항식",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]\n이번 장에서는 본격적인 수치해석에 들어가기에 앞서 선형 시스템을 코딩을 통해 푸는 방법, 코딩에서 이루어지는 계산의 성능 분석과 다항식에 관한 객체를 만들어본다. 수학적 내용을 코드로 만들어 보고 이를 이용하여 문제를 풀어 보면서 선형대수학과 julia 언어에 대한 지식을 습득하게 된다. 그리고 다항식에 대한 객체는 이후 계속 사용하게 될 것이다.",
    "crumbs": [
      "수치해석 I",
      "수치해석 입문 : 선형시스템과 다항식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/03_linear_system.html#선형-시스템과-선형-방정식",
    "href": "src/numerical_analysis_using_julia/03_linear_system.html#선형-시스템과-선형-방정식",
    "title": "수치해석 입문 : 선형시스템과 다항식",
    "section": "1 선형 시스템과 선형 방정식",
    "text": "1 선형 시스템과 선형 방정식\n\n\n\n\n\n\n\n정의 1 (선형 시스템) 어떤 시스템이 \\(n\\) 개의 독립변수 \\(x_1,\\ldots,\\,x_n\\) 에 대해 아래와 같은 방정식으로 기술될 때 이 시스템을 선형 시스템이라고 한다. \\[\n\\begin{aligned}\nf_1 (x_1,\\ldots,\\,x_n ) &= A_{11}x_1 + \\cdots + A_{1n}x_n \\\\\n& \\vdots \\\\\nf_m (x_1,\\ldots,\\,x_n ) &= A_{m1}x_1 + \\cdots + A_{mn}x_n \\\\\n\\end{aligned}\n\\tag{1}\\]\n\n\n\n\n이 시스템은 \\(m \\times n\\) 행렬 \\(\\boldsymbol{A}\\) 와 \\(n \\times 1\\) 행렬 \\(\\boldsymbol{x}\\) 를 이용해 다음과 같은 식으로 정리 할 수 있다.\n\\[\nf_i(\\boldsymbol{x})=\\boldsymbol{Ax},\\qquad \\boldsymbol{A} = \\begin{bmatrix} A_{11} & \\cdots & A_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ A_{m1} & \\cdots &A_{mn} \\end{bmatrix},\\qquad \\boldsymbol{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n\\]\n이 시스템이 선형시스템이라고 불리는 이유는 \\(\\boldsymbol{x}_1,\\,\\boldsymbol{x}_2 \\in \\mathbb{F}^n\\) 이며 \\(a\\) 가 상수 일 때,\n\\[\nf_i(\\boldsymbol{x}_1 + a \\boldsymbol{x}_2) = f_i(\\boldsymbol{x}_1) + af_i(\\boldsymbol{x}_2) = \\boldsymbol{Ax}_1 + a\\boldsymbol{Ax}_2\n\\]\n를 만족하기 때문이다.\n주어진 \\(m \\times n\\) 행렬 \\(\\boldsymbol{A}\\) 에 대해 \\(\\boldsymbol{A}\\) 의 커널 \\(\\ker (\\boldsymbol{A})\\) 와 이미지 \\(\\text{im}(\\boldsymbol{A})\\) 를 다음과 같이 정의한다.\n\\[\n\\begin{aligned}\n\\ker (\\boldsymbol{A}) &:= \\left\\{\\boldsymbol{x}\\in \\mathbb{F}^n : \\boldsymbol{Ax}=\\boldsymbol{0} \\right\\}, \\\\[0.3em]\n\\text{im}(\\boldsymbol{A}) &:= \\left\\{\\boldsymbol{Ax}: \\boldsymbol{x} \\in \\mathbb{F}^n\\right\\}\n\\end{aligned}\n\\tag{2}\\]\n만약 \\(\\boldsymbol{b}\\in \\text{im}(\\boldsymbol{A})\\) 라면 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 만족하는 \\(\\boldsymbol{x}\\) 가 존재하며 이 \\(\\boldsymbol{x}\\) 를 선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 의 해(solution) 라고 한다. \\(\\boldsymbol{x}_0 \\in \\ker (\\boldsymbol{A})\\) 라면 \\(\\boldsymbol{A}(\\boldsymbol{x}+\\boldsymbol{x}_0) = \\boldsymbol{Ax}\\) 이다.\n\\(\\boldsymbol{A}\\) 가 \\(n \\times n\\) 정사각 행렬이며 \\(\\det (\\boldsymbol{A}) \\ne 0\\) 이면, 즉 \\(\\boldsymbol{A}\\) 의 역행렬이 존재한다면 선형방정식의 해는 유일하게 존재하며 \\(\\boldsymbol{x}=\\boldsymbol{A}^{-1}\\boldsymbol{b}\\) 를 통해 구할 수 있다. 역행렬이 존재하지 않는 정사각 행렬을 특이 행렬 (singular matrix) 라고 한다. \\(\\boldsymbol{A}\\) 가 특이행렬이라면 선형방정식의 해는 존재하지 않거나, 그 해가 무수히 많이 존재한다. 예를 들어\n\\[\n\\boldsymbol{A}=\\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\end{bmatrix}, \\qquad \\boldsymbol{b}=\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n\\]\n의 해는 \\(\\begin{bmatrix} x \\\\ 1-x\\end{bmatrix},\\, x\\in \\mathbb{F}\\) 인 모든 행렬이다.\n\n\n\n\n\n\n\n다음부터 소개할 여러가지 방법은 LinearAlgebra 모듈에 그 기능이 거의 포함되어 있기으며 아마 직접 코딩하는 것보다 처리속도가 더 빠를 것이다. 그러나 우리가 배워야 할 것은 모듈과 함수의 사용법 뿐만 아니라, 생각하는 알고리즘을 코드로 만들어 정확하게 구현하는 것이기에 앞으로 나올 여려 방법들을 직접 구현해보고자 한다.",
    "crumbs": [
      "수치해석 I",
      "수치해석 입문 : 선형시스템과 다항식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/03_linear_system.html#상삼각-행렬과-하삼각-행렬과-선형방정식",
    "href": "src/numerical_analysis_using_julia/03_linear_system.html#상삼각-행렬과-하삼각-행렬과-선형방정식",
    "title": "수치해석 입문 : 선형시스템과 다항식",
    "section": "2 상삼각 행렬과 하삼각 행렬과 선형방정식",
    "text": "2 상삼각 행렬과 하삼각 행렬과 선형방정식\n선형 시스템 가운데 비교적 단순한 상삼각 행렬과 하삼각 행렬에 대해 알아보자. 상삼각 행렬이나 하삼각 행렬의 선형방정식은 단순하지만 복잡한 선형 시스템을 푸는 기반이 된다. 이에 관련된 것은 LU 분해 에서 다루기로 한다\n\\(n\\times n\\) 정사각 행렬에 대해 상삼각 행렬(upper triangular matrix)은 (대각성분을 포함하지 않은) 대각 성분의 아랫부분이 모두 \\(0\\) 인 행렬을 말한다, 하삼각 행렬(lower triangular matrix)은 대각성분의 윗부분이 모두 \\(0\\) 인 행렬을 말한다. 예를 들어 \\[\n\\boldsymbol{U}=\\left[\\begin{array}{rrr} 1 & 3 & 4 \\\\ 0 & 2 & 1 \\\\ 0 & 0 & -1\\end{array}\\right], \\qquad \\boldsymbol{L}=\\left[\\begin{array}{rrr} 4 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & -3\\end{array} \\right]\n\\]\n에서 \\(\\boldsymbol{U}\\) 는 상삼각행렬, \\(\\boldsymbol{L}\\) 은 하삼각행렬이다. 두 삼각행렬에 대해 다음이 성립한다.\n\n\n명제 1 행렬 \\(\\boldsymbol{A} \\in \\mathbb{F}^{n\\times n}\\) 가 상삼각 행렬 혹은 하삼각 행렬일 때 다음은 동치이다.\n  (\\(1\\)) \\(\\det (\\boldsymbol{A}) = \\prod_{i=1}^n A_{ii} \\ne 0\\)\n  (\\(2\\)) \\(\\boldsymbol{A}\\) 는 가역행렬이다.\n  (\\(3\\)) \\(\\boldsymbol{b}\\in\\mathbb{F}^n\\) 에 대해 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 만족하는 \\(\\boldsymbol{x}\\) 가 유일하게 정해진다.\n\n\n\n\n2.1 하삼각 행렬에서의 선형방정식의 풀이\n\\(\\boldsymbol{L}\\in \\mathbb{F}^{n\\times n}\\) 과 \\(\\boldsymbol{b}\\in \\mathbb{F}^n\\) 에 대해 \\(\\boldsymbol{Lx}=\\boldsymbol{b}\\) 를 만족하는 \\(\\boldsymbol{x}\\in \\mathbb{F}^n\\) 을 구해보자. 이 때 \\(\\boldsymbol{L}\\) 의 각각의 대각 성분은 \\(0\\) 이 아니라고 하자.\n\\[\n\\begin{bmatrix} L_{11} & & & \\\\ L_{21} & L_{22}  & & \\\\ \\vdots & & \\ddots & \\\\ L_{n1} & L_{n2} & \\cdots & L_{nn} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\\\ \\vdots \\\\x_n \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix}\n\\]\n에 대해,\n\\[\n\\begin{aligned}\nL_{11} x_1 &= b_1, \\\\\nL_{21} x_1 + L_{22} x_2 &= b_2, \\\\\n\\vdots \\\\\nL_{k1} x_k + L_{k2}x_2 + \\cdots + L_{kk}x_k &= b_k,\\\\\n\\vdots \\\\\nL_{n1}x_1 + L_{n2}x_2 + \\cdots + L_{nn}x_n &= b_n\n\\end{aligned}\n\\] 를 얻는다. 첫번째 식으로부터 \\(x_1\\) 을 구할 수 있으며, 두번째 식에서는 이미 구한 \\(x_1\\) 을 이용하여 \\(x_2\\) 를 구할 수 있다. 즉 \\(x_1,\\ldots,\\,x_k\\) 까지 구했다면 이미 알고 있는 \\(\\boldsymbol{L}\\) 과 \\(\\boldsymbol{b}\\) 의 성분을 이용하여 \\(x_{k+1}\\) 을 구할 수 있다. 이를 정리하면 다음과 같다. \\[\n\\begin{aligned}\nx_1 &= \\dfrac{b_1}{L_{11}}, \\\\\nx_k &= \\dfrac{1}{L_{kk}} \\left( b_k - \\sum_{i=1}^{k-1} L_{ki}x_{i}\\right),\\,k=2,\\ldots,\\,n\n\\end{aligned}\n\\tag{3}\\]\n\n\n\n2.2 상삼각 행렬의 선형방정식의 풀이\n상삼각 행렬 \\(\\boldsymbol{U}\\in \\mathbb{F}^{n\\times n}\\) 와 \\(\\boldsymbol{b}\\in \\mathbb{F}^n\\) 에 대해 \\(\\boldsymbol{Ux}=\\boldsymbol{b}\\) 를 만족하는 \\(\\boldsymbol{x} \\mathbb{F}^n\\) 을 구해보자. 역시 \\(\\boldsymbol{U}\\) 의 각각의 대각성분은 \\(0\\) 이 아니다.\n\\[\n\\begin{bmatrix} U_{11} & U_{12} &\\cdots & U_{11} \\\\  & U_{22}  &\\cdots  & U_{21} \\\\  & & \\ddots & \\vdots\\\\  & &  & U_{nn} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\\\ \\vdots \\\\x_n \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix}\n\\]\n에 대해,\n\\[\n\\begin{aligned}\nU_{nn} x_n &= b_n, \\\\\nU_{n-1,n-1} x_{n-1} + U_{n-1,n} x_n &= b_{n-1}, \\\\\n\\vdots \\\\\nU_{kk} x_k + U_{k,k+1}x_{k+1} + \\cdots + U_{kn}x_n &= b_k,\\\\\n\\vdots \\\\\nU_{11}x_1 + U_{12}x_2 + \\cdots + U_{1n}x_1 &= b_1\n\\end{aligned}\n\\] 를 얻는다. 첫번째 식으로부터 \\(x_n\\) 을 구할 수 있으며, 두번째 식에서는 이미 구한 \\(x_{n}\\) 을 이용하여 \\(x_{n-1}\\) 를 구할 수 있다. 즉 \\(x_{n},\\ldots,\\,x_{k}\\) 까지 구했다면 이미 알고 있는 \\(\\boldsymbol{L}\\) 과 \\(\\boldsymbol{b}\\) 의 성분을 이용하여 \\(x_{k-1}\\) 을 구할 수 있다. 이를 정리하면 다음과 같다. \\[\n\\begin{aligned}\nx_n &= \\dfrac{b_n}{U_{nn}}, \\\\\nx_k &= \\dfrac{1}{U_{kk}} \\left( b_k - \\sum_{i=k+1}^{n} U_{ki}x_{i}\\right),\\,k=n-1,\\,n-2,\\ldots,\\,1\n\\end{aligned}\n\\tag{4}\\]\n임을 안다.\n\n\n\n2.3 코드 작성\n상삼각행렬에 대해서는 Us(), 하삼각행렬에 대해서는 Ls() 함수로 선형방정식의 해를 구하는 코드를 작성해보자. 아래의 코드는 가능한 답중의 하나이다.\n\n\n\n\n\n\n이 코드를 포함하여 앞으로 나올 코드에 나오는 부동소수값은 별도로 특별한 언급이 없다면 모두 Float64 타입이며, 정수는 모두 Int64 타입이라고 가정한다. 그렇지 않다면 서로 다른 타입의 값에 대한 연산 문제로 코드가 불필요하게 길어지고 가독성도 해친다.\n\n\n\n\"\"\"\n    Ls(A, b)\n\n하삼각행렬 A 에 대해 Ax=b 의 해 x 를 구한다.\n\"\"\"\nfunction Ls(L::Matrix, b::Vector) \n    m, n = size(L)        \n    x = zeros(n)\n    x[1] = b[1]/L[1, 1]\n    for i in 2:n\n        x[i] = b[i]\n        for j in 1:1:(i-1)\n            x[i] -= L[i, j]*x[j]\n        end\n        x[i] = x[i]/L[i, i]\n    end\n    return x\nend\n\n\"\"\"\n    Us(A, b)\n\n상삼각행렬 A 에 대해 Ax=b 의 해 x 를 구한다.\n\"\"\"\nfunction Us(U::Matrix, b::Vector) \n    m, n = size(U)       \n    x = zeros(n)\n    x[n] = b[n]/U[n, n]\n\n    for i in (n-1):-1:1\n        x[i] = b[i]\n        for j in (i+1):1:n\n            x[i] -= U[i, j] * x[j]\n        end\n        x[i] = x[i]/U[i, i]\n    end\n    return x\nend\n\n이제 \\(\\boldsymbol{L}=\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 2 & 2 & 0 & 0 \\\\3 & 1 & -3 & 0 \\\\5 & -2 & 3 & 7 \\end{bmatrix}\\) 와 \\(\\boldsymbol{b}= \\begin{bmatrix} 3.1 \\\\  5.3 \\\\ -2.2 \\\\ 6.0 \\end{bmatrix}\\) 에 대해 풀어보면\nL = [1. 0. 0. 0.; 2. 2. 0. 0.; 3. 1. -3. 0.; 5. -2. 3. 7.]\nb = [3.1; 5.3; -2.2; 6.0]\nx = Ls(L, b) \n를 통해 \\(\\boldsymbol{x}\\) 를 구할 수 있다. 이제 \\(\\boldsymbol{L\\cdot x}\\) 가 \\(\\boldsymbol{b}\\) 와 같은지 확인하기 위해 L*x 를 실행해보면,\n4-element Vector{Float64}:\n  3.1\n  5.3\n -2.2\n  6.0\n의 결과가 나오므로 \\(\\boldsymbol{b}\\) 를 잘 구했다.\n\n\nRoundoff 에러의 예\n이제 약간 극단적인 경우를 살펴 보자. 하삼각 행렬에서 대각성분이 0 이 아닌 다른 성분에 비해 매우 작은 경우이다.\nA1= [1.0 0 0; 1.0e8 1 0; 1.0e8 1.0e8 1]\nb1 = [1; 1; 1.0]\nx1 = Ls(A1, b1)\n이 때 구한 x1 은 다음과 같다.\n3-element Vector{Float64}:\n  1.0\n -9.9999999e7\n  9.9999998e15\n이 경우 A1*x1 을 수행하면,\n3-element Vector{Float64}:\n 1.0\n 1.0\n 0.0\n가 나와 실제 b1 값과 차이가 남을 알 수 있다. 이것은 64 비트 부동소수의 유효자리수 때문이다. 직접 손으로 계산해 보면 알겠지만 x1 은 정확히 계산이 되었다. 다만 A1*x1 의 세번째 성분은 다음 식을 통해 계산되는데\n\\[\n1.0\\times 10^8 \\times 1 - 9.9999999\\times 10^{15} + (1- 1.0\\times 10^8 \\times 1 + 9.9999999\\times 10^{15})\n\\]\n뒤의 괄호 안을 계산 할 때 \\(9.9999999\\times 10^{15}\\) 라는 숫자가 너무 커서 1 정도의 차이를 표현 할 수 없다. Float64 에서 9.9e15+1 은 9.9e15 과 구별 할 수 없다. 즉 Roundoff 에러가 발생한 것이다. 이런 문제는 BigFloat 와 같은 타입을 사용하면 어느 정도 해소되지만 여기서는 일단은 다루지 않는다.\n\n\n\n\n2.4 계산 복잡도 분석\n하삼각 행렬의 경우 \\(x_1\\) 을 계산하는데흔 한번의 나눗셈, \\(k\\ne 1\\) 일 때 \\(x_k\\) 를 계산하는데 스칼라곱이 \\(k-1\\) 번, 덧셈과 뺄셈이 \\(k\\) 번, 나눗셈 \\(1\\) 번이 필요하므로 모두 \\(2k\\) 번의 계신이 필요하다. \\(n \\times n\\) 하삼각 행렬의 행렬식에 대해서\n\\[\nT(n) = 1 + \\sum_{k=2}^n 2k = n^2 + n -1 = O(n^2)\n\\]\n이다.",
    "crumbs": [
      "수치해석 I",
      "수치해석 입문 : 선형시스템과 다항식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/03_linear_system.html#sec-gauss_jordan_elimination",
    "href": "src/numerical_analysis_using_julia/03_linear_system.html#sec-gauss_jordan_elimination",
    "title": "수치해석 입문 : 선형시스템과 다항식",
    "section": "3 가우스-요르단 소거법",
    "text": "3 가우스-요르단 소거법\n\\(m \\times n\\) 행렬 \\(\\boldsymbol{A}\\) 와 \\(n \\times k\\) 행렬 \\(\\boldsymbol{X}\\), \\(m \\times k\\) 행렬 \\(B\\) 가 \\(\\boldsymbol{AX}=\\boldsymbol{B}\\) 를 만족하며 \\(\\boldsymbol{L}\\) 이 \\(n \\times n\\) 가역행렬 일 때 다음이 성립한다.\n\\[\n\\boldsymbol{AX}=\\boldsymbol{B} \\iff  \\boldsymbol{LAX} = \\boldsymbol{LB}\n\\]\n만약 \\(k=1\\) 이라면, 즉 \\(n \\times 1\\) 행렬 \\(\\boldsymbol{x}\\),, \\(m \\times 1\\) 행렬 \\(\\boldsymbol{b}\\) 에 대해,\n\\[\n\\boldsymbol{Ax}=\\boldsymbol{b} \\iff  \\boldsymbol{LAx} = \\boldsymbol{Lb}\n\\]\n가 성립한다. 즉 주어진 \\(\\boldsymbol{A}\\) 와 \\(\\boldsymbol{b}\\) 에 대해 가역 행렬 \\(\\boldsymbol{L}\\) 을 통해 \\(\\boldsymbol{LA}\\) 가 매우 간단한 행렬이 된다면 \\(\\boldsymbol{LAx}=\\boldsymbol{Lb}\\) 를 만족하는 \\(\\boldsymbol{x}\\) 를 쉽게 구할 수 있으며, 이 \\(\\boldsymbol{x}\\) 가 우리가 구하고자하는 선형방정식의 해이다.\n\n\n3.1 행 사다리꼴 행렬, 행 간소 사다리꼴 행렬과 행 기본 연산\n\n행 사다리꼴과 행 간소 사다리꼴\n행 사다리꼴 행렬(row echelon form matrix)은 다음의 조건을 만족하는 행렬이다.\n\n영벡터가 존재할 경우 이 영벡터는 영벡터가 아닌 행벡터의 아래에 위치한다.\n행렬의 행벡터가 영벡터가 아닐 때, 처음으로 나타나는 0 이 아닌 성분을 선행 성분이라 한다. 윗 행의 선행 성분은 아래 행 전체의 각각의 선행성분보다 앞서 존재한다.\n\n아래의 행렬 \\(\\boldsymbol{A}_1,\\,\\boldsymbol{A}_2\\) 는 각각 1, 2 번 조건을 거스르는 행렬이므로 행사다리꼴이 아니다. (선행 성분을 밑줄로 표시하였다.)\n\\[\n\\begin{aligned}\n\\boldsymbol{A}_1 &= \\begin{bmatrix} \\underline{1} & 2 & 0 \\\\0 & 0 & 0 \\\\ 0 & \\underline{1} & 0\\end{bmatrix}, \\\\\n\\boldsymbol{A}_2 &= \\begin{bmatrix} 0 & 0 & \\underline{1} \\\\ \\underline{1} & 0 & 2\\end{bmatrix}.\n\\end{aligned}\n\\]\n\n행 간소 사다리꼴 행렬(row-reduced echelon form matrix)은 행 사다리꼴 행렬의 조건에 더해 다음의 조건이 추가된다.\n\n각 행의 선행성분은 \\(1\\) 이다. 이를 선행 1 성분 이라 하자.\n선행 1 성분이 존재하는 열은 선행성분을 제외한 모든 성분이 \\(0\\) 이다.\n\n\n\n\n기본 행 연산\n우리는 선형대수학으로부터 모든 행렬은 세가지의 기본 행 연산(elementary row operation)을 통해 행 사다리꼴과 행 간소 사다리꼴 행렬로 만들 수 있으며, 행렬의 행 간소 사다리꼴은 유일하다는 것을 안다. 이때의 기본 행 연산은 다음과 같다.\n\n두 행의 위치를 서로 바꾼다. (\\(\\hat{L}_1\\) 연산)\n특정 행에 \\(0\\) 이 아닌 스칼라를 곱한다. (\\(\\hat{L}_2\\) 연산)\n한 행에 다른 행의 스칼라곱을 더한다. (\\(\\hat{L}_3\\) 연산)\n\n\n\\(m \\times n\\) 행렬에 대한 행 기본 연산은 \\(m \\times m\\) 행렬로 표현된다. \\(k\\) 번째 행과 \\(l\\) 번째 행의 위치를 서로 바꾸는 행렬 \\(\\boldsymbol{L}_1 (k, l)\\) 는 \\[\n[\\boldsymbol{L}_1(k,\\,l)]_{i,\\, j} = \\left\\{\\begin{array}{ll} 1 \\qquad &\\text{if } i = j \\ne k, \\text{ and }i = j \\ne l \\\\ 1 & \\text{if } i = k,\\, j = l,\\\\ 1 & \\text{if } j = k,\\, i = l, \\\\ 0 & \\text{otherwise}\\end{array} \\right.\n\\]\n이며 \\(4 \\times 4\\) 행렬에서 1 행과 3행을 교환하는 행렬 \\(\\boldsymbol{L}_1 (1, 3)\\) 은 다음과 같다.\n\\[\n\\boldsymbol{L}_1(1, 3)\n=\\begin{bmatrix}\n0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0\\\\  0 & 0 & 0 & 1\n\\end{bmatrix} .\n\\]\n\\(k\\) 번째 행에 스칼라 \\(c\\) 를 곱하는 연산을 나타내는 행렬 \\(\\boldsymbol{L}_2(k, c)\\) 는\n\\[\n[\\boldsymbol{L}_2(k,\\, c)]_{i,\\,j}= \\left\\{\\begin{array}{ll} 1 \\qquad & \\text{if } i=j\\ne k,\\, \\\\ c &\\text{if } i=j=k,\\,\\\\ 0 &\\text{otherwise}\\end{array}\\right.\n\\]\n이며 \\(4\\times 4\\) 행렬에서 2행에 스칼라 \\(c\\) 를 곱하는 행렬은 \\(\\boldsymbol{L}_2 (2, c)\\) 는 다음과 같다.\n\\[\n[\\boldsymbol{L}_2 (2,\\,c)] = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}.\n\\]\n\\(l\\) 번째 행에 \\(c\\) 를 곱한 것을 \\(k\\) 번째 행에 더하는 행렬 \\(\\boldsymbol{L}_3 (k, l, c)\\) 은\n\\[\n[\\boldsymbol{L}_3 (k,\\,l,\\,c)]_{i,\\,j} = \\left\\{ \\begin{array}{ll}1 \\qquad & \\text{if } i=j \\,,\\\\\nc & \\text{if } i = k, j = l\\,, \\\\ 0 &\\text{oterwise}  \\end{array}\\right.\n\\]\n이며 \\(4 \\times 4\\) 행렬에서 3 행에 \\(c\\) 를 곱해 \\(1\\) 행에 더하는 행렬 \\(\\boldsymbol{R}\\) 은 다음과 같다.\n\\[\n\\boldsymbol{L}_3 (1, 3, c) = \\begin{bmatrix} 1 & 0 & c & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix}.\n\\]\n모든 행 기본 연산은 \\(\\boldsymbol{L}_1,\\,\\boldsymbol{L}_2,\\,\\boldsymbol{L}_3\\) 가역행렬이며 그 역행렬은 다음과 같다.\n\\[\n\\begin{aligned}\n\\left[\\boldsymbol{L}_1(k, l) \\right]^{-1} &= \\boldsymbol{L}_1(k, l), \\\\\n\\left[\\boldsymbol{L}_2(k, c) \\right]^{-1} &= \\boldsymbol{L}_2(k, 1/c), \\\\\n\\left[\\boldsymbol{L}_3(k, l, c) \\right]^{-1} &= \\boldsymbol{L}_3(k, l, -c),\n\\end{aligned}\n\\]\n\n\n\n\n3.2 가우스-요르단 소거 과정 (Gauss-Jordan elimination method)\n가우스 소거법은 기본 행 연산을 유한번 시행하여 행렬을 행 사다리꼴(Row echelon form) 행렬로 변환시키는 것을 말한다. 목적이 사다리꼴이 아닌 행 간소 사다리꼴(Row-reduced echelon form) 일 경우 가우스-요르단 소거법이라 한다. 사람에 따라 가우스-요르단 소거법을 가우스 소거법이라고 하는데 여기서는 둘을 구분하기로 하자. 정사각행렬의 경우 가우스 소거법에 의해 상삼각행렬로 변환되며, 가역행렬일 경우 가우스-요르단 소거법의 결과는 단위행렬이다.\n이제 \\(m\\times n\\) 행렬 \\(\\boldsymbol{A}\\) 에 대한 가우스 소거법을 생각해보자. \\(\\boldsymbol{A}\\) 가 영행렬이면 의미가 없으므로 영행렬이 아닐 때만 생각한다.\nP1-1. 영벡터가 아닌 첫번째 열벡터를 찾는다. 그 열벡터의 첫번째 행이 \\(0\\) 이거나 절대값이 작을 경우 그 열벡터에서 적당한 \\(0\\) 이 아닌 행을 찾아 그 행과 첫번째 행을 교환한다(\\(\\hat{L}_1\\) 연산). 이것을 피보팅 (pivoting) 이라 한다. 보통 첫번째 행이 \\(0\\) 이 아니더라도 절대값이 가장 큰 값을 찾아 교환한다. 예를 들어\n\\[\n\\boldsymbol{A}=\\begin{bmatrix} 1 & 3 & 2 \\\\ 0 & 2 & 4 \\\\ 4 & 1 & 3\\end{bmatrix}\n\\]\n일 경우, 첫번째 열벡터가 영벡터가 아니며, 이 열벡터의 첫번째 행이 \\(0\\) 이 아니지만 이 열벡터에 절대값이 가장 큰 \\(4\\) 가 성분으로 존재하므로 첫번째 행과 마지막 행을 교환한다. 이를 \\(\\boldsymbol{A}_{1\\_1}\\) 이라 하자. 이 때 선행 성분은 \\(4\\) 이다. \\[\n\\boldsymbol{A}_{1\\_1}=\\begin{bmatrix} \\underline{4} & 1 & 3\\\\ 1 & 3 & 2 \\\\ 0 & 2 & 4\\end{bmatrix}\n\\]\nP1-2. 선행성분으로 첫번째 행벡터를 나누어준다(\\(\\hat{L}_2\\) 연산). 이제 첫번째 행의 첫번째 성분은 1이다(선행 1 성분). 여기까지 수행한 행렬을 \\(\\boldsymbol{A}_{1\\_2}\\) 라고 하면 다음과 같다.\n\\[\n\\boldsymbol{A}_{1\\_2}=\\begin{bmatrix} \\underline{1} & 1/4 & 3/4 \\\\ 1 & 3 & 2 \\\\ 0 & 2 & 4\\end{bmatrix}.\n\\]\nP1-3. 첫번째 행벡터의 선행 1 성분의 열 위치를 \\(l_1\\) 이라 하자. \\(j=2,\\ldots,\\,m\\) 에 대해 \\(\\hat{L}_3 (j, i, -A_{j,l_1})\\) 를 적용하면 \\(l_1\\) 열은 첫번째 성분을 제외하면 모두 \\(0\\) 이 된다. 첫번째 행에 대한 마지막 과정이므로 \\(\\boldsymbol{A}_1\\) 이라 하면 다음과 같다.\n\\[\n\\boldsymbol{A}_1=\\begin{bmatrix} \\underline{1} & 1/4 & 3/4 \\\\ 0 & 3/4 & 5/4 \\\\ 0 & 2 & 4\\end{bmatrix}.\n\\]\n\n이제 행렬 \\(\\boldsymbol{A}_k\\) 가 1) \\(1\\) 행부터 \\(k\\) 행까지는 (\\(1\\)) 행 간소 사다리꼴 행렬이며, (\\(2\\)) \\(k\\) 행의 선행 1 성분의 위치가 \\(l_k\\) 열일 때 \\(k\\) 행 아래의 모든 행이 1열부터 \\(l_k\\) 열까지 \\(0\\) 이라고 하자. 이에 다음 Pk-1-1., Pk-1-2., Pk-1-1. 과정을 수행한다.\nPk+1-1. \\(k+1\\) 행부터 \\(m\\) 행까지만 생각한다. 여기서 첫번째 \\(0\\) 벡터가 아닌 열벡터를 찾는다. 당연히 이 열벡터는 \\(l_k\\) 열보다 오른쪽의 열이다. 이것을 \\(l_{k+1}\\) 열이라고 하자. 이 열에서 가장 절대값이 큰 성분의 행과 \\(k+1\\) 행을 교환한다 (\\(\\hat{L}_1\\) 연산). 만약 존재하지 않는다면 \\(k+1\\) 행부터 마지막 행 까지의 모든 행벡터가 영벡터이므로 과정을 더 이상 진행시키지 않는다.\nPk+1-2. 교환된 \\(k+1\\) 행을 가장 처음 \\(0\\) 이 아닌 성분으로 나눈다 (\\(\\hat{L}_2\\) 연산). 이제 첫번째 행의 첫번째 성분은 1이다.(선행 1 성분)\nPk+1-3. \\(k+1\\) 행이 마지막 행이라면 더 이상 진행하지 않는다. 그렇지 않다면 \\(j=1, \\ldots, \\, k, k+2,\\ldots,\\,m\\) 에 대해 \\(\\hat{L}_3 (j, k+1, -A_{j,l_1})\\) 를 적용하면 \\(l_{k+1}\\) 열 의 \\(k+2\\) 행 이하는 모두 \\(0\\) 이 된다.\n이 과정을 수행한 후 \\(1\\) 행부터 \\(k+1\\) 행까지 행 간소 사다리꼴 형태가 됨을 알 수 있다. 이제 \\(m \\times n\\) 행렬에 대해 1 행부터 \\(m\\) 행에 대해 위의 과정을 수행한다면 행렬의 행 간소 사다리꼴 형태를 얻을 수 있다.\n우리는 가역행렬의 행 간소 사다리꼴이 항등행렬임을 안다. \\(n \\times n\\) 가역행렬 \\(\\boldsymbol{A}\\) 와 \\(n\\) 차원 열벡터 \\(\\boldsymbol{b}\\) 에 대해 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 만족하는 열벡터 \\(\\boldsymbol{x}\\) 를 찾는다고 하자. \\(\\boldsymbol{A}\\) 가 기본 행 연산 \\(\\boldsymbol{E}_1, \\ldots,\\,\\boldsymbol{E}_N\\) 에 대해 행 간소 사다리꼴이 된다고 하면\n\\[\n\\begin{aligned}\n&\\boldsymbol{E}_N \\cdots \\boldsymbol{E}_1 \\boldsymbol{Ax} = \\boldsymbol{E}_N \\cdots \\boldsymbol{E}_1 \\boldsymbol{b} \\\\\n\\implies &\\boldsymbol{x} = \\boldsymbol{E}_N \\cdots \\boldsymbol{E}_1 \\boldsymbol{b}\n\\end{aligned}\n\\]\n이다. 이 때 \\(\\boldsymbol{E}=\\boldsymbol{E}_N \\cdots \\boldsymbol{E}_1\\) 라고 하면, \\(\\boldsymbol{EA}=\\boldsymbol{I}\\) 이므로,\n\\[\n\\boldsymbol{E} = \\boldsymbol{E}_N \\cdots \\boldsymbol{E}_1 = \\boldsymbol{A}^{-1}\n\\]\n이다. 즉 우리는 가우스-요르단 소거법을 통해 선형방정식을 풀 수 있을 뿐만 아니라, 역행렬도 구할 수 있다.\n이제 가우스-요르단 소거법을 수행하는 함수를 만들어 보자. 행렬 \\(\\boldsymbol{A}\\) 와 열행렬 혹은 행렬 \\(\\boldsymbol{B}\\) 에 대해 \\(\\boldsymbol{A}\\) 를 행 간소 사다리꼴로 만드는 프로세스를 \\(\\begin{bmatrix}\\boldsymbol{A} & \\boldsymbol{B}\\end{bmatrix}\\) 에 대해 수행하도록 한다. 다만 \\(\\boldsymbol{B}\\) 는 선택적으로 입력 가능하다. 함수 gauss_jordan_elimination 을 아래에 구현하였다. 여기서 eptols 는 절대값이 작은 수로 피보팅 할 때 절대값이 이 수보다 작다면 0 과 차이가 없도록 간주한다.\nfunction gauss_jordan_elimination(A::Matrix, b::Union{Nothing, Vector, Matrix}=nothing; eptols = 1.0e-10)\n    m, n = size(A)\n\n    if b ≠ nothing\n        @assert m == size(b)[1]\n        B = [A b]\n    else\n        B = A\n    end\n    \n    ld = 0 #선행 1 성분의 column index\n\n    for i in 1:m\n        termination = true # 종료 조건\n        for j in (ld+1):n\n            p = argmax(abs.(B[i:end, j])) + i -1\n            \n            if abs(B[p, j]) &gt; eptols\n                B[i,:], B[p, :] = B[p, :], B[i,:]\n                ld = j\n                termination = false\n                break\n            end\n        end\n\n        if termination \n            break\n        end\n        \n        B[i, :] .= B[i, :]./B[i, ld]\n        \n        # 선행 1 성분의 열을 자신을 제외하고는 제거\n        for k in 1:m\n            if k ≠ i \n                B[k, :] .= B[k, :] .- (B[k, ld].* B[i, :])\n            end\n        end\n    end\n\n    if b ≠ nothing \n         return B[:, 1:n], B[:,(n+1):end]\n    else \n         return B\n    end\nend\n\n\n\n3.3 계산 복잡도 분석\n우선 피보팅 없이 \\(n \\times n\\) 행렬 \\(\\boldsymbol{A}\\) 와 \\(n\\) 차원 벡터 \\(\\boldsymbol{b}\\) 에 대해 \\(\\boldsymbol{Ax}=b\\) 를 만족하는 \\(\\boldsymbol{x}\\) 를 가우스-요르단 소거법을 통해 구하는 데 필요한 계산복잡도를 알아보자. 우선 \\(n\\times (n+1)\\) 행렬 \\(\\begin{bmatrix} \\boldsymbol{A} & \\boldsymbol{b}\\end{bmatrix}\\) 에 대해 수행하므로,\n우선 \\(k\\) 번째 행에 대해\n\n\\(k\\) 행을 대각성분 \\(A_{k,k}\\) 으로 나누어 주는데 \\(n-k+2\\) 번의 연산이 필요하며,\n\\(k\\) 행 아래의 \\(n-k\\) 개의 행에 대해 \\(\\hat{L}_3\\) 연산을 통해 \\(k\\) 열의 성분을 대각성분을 제외하고 모두 \\(0\\) 으로 만드는데, 각각 나누기 한번과 빼기 한번, 두번의 연산이 소요되므로, \\(2(n-k)(n-k+2)\\) 번의 스칼라 사칙연산이 필요하다\n\n따라서\n\\[\nT(n) = \\sum_{k=1}^n (n-k+2) + 2(n-k)(n-k+2) = O\\left( \\frac{2}{3}n^3 \\right) = O(n^3)\n\\]\n이다.",
    "crumbs": [
      "수치해석 I",
      "수치해석 입문 : 선형시스템과 다항식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/03_linear_system.html#sec-lu_decomposition",
    "href": "src/numerical_analysis_using_julia/03_linear_system.html#sec-lu_decomposition",
    "title": "수치해석 입문 : 선형시스템과 다항식",
    "section": "4 LU 분해",
    "text": "4 LU 분해\n선형방정식 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 의 해를 수치해석적으로 얻는 가장 기본적인 방법이 LU 분해(LU decomposition, LU factorization) 이다. 또한 행렬의 행렬식과 역행렬을 구하는 가장 기본적인 방법이 LU 분해를 통해 구하는 것이다.\n\n\n4.1 LU 분해 (LU decomposition)\n행렬 \\(\\boldsymbol{A}\\) 를 어떤 하삼각행렬 \\(\\boldsymbol{L}\\) 과 상삼각행렬 \\(\\boldsymbol{U}\\) 의 곱으로 다음과 같이 나타내는 것을 LU 분해 (LU decomposition 혹은 LU factorization) 이라고 한다. LU 분해 자체는 정사각 행렬이 아니더라도 가능하다.\n\\[\n\\boldsymbol{A}=\\boldsymbol{LU}\n\\]\n\\(2 \\times 2\\) 행렬의 예를 보자.\n\\[\n\\boldsymbol{A}=\\begin{bmatrix} 2 & 3 \\\\ 2 & 4 \\end{bmatrix} =\\boldsymbol{LU}=\\begin{bmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{bmatrix} \\begin{bmatrix} u_{11} & u_{12} \\\\ 0 & u_{22} \\end{bmatrix}\n\\]\n이 경우,\n\\[\n\\begin{aligned}\nl_{11}u_{11} & = 2, \\\\\nl_{11}u_{12} &= 3 , \\\\\nl_{21}u_{11} &= 2 , \\\\\nl_{21}u_{12} + l_{22}u_{22} & = 4,\n\\end{aligned}\n\\]\n의 네 개의 식이 나온다. 미지수 6개에 식이 4개이므로 미지수를 결정 할 수 없다. 만약 \\(l_{11}= l_{22}=1\\) 의 제한조건을 걸어 놓고 계산을 하면,\n\\[\n\\boldsymbol{L} = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1\\end{bmatrix},\\qquad \\boldsymbol{U} = \\begin{bmatrix} 2 & 3 \\\\ 0 &1\\end{bmatrix}\n\\]\n이라는 것을 알 수 있다. 일반적인 LU 분해에서도 하삼각행렬 \\(\\boldsymbol{L}\\) 의 대각성분을 1로 고정시켜 구한다.\n다음 행렬에 대한 \\(LU\\) 분해를 생각하자.\n\\[\n\\boldsymbol{B}=\\begin{bmatrix} 0 & 3 \\\\ 2 & 4 \\end{bmatrix} =\\boldsymbol{LU}=\\begin{bmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{bmatrix} \\begin{bmatrix} u_{11} & u_{12} \\\\ 0 & u_{22} \\end{bmatrix}\n\\]\n이 경우,\n\\[\n\\begin{aligned}\nl_{11}u_{11} & = 0, \\\\\nl_{11}u_{12} &= 3 , \\\\\nl_{21}u_{11} &= 2 , \\\\\nl_{21}u_{12} + l_{22}u_{22} & = 4,\n\\end{aligned}\n\\]\n이며, \\(u_{11}=0\\) 이므로 \\(l_{21}\\) 값을 정할 수 없다. \\(l_{11}=l_{22}=1\\) 이라는 제한조건을 푼다고 해도 마찬가지 이다. 이 경우 우리가 가우스 소거법에서 수행했던 행 교환 연산인 피보팅을 한다. 1행과 2행을 바꾼 행렬을 \\(\\boldsymbol{B}\\) 라고 하면,\n\\[\n\\boldsymbol{B}'=\\begin{bmatrix} 2 & 4 \\\\ 0 & 3 \\end{bmatrix} =\\boldsymbol{LU}=\\begin{bmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{bmatrix} \\begin{bmatrix} u_{11} & u_{12} \\\\ 0 & u_{22} \\end{bmatrix}\n\\]\n이 경우,\n\\[\n\\begin{aligned}\nl_{11}u_{11} & = 2, \\\\\nl_{11}u_{12} &= 4 , \\\\\nl_{21}u_{11} &= 0 , \\\\\nl_{21}u_{12} + l_{22}u_{22} & = 3,\n\\end{aligned}\n\\]\n이므로\n\\[\n\\boldsymbol{L} =  \\begin{bmatrix}1 &  0 \\\\ 0 & 1 \\end{bmatrix},\\qquad \\boldsymbol{U} =  \\begin{bmatrix} 2 & 4 \\\\ 0 & 3 \\end{bmatrix}\n\\]\n로 LU 분해가 가능하다. 즉 \\(\\boldsymbol{P} =\\begin{bmatrix} 0 & 1 \\\\ 1 & 0\\end{bmatrix}\\) 에 대해 \\(\\boldsymbol{PB}\\) 가 LU 분해가 가능하다. 이렇게 피보팅까지 포함하여 LU 분해를 수행하는 것을 PLU 분해라고 하며 보통 LU 분해를 구현하는 경우 PLU 분해를 포함하여 구현할 수 밖에 없다. 이제부터는 LU 분해는 항상 PLU 분해를 의미한다.\n\n\n\n4.2 LU 분해를 이용한 선형방정식의 풀이 및 행렬식\n\\(\\boldsymbol{A}\\) 가 LU 분해 가능이고 \\(\\boldsymbol{PA}=\\boldsymbol{LU}\\) 로 분해되었다고 하자. \\(\\boldsymbol{A}\\) 가 가역행렬이 아니라면 분해가 의미가 없으므로 가역행렬일 때만 생각한다. \\(\\boldsymbol{P}\\) 는 치환행렬의 곱이며 각각의 치환행렬은 가역행렬이므로 \\(\\boldsymbol{P}\\) 도 가역행렬이다.\n\\[\n\\boldsymbol{Ax}=\\boldsymbol{b} \\iff \\boldsymbol{PAx}=\\boldsymbol{Pb} \\iff \\boldsymbol{LUx}=\\boldsymbol{Pb}\n\\]\n가 성립한다. 여기서 우리는 \\(\\boldsymbol{y}=\\boldsymbol{Ux}\\) 라고 놓고 우선 \\(\\boldsymbol{Ly}=\\boldsymbol{Pb}\\) 를 통해 \\(\\boldsymbol{y}\\) 를 구한 후, \\(\\boldsymbol{Ux}=\\boldsymbol{y}\\) 를 풀어서 우리가 구하고자 하는 \\(\\boldsymbol{x}\\) 를 구한다. 가우스-요르단 소거법으로 선형 시스템을 풀기 위해서는 \\(\\boldsymbol{b}\\) 값이 바뀔 때마다 소거법을 수행해야 하지만, LU 분해나 PLU 분해는 \\(\\boldsymbol{A}\\) 에 대해서만 분해 한 후 삼각행렬에 대한 식을 풀면 되기 때문에 훨씬 간단하다. 또한 뒤에 보겠지만 대각성분으로 나눠주는 항이 없기 때문에 roundoff 에러로부터 더 안전하다.\n행렬식 \\(\\det\\) 를 구하는 데도 사용된다. LU 분해 시 \\(\\boldsymbol{L}\\) 의 모든 대각성분을 \\(1\\) 로 고정시키기 때문에,\n\\[\n\\det(\\boldsymbol{A}) = \\det (\\boldsymbol{P}) \\det(\\boldsymbol{U}) = (-1)^n(P) \\prod_{i=1}^n U_{ii}\n\\]\n이다. 여기서 \\(n(P)\\) 는 \\(\\boldsymbol{P}\\) 에 나타나는 치환의 횟수이다.\n\n\n\n4.3 PLU 분해\n첫번째 행에 대한 피보팅을 수행하는 치환행렬을 \\(\\boldsymbol{P}_1\\) 이라고 하자. 행렬의 \\(k\\) 행에 대해 피보팅 할 때마다 \\(\\boldsymbol{L}\\) 의 \\(k\\) 행과 \\(\\boldsymbol{U}\\) 의 \\(k\\) 열을 결정할 수 있다는 것을 보이자. 여기서 \\(\\boldsymbol{L}\\) 의 대각 성분은 모두 \\(1\\) 이다.\n\\[\n\\boldsymbol{A}_1=\\boldsymbol{P}_1\\boldsymbol{A}= \\boldsymbol{L}_1\\boldsymbol{U}_1\n\\]\n을 생각하자. \\([\\boldsymbol{L}_1]_j= \\delta_{1j}\\) 이므로 \\([\\boldsymbol{P}_1\\boldsymbol{A}]_{1j} = \\sum_{k}[\\boldsymbol{L}]_{1k} [\\boldsymbol{U}_1]_{kj} = [\\boldsymbol{U}]_{1j}\\) 이므로 \\(\\boldsymbol{U}_1\\) 의 첫번째 행이 정해진다.\n이제 두번째 행에 대한 피보팅을 수행한다.\n\\[\n\\boldsymbol{P}_2\\boldsymbol{A}_1=\\boldsymbol{P}_2\\boldsymbol{P}_1\\boldsymbol{A}= \\boldsymbol{L}_2\\boldsymbol{U}_2\n\\]\n\\(\\boldsymbol{P}_2\\) 에 의해 앞서 정해진 \\(\\boldsymbol{L}_2\\) 의 첫번째 행은 영항을 받지 않는다. \\(\\boldsymbol{L}_2\\) 와 \\(\\boldsymbol{U}_2\\) 의 첫번째 행은 \\(\\boldsymbol{L}_1\\) 과 \\(\\boldsymbol{U}_1\\) 의 첫번째 행과 같고 여기서는 \\(\\boldsymbol{L}_2\\) 와 \\(\\boldsymbol{U}_2\\) 의 두번째 행을 결정한다.\n\\[\n[\\boldsymbol{P}_2\\boldsymbol{A}_1]_{2j} = \\sum_{k=1}^n [\\boldsymbol{L}_2]_{2k}[\\boldsymbol{U}_2]_{kj} = [\\boldsymbol{L}_2]_{21}[\\boldsymbol{U}_2]_{1j} + [\\boldsymbol{U}_2]_{2j}\n\\]\n이다. \\([\\boldsymbol{U}_2]_{21}=0\\) 이므로 \\([\\boldsymbol{L}_2]_{21}=  [\\boldsymbol{P}_2\\boldsymbol{A}_1]_{21}/[\\boldsymbol{U}_2]_{11}\\) 이며 \\(j&gt;2\\) 에 대해 \\([\\boldsymbol{U}_2]_{2j} =  [\\boldsymbol{P}_2\\boldsymbol{A}_1]_{2j} - [\\boldsymbol{L}]_{21}[\\boldsymbol{U}_2]_{1j}\\) 이다. 이렇게 \\(\\boldsymbol{L}_2\\) 와 \\(\\boldsymbol{U}_2\\) 의 두번째 행을 구했다. 이 방법을 \\(n\\) 행 까지 계속하면\n\\[\n\\boldsymbol{P}_n \\cdots \\boldsymbol{P}_1 \\boldsymbol{A}= \\boldsymbol{L}_n \\boldsymbol{U}_n = \\boldsymbol{LU}\n\\]\n로 PLU 분해를 수행 할 수 있다.\nfunction PLU(A::Matrix{T}; eptols = 1.0e-10) where T&lt;:Real\n    M, N = size(A)\n    @assert M == N\n\n    L, P, U = one(A), one(A), zero(A)\n    B = copy(A)\n    \n    for i in 1:(M-1)\n        p = argmax(abs.(B[i:end, i])) + i -1\n        \n        if abs(B[p, i]) &lt; eptols\n            error(\"Singularity error\")    \n        end\n\n        P[i,:], P[p, :] = P[p, :], P[i, :]\n        B[i,:], B[p, :] = B[p, :], B[i, :]\n        \n        if i&gt;1\n            L[i,1:i-1], L[p, 1:i-1] = L[p, 1:i-1], L[i, 1:i-1]\n        end\n        \n        U[i, i] = B[i, i]\n        U[i, (i+1):end] = B[i, (i+1):end]\n        L[(i+1):end, i] = B[(i+1):end, i] / B[i, i]\n        B[(i+1):end, (i+1):end] = B[(i+1):end, (i+1):end] - (L[(i+1):end, i:i] * U[i:i, (i+1):end])\n    end\n    U[M, M] = B[M, M]\n    return P, L, U\nend\n\n\n\n\n\n\n경고\n\n\n\n이 코드는 이해를 돕기 위한 코드로, PLU 를 정상적으로 계산 해 주지만 효율적인 코드는 아니다.\n\n\n\n\n\n4.4 복잡도 분석\n피봇을 고려하지 않은 복잡도를 분석해보자. \\(m\\times m\\) 행렬에 대해 첫번째 열의 각 열에 대한 연산은 \\(m\\) 번의 곱하기(코딩상으로는 나누기) 와 \\(m\\) 번의 더하기(코딩상으로는 빼기) 가 이루어지며 총 \\(m-1\\) 번의 행에 대해 이루어지기 때문에 \\(2m(m-1)\\) 번의 연산이 이루어진다. \\(n\\times n\\) 행렬에 대해서라면 \\(m\\) 값이 \\(2\\) 부터 \\(n\\) 까지 변할때의 합이므로\n\\[\nT(n) = \\sum_{m=2}^{n} 2m(m-1) = \\sum_{m=1}^n 2m(m-1)= O\\left(\\dfrac{2n^3}{3}\\right)\n\\]\n이다. 즉 가우스 소거법과 계산복잡도는 같다.\n\n\n\n4.5 왜 LU 인가?\n많은 경우 선형시스템을 푼다는 것은 시스템 행렬 \\(\\boldsymbol{A}\\) 가 주어진 상태에서 \\(\\boldsymbol{b}\\) 가 변함에 따라 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 만족하는 \\(\\boldsymbol{x}\\) 를 찾는다. 가우스 소거법을 이용하는 경우라면 매번 \\(O(n^3)\\) 복잡도의 계산을 해 주어야 한다. LU 분해의 경우 \\(\\boldsymbol{A}=\\boldsymbol{LU}\\) (혹은 \\(\\boldsymbol{A}=\\boldsymbol{P}^{-1}\\boldsymbol{LU}\\)) 에 대해 \\(\\boldsymbol{L}\\), \\(\\boldsymbol{U}\\) 가 정해져 있기 때문에 각각 한번의 상삼각행렬과 하삼각행렬에 대해 풀어주면 된다. 상삼각 행렬과 하삼각 행렬의 복잡도는 \\(O(n^2)\\) 이므로 주어진 시스템 행렬에 대해 많은 계산을 할 때는 LU 분해가 훨씬 유리하며, 따라서 기본적으로 선형방정식의 해는 LU 분해를 사용한다.\n\n\n\n4.6 LDU 분해\nLU 분해에서 상삼각해렬 \\(\\boldsymbol{U}\\) 를 대각행렬 \\(\\boldsymbol{D}\\) 와 대각성분이 \\(1\\) 인 상삼각행렬 \\(\\boldsymbol{U}'\\) 의 곱으로 나타 낼 수 있다. 이를 LDU 분해라고 한다. \\[\n\\underbrace{\\begin{bmatrix} U_{11} & U_{12} & \\cdots & U_{1n} \\\\ 0 & U_{22} & \\cdots & U_{2n} \\\\ & & \\ddots & \\\\ 0 & 0 & \\cdots & U_{nn}\\end{bmatrix}}_{\\Large\\boldsymbol{U}} = \\underbrace{\\begin{bmatrix} U_{11} &  &  & 0 \\\\   & U_{22} &  &  \\\\ & & \\ddots & \\\\ 0& & & U_{nn}\\end{bmatrix}}_{\\Large{\\boldsymbol{D}}} \\underbrace{\\begin{bmatrix} 1 & U_{12}/U_{11}& \\cdots & U_{1n}/U_{11} \\\\ 0 & 1 & \\cdots & U_{2n}/U_{22} \\\\ & & \\ddots & \\\\ 0 & 0 & \\cdots & 1\\end{bmatrix}}_{\\Large\\boldsymbol{U}'}\n\\]",
    "crumbs": [
      "수치해석 I",
      "수치해석 입문 : 선형시스템과 다항식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/03_linear_system.html#qr-분해-qr-factorization",
    "href": "src/numerical_analysis_using_julia/03_linear_system.html#qr-분해-qr-factorization",
    "title": "수치해석 입문 : 선형시스템과 다항식",
    "section": "5 QR 분해 (QR-Factorization)",
    "text": "5 QR 분해 (QR-Factorization)\n\\(m \\times n\\) 행렬 \\(\\boldsymbol{A}\\) 을 \\(m \\times m\\) 행렬 \\(\\boldsymbol{Q}\\) 와 \\(m\\times n\\) 상 삼각행렬 \\(\\boldsymbol{R}\\) 분해하여 \\(\\boldsymbol{A}=\\boldsymbol{QR}\\) 로 나타내는 것을 QR 분해라 한다. 이 때 \\(\\boldsymbol{Q}\\) 행렬의 각 열벡터는 서로 직교하며, 그 크기가 \\(1\\) 이다. \\(m=n\\) 이면 \\(\\boldsymbol{Q}\\) 가 유니터리행렬(unitary matrix) 로 \\(\\boldsymbol{Q}\\boldsymbol{Q}^\\ast = \\boldsymbol{I}\\) 가 된다. 행렬 \\(\\boldsymbol{A}\\) 가 실수체에서 정의되었다면 \\(\\boldsymbol{Q}\\) 행렬은 직교행렬로 \\(\\boldsymbol{Q}\\boldsymbol{Q}^T = \\boldsymbol{I}\\) 이다.\n보통 이론적으로 QR 분해를 설명할 때는 그람-슈미트 과정(Gram-Schmidt process)을 사용하지만, 실제 수치해석적으로 구할 때는 Householder reflection 방법을 사용하거나 기븐스 회전(Givens rotation)을 사용한다. 여기서는 그람-슈미트 과정을 통한 QR 분해를 구현해보기로 한다.\n\n\n5.1 정사영 (Projection)\n\\(\\mathbb{F}\\) 에서 정의된 내적 벡터공간 \\(V\\) 의 기저 \\(\\{\\boldsymbol{u}_1,\\,\\boldsymbol{u}_2,\\ldots,\\boldsymbol{u}_n\\}\\) 이 \\(\\langle \\boldsymbol{u}_i,\\,\\boldsymbol{u}_j \\rangle=\\delta_{ij}\\) 를 만족할 때 이 기저를 정규직교기저 (orthonormal basis) 라 한다. 내적이 정의되면 임의의 기저로부터 항상 정규직교기저를 구할 수 있으며, 이중 가장 유명한 방법이 그람-슈미트 방법이다.\n벡터 \\(\\boldsymbol{v}\\) 의 \\(\\boldsymbol{u}\\) 에 대한 정사영 \\(\\textrm{Proj}_\\boldsymbol{u} \\boldsymbol{v}\\) 는 다음과 같이 정의된다.\n\\[\n\\textrm{Proj}_{\\boldsymbol{u}}\\boldsymbol{v} := \\dfrac{\\langle\\boldsymbol{v},\\, \\boldsymbol{u}\\rangle}{\\langle \\boldsymbol{u,\\, u}\\rangle} \\boldsymbol{u}\n\\]\n\\(\\text{Proj}_\\boldsymbol{u}\\boldsymbol{v}\\) 는 \\(\\boldsymbol{u}\\) 에 평행하며 \\(\\boldsymbol{v}-\\text{Proj}_{\\boldsymbol{u}}\\boldsymbol{v}\\) 는 \\(\\boldsymbol{u}\\) 에 수직하다. 즉\n\\[\n\\langle  \\boldsymbol{u} , \\,\\boldsymbol{v}-\\text{Proj}_{\\boldsymbol{u}}\\boldsymbol{v} \\rangle  = \\boldsymbol{0}\n\\]\n이다. \\(\\boldsymbol{v} = \\text{Proj}_\\boldsymbol{u}\\boldsymbol{v} + (\\boldsymbol{v} - \\text{Proj}_{\\boldsymbol{u}}\\boldsymbol{v})\\) 이므로 \\(\\boldsymbol{v}\\) 를 \\(\\boldsymbol{u}\\) 와 평행한 성분과 \\(\\boldsymbol{u}\\) 에 수직한 성분으로 분리할 수 있다는 것을 알게 되었다. 단위벡터 \\(\\boldsymbol{e} = \\dfrac{\\boldsymbol{u}}{\\|\\boldsymbol{u}\\|}\\) 를 생각하면,\n\\[\n\\text{Proj}_{\\boldsymbol{u}}\\boldsymbol{v} = \\langle \\boldsymbol{v },\\,\\boldsymbol{e}\\rangle\\, \\boldsymbol{e}\n\\]\n이다.\n\n\n\n5.2 그람-슈미트 과정\n그람-슈미트 과정을 통해 유한차원 내적 벡터공간에서 주어진 독립 벡터를 이용하여 같은 갯수의 정규 직교 벡터를 얻을 수 있다. \\(N\\) 차원 내적 벡터 공간 \\(V\\) 에서 \\(M\\) 개의 독립벡터 \\(\\{\\boldsymbol{v}_1,\\ldots,\\boldsymbol{v}_M\\}\\) 가 주어졌다고 하자. (당연히 \\(M \\le N\\) 이다). 다음을 이용하여 \\(\\hat{\\boldsymbol{u}}_1,\\ldots,\\,\\hat{\\boldsymbol{u}}_M\\) 을 얻을 수 있다. 이를 그람-슈미트 과정이라고 한다.\n\\[\n\\begin{aligned}\n\\boldsymbol{u}_1 &=  \\boldsymbol{v}_1, \\hat{\\boldsymbol{u}}_1 = \\dfrac{\\boldsymbol{u}_1}{\\|\\boldsymbol{u}_1\\|}, \\\\\n\\boldsymbol{u}_i &= \\boldsymbol{v}_{i} - \\sum_{j=1}^{i-1} \\text{Proj}_{\\boldsymbol{u}_j} \\boldsymbol{v}_i = \\boldsymbol{v}_i-\\sum_{j=1}^{i-1} \\left\\langle \\boldsymbol{v}_i,\\,\\hat{\\boldsymbol{u}}_j \\right\\rangle\\hat{\\boldsymbol{u}}_j,\\qquad \\hat{\\boldsymbol{u}}_i = \\dfrac{\\boldsymbol{u}_i}{\\|\\boldsymbol{u}_i\\|}\n\\end{aligned}\n\\]\n그람 슈미트 과정에 대해 다음 명제가 성립함을 안다.\n\n연습문제 1 독립벡터의 집합 \\(\\{\\boldsymbol{v}_1,\\ldots,\\boldsymbol{v}_M\\}\\) 로부터 그람 슈미트 과정을 통해 얻은 \\(\\{\\hat{\\boldsymbol{u}}_1,\\ldots,\\,\\hat{\\boldsymbol{u}}_M\\}\\) 는 각각 단위행렬이며 서로 직교한다. 즉 \\(\\hat{\\boldsymbol{u}}_i \\cdot \\hat{\\boldsymbol{u}}_j = \\delta_{ij}\\) 이다.\n\n\n연습문제 2 벡터의 집합 \\(\\{\\boldsymbol{v}_1,\\ldots,\\boldsymbol{v}_M\\}\\) 에서 \\(\\boldsymbol{v}_k\\) 를 \\(\\boldsymbol{v}_1,\\ldots,\\boldsymbol{v}_{k-1}\\) 의 선형결합으로 표현할 수 있을 때 그람-슈미트 과정을 통해 얻은 벡터는 영벡터이다.\n\n\n\n\n5.3 그람 슈미트 과정을 이용한 QR 분해 (QR decomppsition)\n이제 우리는 주어진 독립벡터들로 정규직교벡터를 구성하는 법을 배웠다. 여기서는 \\(m \\times n\\) 행렬 \\(\\boldsymbol{A}\\) 의 열벡터 \\(\\boldsymbol{A}_{:1},\\ldots\\), \\(\\boldsymbol{A}_{:n}\\) 에 대해 그람-슈미트 과정을 수행한다고 하자. \\[\n\\begin{aligned}\n\\boldsymbol{u}_1 &=  \\boldsymbol{A}_{:1},\\qquad \\hat{\\boldsymbol{u}}_1 = \\dfrac{\\boldsymbol{u}_1}{\\|\\boldsymbol{u}_1\\|}, \\\\\n\\boldsymbol{u}_j &= \\boldsymbol{A}_{:j} - \\sum_{k=1}^{j-1} \\text{Proj}_{\\boldsymbol{u}_k} \\boldsymbol{A}_{:j} = \\boldsymbol{A}_{:j}-\\sum_{k=1}^{j-1} \\left\\langle \\boldsymbol{A}_{:j},\\,\\hat{\\boldsymbol{u}}_k \\right\\rangle\\hat{\\boldsymbol{u}}_k,\\qquad \\hat{\\boldsymbol{u}}_j = \\dfrac{\\boldsymbol{u}_j}{\\|\\boldsymbol{u}_j\\|}\n\\end{aligned}\n\\] 라 하면,\n\\[\nA_{ij}=(\\boldsymbol{A}_{:j})_i = \\sum_{k=1}^n \\left(\\langle \\boldsymbol{A}_{:j},\\,\\hat{\\boldsymbol{u}}_k\\rangle \\hat{\\boldsymbol{u}}_k \\right)_i\n\\] 이다. 이 때 \\(\\boldsymbol{Q},\\, \\boldsymbol{R}\\) 을 다음과 같이 정의하면 \\(\\boldsymbol{A}= \\boldsymbol{QR}\\) 이 된다.\n\\[\nQ_{ik} := (\\hat{\\boldsymbol{u}}_k)_i,\\qquad R_{kj} := \\langle \\boldsymbol{A}_{:j},\\,\\hat{\\boldsymbol{u}}_k\\rangle\n\\]\n즉 \\(\\boldsymbol{Q}\\) 의 \\(i\\) 번째 열벡터는 \\(\\boldsymbol{A}\\) 의 각각의 열벡터에 대해 그람-슈미트 과정을 수행했을 때의 단위벡터 혹은 영벡터(연습문제 2) 이며 \\(\\boldsymbol{R}\\) 은 그람-슈미트 과정에서의 계수이다. 아래는 그람-슈미트 과정에 의한 QR 분해를 구현한 코드이다.\nfunction qr_gram_schmidt(A::AbstractMatrix{T}, normeps=1.0e-14) where T&lt;:Number\n    M, N = size(A)\n    Q = zeros(Float64, (M, N))\n    R = zeros(Float64, (N, N))\n    \n    Q[:,1] = A[:,1]/norm(A[:,1])\n    R[1,1] = dot(A[:,1], Q[:,1])\n\n    for j = 2:N\n        Uj = A[:,j] \n        for k = 1:j-1\n            R[k, j] = dot(A[:,j], Q[:, k])\n            Uj = Uj .- R[k, j] .* Q[:,k]\n        end\n        if norm(Uj)&gt;normeps\n            Q[:,j]= Uj/norm(Uj)\n            R[j, j] = dot(A[:,j], Q[:, j])\n        end \n    end\n    return Q, R\n\nend\n\n여기서 norm(A) 는 벡터의 노름을 구하는데 사용되었으며, 정확히는 norm(A, p::Real=2) 의 형태로 벡터, 혹은 행렬의 \\(p\\)-노름을 구하는데 사용되는 함수이다. 앞선 벡터들의 선형결합인 벡터는 영벡터가 되어야 하지만 Roundoff 에러로 인해 0 이 아닌 작은 노름을 가질 수 있으므로, 함수의 normeps 보다 작은 값을 가질 경우 영벡터로 간주한다.\n실제로는 그람-슈미트 방법을 이용한 QR-분해는 잘 사용되지 않는데, round-off 에러가 발생하여 수치해석적으로 불안정하기 때문이다. 보통은 밀집 행렬의 경우 하우스홀더 변환을 통한 방법을 사용한다.",
    "crumbs": [
      "수치해석 I",
      "수치해석 입문 : 선형시스템과 다항식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/04_matrix_algebra.html",
    "href": "src/numerical_analysis_using_julia/04_matrix_algebra.html",
    "title": "Julia 에서의 행렬 계산",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]\n지금까지는 Julia 의 기본적인 벡터와 행렬, 그리고 연산을 이용하여 일반적인 행렬에 대한 가우스 소거법과 LU 분해, QR 분해를 구현하여 보았다. Julia 는 큰 벡터와 행렬을 다루기 위한 SparseArrays 모듈과 행렬 및 벡터 계산을 위한 LinearAlgebra 모듈을 기본으로 제공한다. 또한 다양한 형태의 행렬에 대한 특별한 연산도 제공한다. 이제 이런 기능들을 알아보도록 하자. 그 전에 벡터와 행렬의 크기 혹은 거리를 의미하는 노름을 먼저 알아보기로 하자.",
    "crumbs": [
      "수치해석 I",
      "Julia 에서의 행렬 계산"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/04_matrix_algebra.html#sec-norm_of_vector_and_matrix",
    "href": "src/numerical_analysis_using_julia/04_matrix_algebra.html#sec-norm_of_vector_and_matrix",
    "title": "Julia 에서의 행렬 계산",
    "section": "1 벡터와 행렬의 노름과 조건수",
    "text": "1 벡터와 행렬의 노름과 조건수\n\n1.1 벡터의 노름\n벡터의 노름(norm) 은 벡터의 크기에 대한 척도로서 실수, 혹은 복소수의 절대값과 비슷한 역할을 한다. 보통 \\(n\\) 차원 유클리드 공간에서 사용하는 거리도 노름의 일종이다. 벡터공간에서의 노름은 특정한 성질을 만족하는 함수 가운데 선택된다. 내적벡터공간에서는 내적에 의해 자연스럽게 노름을 정의할 수 있지만 노름 자체는 집합에 부여되는 성질이며 내적과 관계 없이 노름을 정의할 수도 있다. 즉 어떤 집합에 위의 노름이 정의되면 노름공간이라고 한다. \n벡터의 노름에 대한 성질을 알기 위해 다음 부등식을 알 필요가 있다.\n\n\n명제 1 (횔더부등식(Hölder’s inequality) 과 민코프스키 부등식(Minkowski’s ineqality)) \\(p\\ge 1\\) 이며 \\(1/p+1/q =1\\) 일 때 다음이 성립한다. \\[\n\\begin{aligned}\n\\text{H\\\"older's inequality} & \\sum_{i=1}^n |u_i v_i| \\le \\left(\\sum_{i=1}^n |u_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |v_i|^q\\right)^{1/q}, \\\\\n\\text{Mincowski's inequality} & \\left(\\sum_{i=1}^n |u_i+v_i|^p\\right)^{1/p} \\le \\left(\\sum_{i=1}^n |u_i|^p\\right)^{1/p} + \\left(\\sum_{i=1}^n |v_i|^p\\right)^{1/p}.\n\\end{aligned}\n\\]\n\n\n\n\n(증명). 수학적 증명 의 Hölder 부등식 를 참고하라.\n\n\n\n\n\n\n\n\n\n정의 1 (\\(L_p\\)-노름 과 \\(\\infty\\)-노름) \\(p&gt;1\\) 에 대해 아래와 같이 정의된 \\(\\| \\cdot \\|_p : \\mathbb{R}^n \\to [0, \\infty)\\) 는 노름이며 이 노름을 \\(L_p\\)-노름 이라고 한다.\n\\[\n\\|\\boldsymbol{v}\\|_p := \\left( \\sum_{i=1} |v_i|^p \\right)^{1/p}.\n\\]\n\n\n\n\n\n\\(p&gt;1\\) 인 실수에 대해 사용할 수 있으며 보통 \\(p=1,\\,2,\\,\\infty\\) 인 경우를 많이 사용한다. \\(p=1\\) 인 경우는 맨해튼 노름(Manhattan norm) 혹은 taxicab norm 이라고 하며,\n\\[\n\\|\\boldsymbol{v}\\|_1 = \\sum_i |v_i|\n\\]\n이다. \\(p=2\\) 인 경우는 우리가 많이 사용하는 유클리드 노름(Euclidean norm) 으로\n\\[\n\\|\\boldsymbol{v}\\|_2 = \\sqrt{\\sum_i |v_i|^2}\n\\]\n이다. \\(p=\\infty\\) 인 경우는 상한 노름(maximum norm) 이라고 하며\n\\[\n\\|\\boldsymbol{v}\\|_{\\infty} = \\max_{i=1,\\ldots} |x_i|\n\\]\n이다. 혹은 단순히 \\(L_1,\\,L_2,\\,L_\\infty\\) 노름 이라고도 한다.\n\n다음은 쉽게 증명 할 수 있다.\n\n\n\n명제 2 \\(p=\\infty\\) 를 포함하여 \\(p \\ge 1\\) 일 때, 유한차원 벡터공간에서의 \\(L_p\\)-노름은 노름의 정의를 만족한다.\n\n\n\n\n\n\n\n\n\n\n정의 2 (노름의 동등함) 벡터공간 \\(V\\) 에 두개의 노름 \\(\\|\\cdot \\|_\\alpha\\) 와 \\(\\|\\cdot \\|_\\beta\\) 가 정의되었을 때, 모든 \\(\\boldsymbol{v}\\in V\\) 에 대해 어떤 상수 \\(c_1,\\,c_2\\) 가 존재하여\n\\[\nc_1 \\|\\boldsymbol{v}\\|_\\alpha \\le \\|\\boldsymbol{v}\\|_\\beta \\le c_2 \\|\\boldsymbol{v}\\|_\\alpha\n\\]\n라면 이 두 노름을 동등하다(equivalent) 라고 한다. 유한차원 벡터공간에 대해서는 모든 노름이 동등하다는 것이 알려져 있다.\n\n\n\n\n\n\n\n명제 3 \\(\\boldsymbol{v}\\in \\mathcal{M}_n(\\mathbb{F})\\) 에 대해 다음이 성립한다. 즉 \\(\\|\\cdot \\|_\\infty\\) 와 \\(\\|\\cdot \\|_2\\) 는 동등하다.\n\\[\n\\|\\boldsymbol{v}\\|_{\\infty} \\le \\|\\boldsymbol{v}\\|_2 \\le \\sqrt{n}\\|\\boldsymbol{v}\\|_\\infty\n\\]\n\n\n\n\n(증명). \\(v_M = \\max \\{ |v_i|, i=1,\\ldots,\\,v_n\\}\\) 이라고 하자. \\[\n\\|\\boldsymbol{v}\\|_\\infty = v_M \\le \\sqrt{\\sum_{i=1}^n |v_i|^2} = \\|\\boldsymbol{v}\\|_2 \\le \\sqrt{\\sum_{i=1}^n {v_M}^2} = \\sqrt{n} v_M = \\sqrt{n} \\|\\boldsymbol{v}\\|_\\infty\n\\]\n이다. \\(\\square\\)\n\n\nJulia 에서는 LinearAlgebra 모듈의 norm(v, p=2) 로 정의되는 함수를 이용하여 계산 할 수 있으며 p 는 1, 2, Inf 가 가능한데 각각 \\(L_1,\\,L_2,\\,L_\\infty\\) 노름을 의미한다. 노름이 정의된 벡터공간에서 벡터를 그 크기로 나누는 것 (\\(\\boldsymbol{v}/\\|\\boldsymbol{v}\\|\\)) 을 정규화(normalization) 라고 하며 역시 LinearAlgebra 모듈에 normalize(v, p=2) 함수로 구현되어 있다. norm(v, p=2) 에서 p=2 는 p 의 기본값이 2 라는 의미이다. 즉, norm(v) 로 벡터만을 인자로 하여 함수를 호출했을 경우는 p=2 일 경우를 계산한다.\nIn [1]: using LinearAlgebra\n\nIn [2]: v=[1, 2, 3];\n\nIn [3]: norm(v, 1)\nOut[3]: 6.0\n\nIn [4]: norm(v, Inf)\nOut[4]: 3.0\n\nIn [5]: norm(v, 2)\nOut[5]: 3.7416573867739413\n\nIn [6]: normalize(v)\nOut[6]: 3-element Vector{Float64}:\n 0.2672612419124244\n 0.5345224838248488\n\n\n\n1.2 행렬의 노름\n행렬의 노름(norm) 역시 행렬의 크기에 대한 척도로서 특정한 성질을 만족하는 함수 가운데 선택된다.\n\n\n\n\n\n\n\n정의 3 행렬의 노름 \\(\\|\\cdot \\|: \\mathbb{F}^{n\\times n} \\to \\mathbb{R}\\) 은 행렬 \\(\\boldsymbol{A},\\, \\boldsymbol{B}\\) 와 스칼라 \\(\\alpha\\) 에 대해 다음의 성질을 만족한다.\n  (\\(1\\)) \\(\\|\\boldsymbol{A}\\| \\ge 0\\),\n  (\\(2\\)) \\(\\|\\boldsymbol{A}\\| = 0 \\iff \\boldsymbol{A} = 0\\),\n  (\\(3\\)) \\(\\|\\alpha \\boldsymbol{A}\\| = |\\alpha| \\|\\boldsymbol{A}\\|\\),\n  (\\(4\\)) \\(\\|\\boldsymbol{A}+\\boldsymbol{B}\\| \\le \\|\\boldsymbol{A}\\|+\\|\\boldsymbol{B}\\|\\).\n또한 \\(\\boldsymbol{A}\\in \\mathbb{F}^{m\\times n},\\, \\boldsymbol{B} \\in \\mathbb{F}^{n\\times p}\\) 에 대해 다음이 성립한다.\n  (\\(5\\)) \\(\\|\\boldsymbol{AB}\\| \\le \\|\\boldsymbol{A}\\| \\cdot\\|\\boldsymbol{B}\\|\\).\n\n\n\n\n\n위의 정의 중 5번을 제외한 나머지 성질은 벡터에 대해서 성립하며, 5번은 행렬의 노름에만 성립하는 고유한 성질이다.\n\n\n프로베니우스 노름\n프로베니우스 노름은 힐버트-슈미트 노름(Hilbert-Schmidt norm), 혹은 슈어 노름(Schur norm) 으로도 불린다.\n선형대수학에서 배웠다시피 \\(\\mathbb{F}^{m\\times n}\\) 은 벡터공간을 이루며 각각의 \\(\\boldsymbol{A}\\in \\mathbb{F}^{m\\times n}\\) 도 벡터므로, 벡터에 대한 노름 처럼 행렬에 대한 노름도 정의 할 수 있다. 대표적으로 프로베니우스 노름(Frobenius norm) \\(\\|\\cdot \\|_F\\) 은 행렬을 벡터처럼 간주했을 때의 \\(L_2\\) 노름이다.\n\n\n\n\n\n\n\n정의 4 (프로베니우스 노름) \\(m \\times n\\) 행렬에 대한 프로베니우스 노름(Frobenius norm) 은 다음과 같이 정의된다. \\[\n\\|\\boldsymbol{A}\\|_F := \\left(\\sum_{i=1}^m \\sum_{j=1}^n |A_{ij}|^2\\right)^{1/2} = \\sqrt{\\text{tr}(\\boldsymbol{AA}^\\dagger)}\n\\]\n\n\n\n\n\n\n\n연습문제 1 프로베니우스 노름이 행렬의 노름의 정의에 나열된 5가지의 성질을 만족한다는 것을 보여라.\n\n\n\n\n\n자연스러운 노름\n행렬은 벡터공간에서 정의된 선형 함수로서의 의미가 중요하기 때문에, 벡터가 연산되었을 때 그 크기가 어떻게 되는지에 관심이 있는 경우가 많다. 이런 의미에서 행렬의 노름을 벡터와의 연산에 대해 정의할 수 있는데 이렇게 정의된 행렬의 노름은 유도된 행렬 노름 (induced matrix norm), 자연스러운 행렬 노름(natural matrix norm) 혹은 종속된 노름(subordinate norm) 이라고 하며 정사각 행렬의 경우는 연산자 노름(operator norm) 이라고도 한다. 앞으로 행렬의 노름을 이야기할 때는 별다른 언급이 없다면 자연스러운 노름을 의미한다.\n\n\n\n\n\n\n\n\n정의 5 (행렬의 노름) \\(\\mathbb{F}^n\\) 에서의 노름이 \\(\\|\\cdot \\|_\\alpha\\), \\(\\mathbb{F}^m\\) 에서의 노름이 \\(\\| \\cdot \\|_\\beta\\) 로 정의되었다고 하자. \\(\\boldsymbol{A}\\in \\mathbb{F}^{m \\times n}\\) 에 대한 연산자 노름은 다음과 같이 정의된다.\n\\[\n\\|\\boldsymbol{A}\\|_{\\alpha,\\,\\beta} := \\sup \\{\\|\\boldsymbol{Av}\\|_\\beta : \\|\\boldsymbol{v}\\|_\\alpha=1\\} = \\sup_{\\|\\boldsymbol{v}\\|_\\alpha = 1} \\|\\boldsymbol{Av}\\|_\\beta\n\\]\n위의 정의와 다음의 정의는 같다. \\[\n\\|\\boldsymbol{A}\\|_{\\alpha,\\,\\beta} = \\sup \\left\\{\\dfrac{\\|\\boldsymbol{Av}\\|_\\beta}{\\|\\boldsymbol{v}\\|_\\alpha} : \\boldsymbol{v}\\in \\mathbb{F}^n,\\, \\boldsymbol{v}\\ne \\boldsymbol{0} \\right\\} = \\sup_{\\|\\boldsymbol{v}\\|_\\alpha \\ne 0}\\dfrac{\\|\\boldsymbol{Av}\\|_\\beta}{\\|\\boldsymbol{v}\\|_\\alpha}.\n\\]\n\n\n\n\n\n\n\n명제 4 자연스러운 노름은 행렬의 노름의 정의에 나열된 5 가지 성질을 만족한다.\n\n\n\n\n(증명). \\(1\\), \\(2\\), \\(3\\), \\(4\\)는 쉽게 보일 수 있다. 각각의 벡터공간에서 노름을 상징하는 아래첨자는 생략해도 이해하는데 무리가 없을 것이다.\n\\(\\text{5}\\). 우선 \\(\\|\\boldsymbol{Av}\\| \\le \\|\\boldsymbol{A}\\| \\cdot \\|\\boldsymbol{v}\\|\\) 임을 보이자. \\(\\boldsymbol{v}=\\boldsymbol{0}\\) 일 때는 자명하다. \\(\\boldsymbol{v}\\ne \\boldsymbol{0}\\) 이라면,\n\\[\n\\|\\boldsymbol{A}\\| = \\sup_{\\|\\boldsymbol{v}\\|\\ne 0} \\left\\{ \\dfrac{\\|\\boldsymbol{Av}\\|}{\\|\\boldsymbol{v}\\|} \\right\\} \\ge \\dfrac{\\|\\boldsymbol{Av}\\|}{\\|\\boldsymbol{v}\\|}\n\\]\n이므로 \\(\\|\\boldsymbol{Av}\\| \\le \\|\\boldsymbol{A}\\|\\cdot \\|\\boldsymbol{v}\\|\\) 이다. 이를 이용하면 \\[\n\\|\\boldsymbol{ABv}\\| \\le \\|\\boldsymbol{A}\\| \\cdot \\|\\boldsymbol{Bv}\\| \\le \\| \\boldsymbol{A}\\| \\cdot \\|\\boldsymbol{B}\\| \\cdot \\|\\boldsymbol{v}\\|\n\\]\n이므로 \\(\\dfrac{\\|\\boldsymbol{ABv}\\|}{\\|\\boldsymbol{v}\\|} \\le \\|\\boldsymbol{A}\\|\\|\\boldsymbol{B}\\|\\) 이다. 따라서,\n\\[\n\\|\\boldsymbol{AB}\\| =\\sup \\left\\{ \\dfrac{\\|\\boldsymbol{ABv}\\|}{\\|\\boldsymbol{v}\\|}\\right\\} \\le \\|\\boldsymbol{A}\\| \\|\\boldsymbol{B}\\|\n\\]\n이다. \\(\\square\\)\n\n\n다음은 증명 과정에서 보였지만 별도로 언급할 가치가 있다.\n\n\n\n따름정리 1 \\(\\|\\boldsymbol{Av}\\|\\le \\|\\boldsymbol{A}\\|\\cdot \\|\\boldsymbol{v}\\|\\) 이다.\n\n\n\n이것이 의미하는 것은 행렬 \\(\\boldsymbol{A}\\) 의 노름은 \\(\\boldsymbol{A}\\) 에 의해 변환되는 벡터의 노름이 변화하는 최대값을 규정한다는 것으로, 앞으로 매우 많이 나올 내용이다.\n행렬의 정의역(domain)과 공역(codomain)에서의 노름이 동일할 때 \\(\\|\\boldsymbol{A}\\|_\\alpha\\) 와 같이 노름에 대한 첨자를 하나만 표기한다. 또한 노름을 계산할 때 \\(L_p\\) 벡터 노름, 그중에서도 \\(p=1, 2, \\infty\\) 를 많이 사용한다. 즉\n\\[\n\\|\\boldsymbol{A}\\|_p = \\sup \\left\\{ \\dfrac{\\|\\boldsymbol{Av}\\|_p}{\\|\\boldsymbol{v}\\|_p}\\right\\}\n\\]\n이다.\n\n\n\n명제 5 \\(\\boldsymbol{A}\\in \\mathbb{F}^{m \\times n}\\) 일 때, \\[\n\\begin{aligned}\n\\|\\boldsymbol{A}\\|_\\infty &= \\max_{i=1,\\ldots,\\,m} \\sum_{j=1}^n |A_{ij}|,\\\\\n\\|\\boldsymbol{A}\\|_1 &= \\max_{j=1,\\ldots,\\,n} \\sum_{i=1}^m |A_{ij}|,\\\\\n\\end{aligned}\n\\]\n이다.\n\n\n\n\n(증명). 벡터 \\(L_p\\)-노름의 정의로부터, \\[\n\\|\\boldsymbol{Av}\\|_\\infty = \\sup \\left\\{ \\left|\\sum_{j=1}^n A_{ij}v_j\\right| : i=1,\\ldots,\\,m,\\, \\|\\boldsymbol{v}\\|_\\infty = 1 \\right\\}\n\\] 이다. \\(\\|\\boldsymbol{v}\\|_\\infty=1\\) 이면 \\(|v_j|\\le 1,\\, i=1,\\ldots,\\,n\\) 이므로 \\[\n\\left| \\sum_{j=1}^n A_{ij}v_j\\right| \\le \\sum_{j=1}^n |A_{ij}|\\cdot |v_j| \\le \\sum_{j=1}^n |A_{ij}|\n\\] 인데 \\(v_j = \\text{sign}(A_{ij})\\) 일 때 등호가 성립한다. 따라서, \\(\\displaystyle \\|\\boldsymbol{A}\\|_\\infty = \\max_{i=1,\\ldots,\\,m} \\sum_{j=1}^n |A_{ij}|\\) 이다.\n\\(\\|\\boldsymbol{A}\\|_1 = \\displaystyle \\sup_{\\|\\boldsymbol{v}\\|_1=1} \\|\\boldsymbol{Av}\\|_1\\) 이며, \\(\\displaystyle \\|\\boldsymbol{v}\\|_1 = \\sum_{j=1}^n |v_j|\\) 이다.\n\\[\n\\begin{aligned}\n\\|\\boldsymbol{Av}\\|_1 &= \\sum_{i=1}^m \\left| \\sum_{j=1}^n A_{ij}v_j\\right| \\le \\sum_{i=1}^m \\sum_{j=1}^n |A_{ij}| \\cdot |v_j| = \\sum_{j=1}^n |v_j| \\left(\\sum_{i=1}^n |A_{ij}|\\right) \\\\\n&\\le \\max_{j=1,\\ldots,n} \\left(\\sum_{i=1}^n |A_{ij}|\\right) \\\\\n\\end{aligned}\n\\]\n이다. \\(\\displaystyle \\sum_{i=1}^n |A_{ij}|\\) 이 최대가 되는 \\(j\\) 인덱스를 찾아 \\(j_M\\) 이라 하면\n\\[\n\\|\\boldsymbol{Ae}_{j_M}\\|_1 = \\sum_{i=1}^m |A_{i,j_M}| = \\max_{j=1,\\ldots,n} \\sum_{i=1}^m |A_{ij}|\n\\]\n이다. 따라서 \\(\\displaystyle \\|\\boldsymbol{A}\\|_1 = \\max_{j=1,\\ldots,\\,n}\\sum_{i=1}^m |A_{ij}|\\) 이다. \\(\\square\\)\n\n\nJulia 에서 행렬의 \\(L_p\\) 노름은 opnorm(A, p=2) 로 구현되었으며 가능한 p 값은 1, 2 와 Inf 로 각각 \\(L_1,\\, L_2,\\,L_\\infty\\) norm을 계산한다. Frobenius 노름은 norm(A, 2) 로 벡터의 norm 처럼 계산 할 수 있다.\nIn [7]: A = [1 3 2; 4.0 2.0 -3.0];\n\nIn [8]: opnorm(A, 1), opnorm(A, 2), opnorm(A, Inf)\nOut[8]: (5.0, 5.47722557505166, 9.0)\n\nIn [9]: norm(A, 2) # Frobenius norm\nOut[9]: 6.557438524302\n\n\n\n\n1.3 행렬의 조건수\n어떤 함수 \\(f\\) 와 이를 구현하여 수치해석적으로 계산하는 함수 \\(\\overline{f}\\) 가 있다고 하자. 많은 경우 \\(f(x)\\) 와 \\(\\overline{f}(x)\\) 는 차이가 나며 이 차이를 계량하기 위애 보통 아래의 세가지 오차를 사용한다.\n\n\n\n\n\n\n\n정의 6 (오차, 절대오차, 상대오차) 함수 \\(f(x)\\) 와 그 수치해석적인 구현 \\(\\overline{f}(x)\\) 에 대해 아래와 같은 세가지 오차를 정의 할 수 있다.\n  (\\(1\\)) 오차(error) \\(\\delta f(x) := f(x) - \\overline{f}(x)\\).\n  (\\(2\\)) 절대오차(absolute error) \\(|\\delta f(x)| := |f(x) - \\overline{f}(x)|\\).\n  (\\(3\\)) 상대오차(relative error) \\(\\displaystyle \\dfrac{|\\delta f(x)|}{|f(x)|} = \\dfrac{|f(x) - \\overline{f}(x)|}{|f(x)|}\\).\n\n\n\n\n\n위의 세가지 오차는 우리가 구하고자 하는 \\(f\\) 와 우리가 고안한 \\(\\overline{f}\\) 사이의 차이에 대해 정량화 한 값이다. 이제 시스템 자체의 안정성에 대해 한번 보기로 하자. 시스템을 기술하는 함수 \\(f(x)\\) 가 있다고 하자. 적당하게 작은 \\(\\delta x\\) 애 대해 만약 \\(f(x_0+\\delta x)\\) 가 \\(f(x_0)\\) 와 큰 차이가 난다면 이 시스템을 다루는 데는 큰 주의가 필요할 것이다. 이를 ill-conditioned 라고 한다. 반대로 \\(f(x_0 + \\delta x)\\) 가 \\(f(x)\\) 와 큰 차이가 나지 않는다면 \\(x_0\\) 에 어느 정도 오차가 있더라도 우리가 구하고자 하는 값과 큰 차이가 나지 않을 것이고 우리가 다루기 쉬울 것이다. 이를 well-conditioned 라고 한다. 이 성질의 정량화를 위해 함수 \\(f\\) 에 대한 조건수를 아래와 같이 정의한다.\n\n\n\n\n\n\n\n\n정의 7 (함수의 조건수) 함수 \\(f:X\\subset \\mathbb{R}^m \\to \\mathbb{R}^n\\) 와 \\(x_0 \\in X\\) 에 대해 아래와 같이 정의된 \\(\\kappa_f (x_0)\\) 를 \\(f\\) 의 \\(x_0 (\\ne \\boldsymbol{0})\\) 에 대한 조건수 (condition number) 혹은 상대적 조건수 (relative condition number) 라고 한다.\n\\[\n\\kappa_f(x_0) = \\lim_{\\epsilon \\to 0} \\sup_{\\|\\delta x\\| &lt; \\epsilon} \\dfrac{\\|f(x_0+\\delta x)-f(x_0)\\|/\\|f(x_0)\\|}{\\|\\delta x\\|/\\|x_0\\|}\n\\]\n\n\n\n\n\n\\(\\kappa_f(x_0) \\ll 1\\) 이라면 \\(\\|f(x_0 + \\delta x)\\|\\) 가 \\(\\|f(x_0)\\|\\) 와의 차이가 매우 작다. 즉 well-conditioned 이다. 반대로 \\(\\kappa_f(x_0) \\gg 1\\) 이라면 \\(\\|f(x_0+\\delta x)\\|\\) 가 \\(\\|f(x_0)\\|\\) 와의 차이가 매우 크기때문에 ill-conditioned 이다.\n\n다음은 미분의 정의로부터 쉽게 보일 수 있다.\n\n\n명제 6 \\(F:\\mathbb{R}^m \\to \\mathbb{R}^n\\) 가 미분가능한 함수라면\n\\[\n\\kappa_F(\\boldsymbol{x}_0) = \\dfrac{\\|DF(\\boldsymbol{x}_0) \\| \\cdot \\|\\boldsymbol{x}_0\\|}{\\|F(\\boldsymbol{x}_0)\\|}\n\\]\n이다.\n\n\n\n가역행렬인 정사각 행렬 \\(\\boldsymbol{A}\\in \\mathbb{R}^{n \\times n}\\) 을 생각하자. \\(\\boldsymbol{A}\\) 는 \\(\\mathbb{R}^n \\mapsto \\mathbb{R}^n\\) 함수이므로 조건수를 생각 할 수 있다. \\(\\boldsymbol{Ax}_0 = \\boldsymbol{y}_0\\) 라고 하면 \\(\\boldsymbol{x}_0 = \\boldsymbol{A}^{-1}\\boldsymbol{y}_0\\) 이므로,\n\\[\n\\dfrac{\\|\\boldsymbol{A}(\\boldsymbol{x}_0 + \\delta \\boldsymbol{x}) - \\boldsymbol{Ax}_0\\|/\\|\\boldsymbol{Ax}_0\\|}{\\|\\delta\\boldsymbol{ \\boldsymbol{x}}\\|/\\|\\boldsymbol{x}_0\\|} = \\dfrac{\\|\\boldsymbol{A}\\delta \\boldsymbol{x}\\|}{\\|\\delta \\boldsymbol{x}\\|} \\dfrac{\\|\\boldsymbol{x}_0\\|}{\\|\\boldsymbol{Ax}_0\\|} =  \\dfrac{\\|\\boldsymbol{A}\\delta \\boldsymbol{x}\\|}{\\|\\delta \\boldsymbol{x}\\|} \\dfrac{\\|\\boldsymbol{A}^{-1}\\boldsymbol{y}_0\\|}{\\|\\boldsymbol{y}_0\\|} \\le \\|\\boldsymbol{A}\\| \\cdot \\|\\boldsymbol{A}^{-1}\\|\n\\]\n이다. 또한 정의 3 의 (\\(5\\)) 로 부터,\n\\[\n\\|\\boldsymbol{A}\\|\\cdot\\|\\boldsymbol{A}^{-1}\\| \\ge \\|\\boldsymbol{AA}^{-1}\\| = 1\n\\]\n이 성립한다. 즉 가역행렬의 경우 조건수는 함수에 대한 입력값(\\(\\boldsymbol{x}_0\\)) 과는 상관 없이 행렬과 그 역행렬의 노름의 곱으로 정해진다.\n\n\n\n\n\n\n\n정의 8 (가역행렬의 조건수) \\(n \\times n\\) 가역행렬 \\(\\boldsymbol{A}\\) 에 대해 조건수 \\(\\kappa_\\boldsymbol{A}\\) 는 다음과 같다.\n\\[\n\\kappa_\\boldsymbol{A} := \\|\\boldsymbol{A}\\| \\cdot \\|\\boldsymbol{A}^{-1}\\|\n\\]\n\n\n\n\n\n\n\n명제 7 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 가 정규행렬이며 \\(\\mathbb{F}^n\\) 에 대해 \\(L_2\\) 노름이 정의되어 있다고 하자. \\(\\boldsymbol{A}\\) 의 고유값의 집합 \\(\\Lambda\\) 에 대해 \\(\\lambda_\\max := \\max \\{|\\lambda|: \\lambda \\in \\Lambda\\}\\), \\(\\lambda_\\min := \\min \\{|\\lambda| :\\lambda \\in \\Lambda \\}\\) 일 때 다음이 성립한다.\n\\[\n\\kappa_\\boldsymbol{A}= \\dfrac{\\lambda_{\\max}}{\\lambda_{\\min}}\n\\]\n\n\n\n\n(증명). \\(\\boldsymbol{A}\\) 가 정규행렬이면 고유벡터로 대각화가 가능하며, 가역이므로 고유값은 모두 \\(0\\) 이 아니다. 이제 \\(\\boldsymbol{A}\\) 를 각각의 대각성분이 모두 \\(0\\) 이 아닌 대각행렬로 간주해도 된다. \\(\\boldsymbol{v}\\in \\mathbb{F}^n\\) 가 정규화된 벡터라고 하면\n\\[\n\\|\\boldsymbol{Av}\\|_2  \\le |\\lambda_\\max|\n\\]\n이다. \\(\\boldsymbol{v}\\) 가 \\(\\lambda_\\max\\) 에 대한 고유벡터일 때 등호가 성립하므로 \\(\\|\\boldsymbol{A}\\|_2 = \\lambda_\\max\\) 이다. 같은 방법으로 \\(\\|\\boldsymbol{A}^{-1}\\|_2 = \\dfrac{1}{\\lambda_\\min}\\) 임을 보일 수 있다. \\(\\square\\)\n\n\n\n\n따름정리 2 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 가 에르미트 행렬일 때도 명제 7 가 성립한다.",
    "crumbs": [
      "수치해석 I",
      "Julia 에서의 행렬 계산"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/04_matrix_algebra.html#다양한-행렬들",
    "href": "src/numerical_analysis_using_julia/04_matrix_algebra.html#다양한-행렬들",
    "title": "Julia 에서의 행렬 계산",
    "section": "2 다양한 행렬들",
    "text": "2 다양한 행렬들\n여기서는 julia 언어에서 다양한 벡터와 행렬을 어떻게 다루는 지를 배운다.\n\n\n2.1 성긴행렬과 밀집행렬\n수학적으로 희소행렬(혹은 성긴행렬, sparse matrix) 은 행렬 성분의 대다수가 0 인 행렬을 의미하며, 반대의 경우를 밀집 행렬(dense matrix) 이라고 한다. 우리가 지금까지 다룬 행렬들은 행렬의 모든 성분을 컴퓨터가 저장하였다. 그러나 행렬이 매우 큰 경우, 예를 들면 Float64 타입의 \\(100K \\times 100K\\)(\\(1K=1,000\\)) 행렬은 성분을 저장하는 데만 80 Gbytes 의 저장공간을 차지하므로 개인 컴퓨터에서는 처리가 거의 불가능하며, 실제로 많이 다루는 시스템은 이것보다 훨씬 클 수 가 있다. 다행히 우리가 관심있는 시스템 가운데 상당수는 행렬의 대부분의 성분이 \\(0\\) 인데, 이 경우 행렬의 모든 성분을 저장하는 것이 아니라 \\(0\\) 이 아닌 성분만을 그 인덱스와 함께 저장하여 더 작은 저장공간에서 더 빠르게 계산 할 수 있다. 수치해석에서는 이렇게 행렬의 인덱스와 값을 저장하고, 인덱스가 지정되지 않은 행렬을 \\(0\\) 으로 간주하는 행렬을 희소 행렬이라 하며, 우리가 지금 까지 다뤘던, 인덱스에 따라 순차적으로 성분을 저장하는 행렬을 밀집 행렬 (dense matrix) 이라 한다.\n다차원 배열에 대해서도 희소배열을 생각 할 수 있으며 Julia 의 표준 라이브러리에 포함된 SparseArrays 모듈을 이용하여 1차원 배열인 희소벡터와, 2차원 배열인 희소행렬을 지원한다. Julia 에서의 희소행렬은 내부적으로 아래와 같이 구현되어 있다. 5개의 멤버를 갖는 구조체이며 행렬의 크기와, 행, 열, 값을 갖는다. SmarseMatrixCSC 의 CSC 는 희소행렬을 내부적으로 저장할 때 쓰는 일종의 압축 방식을 표현한다.\nstruct SparseMatrixCSC{Tv,Ti&lt;:Integer} &lt;: AbstractSparseMatrixCSC{Tv,Ti}\n    m::Int                  # Number of rows\n    n::Int                  # Number of columns\n    colptr::Vector{Ti}      # Column j is in colptr[j]:(colptr[j+1]-1)\n    rowval::Vector{Ti}      # Row indices of stored values\n    nzval::Vector{Tv}       # Stored values, typically nonzeros\nend\n가장 기본적인 생성 방법은 sparse() 함수를 이용하는 것이다. 아래의 예에서 A 는 밀집행렬이며 이것을 sparse 함수를 통해 희소행렬로 만들었다. 혹은 sparse(I, J, V) 를 통해서도 생성할 수 있는데, I 는 행 인덱스를 나타내는 벡터, J 는 열 인덱스를 나타내는 벡터, V 는 값을 나타내는 벡터이다. 즉, B=sparse(I, J, V) 라면,\n\\[\nB_{I[i], J[i]}=V[i], \\, i=1,\\,2,\\ldots\n\\]\n이다.\nIn [1]: using LinearAlgebra, SparseArrays\n\nIn [2]: A = Matrix(1.0I, 3, 3)\nOut[2]: 3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\nIn [3]: sparse(A)\nOut[3]: 3×3 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n 1.0   ⋅    ⋅ \n  ⋅   1.0   ⋅ \n  ⋅    ⋅   1.0\n\nIn [4]: B = sparse([1, 1, 2, 3], [1, 3, 2, 3], [-1.0, 2.0, 0.0, 4.0])\nOut[4]: 3×3 SparseMatrixCSC{Float64, Int64} with 4 stored entries:\n -1.0   ⋅   2.0\n   ⋅   0.0   ⋅ \n   ⋅    ⋅   4.0\n\nIn [5]: dropzeros(B)\nOut[5]: 3×3 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n -1.0   ⋅   2.0\n   ⋅    ⋅    ⋅ \n   ⋅    ⋅   4.0\n마지막의 dropzeros() 함수는 희소 행렬 내의 \\(0\\) 을 제거하여 저장공간을 줄인다. B=sparse(I, J, V) 형식으로 생성하면, B 행렬의 크기는 I 와 J 벡터의 최대값으로 정해지지만, 행렬의 크기를 정할 수 있다. 예를 들어,\nIn [6]: sparse([1, 2], [1, 3], [1, -1], 4, 4)\nOut[6]: 4×4 SparseMatrixCSC{Int64, Int64} with 2 stored entries:\n 1  ⋅   ⋅  ⋅\n ⋅  ⋅  -1  ⋅\n ⋅  ⋅   ⋅  ⋅\n와 같이 sparse(I, J, V, m, n) 형식으로 생성하면 이 희소행렬은 \\(m \\times n\\) 행렬이 된다. 희소벡터는 sparsevec(I, V, m) 형식으로 생성 할 수 있다.\nIn [7]: sparsevec([1, 4], [2, -1], 5)\nOut[7]: 5-element SparseVector{Int64, Int64} with 2 stored entries:\n  [1]  =  2\n  [4]  =  -1\n영행렬 혹은 영벡터는 spzeros() 함수를 통해 생성한다. 성긴 단위행렬은 sparse(I, 4, 4) 와 같이 생성한다. 단 I 는 LinearAlgebra 모듈에 정의되어 있으므로 using LinearAlgebra 를 한 후 사용할 수 있다.\nusing LinearAlgebra, SparseArrays\nsm1 = spzeros(Float32, 10, 10) # Float32 타입의 값을 갖는 10x10 성긴행렬\nsm2 = spzeros(5, 5) # 타입이 지정되지 않으면 Float64 타입의 값을 갖는다.\nsv = spzeros(4) #Float64 타입의 4 차원 벡터\nsI = sparse(I, 4, 4) # 4xt 단위행렬\n\n\n\n2.2 타입으로 정의된 행렬\nLinearAlgebra.jl 은 선형대수학에서 사용되는 특별한 명칭이 붙은 행렬중 일부를 특별히 별도의 타입으로 지정하였다. 아주 단순한 예를 들자면 선형 시스템 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 풀 때, \\(\\boldsymbol{A}\\) 가 대각성분을 제외한 성분이 모두 \\(0\\) 인 대각행렬이라면 쉽게 풀 수 있으며, 상삼각행렬이나, 하삼각행렬이라도 일반적인 행렬에 비해서도 훨씬 쉽게 풀 수 있다. 그러나 컴퓨터가 임의로 \\(\\boldsymbol{A}\\) 가 어떤 행렬인지 결정하게 하기는 힘들며, 때때로 바람직 하지 않은 오류를 낼 수 있다. Julia 에서는 다중 디스패치를 이용할 수 있으므로 행렬 \\(\\boldsymbol{A}\\) 를 각각의 형태에 맞는 타입으로(대각행렬이라든가, 상삼각행렬이라든가) 정해 줄 수 있다면 이에 맞춰서 선형 시스템을 푸는 함수를 공통적으로 정의 할 수 있다. 즉 solve_linearsystem_diagonal, solve_linearsystem_uppertriangula 등을 따로 정의 할 필요 없이 solve_linearsystem(A::Diatonal), solve_linearsystem(A::UpperTriangula) 와 같이 할 수 있다는 뜻이다.\nLinearAlgebra.jl 에 정의된 특수한 타입으로서의 행렬의 목록은 아래와 같다.\n\n\n\n\n\n\n경고\n\n\n\n아래의 목록에 정의된 행렬의 타입들 가운데 상당수는 희소행렬이다. 굳이 밀집행렬로 바꾸고 싶다면 Matrix(A) 를 사용한다.\n\n\n\n\n\n\n\n\n\n\n\n타입\n명칭\n설명\n\n\n\n\nUniformScaling\nUniform scaling operator\nc\\(\\boldsymbol{I}\\)\n\n\nDiagonal\n대각행렬 (Diagonal matrix)\n\n\n\nBidiagonal\n상/하 쌍대각 행렬 (Upper/lower bidiagonal matrix)\n\n\n\nTridiagonal\n삼중 대각 행렬 (Tridiagonal matrix)\n\n\n\nSymTridiagonal\n대칭 삼중 대각 행렬 (Symmetric tridiagonal matrix)\n\n\n\nUpperTriangular\n상삼각행렬 (Upper triangular matrix)\n\\(i&gt;j \\implies A_{ij}=0\\)\n\n\nUnitUpperTriangular\n단위 상삼각행렬 (Upper triangular matrix with unit diagonal)\n상삼각행렬이며 대각성분이 \\(1\\)\n\n\nLowerTriangular\n하삼각행렬 (Lower triangular matrix)\n\\(i&lt;j \\implies A_{ij}=0\\)\n\n\nUnitLowerTriangular\n단위 하삼각행렬 (Lower triangular matrix with unit diagonal)\n하삼각행렬이며 대각성분이 \\(1\\)\n\n\nUpperHessenberg\n상 헤센베르크 행렬 (Upper Hessenberg matrix)\n\n\n\nSymmetric\n대칭 행렬 (Symmetric matrix)\n\\(\\boldsymbol{A} =\\boldsymbol{A}^T\\)\n\n\nHermitian\n에르미트 행렬 (Hermitian matrix)\n\\(\\boldsymbol{A} =\\boldsymbol{A}^\\ast\\)\n\n\n\n\n\nUniformScaling\nUniformScaling 은 단위행렬에 상수를 곱한것을 말한다. UniformScaling(2.0) 은 (2.0 이 Float64 타입의 부동소수이므로) Float64 타입의 성분을 갖는 단위행렬이다. 다만 아직까지는 차원이 정해지지 않으며 다른 행렬과 연산될 때 적절한 차원을 갖게 된다. 단위행렬을 나타내는 I 를 대신 사용 할 수 있으며, UniformScaling(2.0)==2.0I 이다.\nIn [1]: using LinearAlgebra, SparseArrays\n\nIn [2]: 2.0I + 3\nOut[2]: 5.0\n\nIn [3]: 2.0I + [0  1; 2 3]\nOut[3]: 2×2 Matrix{Float64}:\n 2.0  1.0\n 2.0  5.0\n\n\n\nDiagonal\n대각행렬을 나타내는 타입이다. \\(n\\) 차원 벡터를 입력하면 차례로 대각성분이 되는 대각행렬을 만든다.\nIn [4]: Diagonal([1, 10, 100])\nOut[4]: 3×3 Diagonal{Int64, Vector{Int64}}:\n 1   ⋅    ⋅\n ⋅  10    ⋅\n ⋅   ⋅  100\nDiagonal 타입의 행렬에 UniformScaling 을 더하면, 아마 여러분이 예상한 결과가 나올 것이다.\nIn [5]: Diagonal([1, 10, 100]) + 3.0I\nOut[5]: 3×3 Diagonal{Float64, Vector{Float64}}:\n 4.0    ⋅      ⋅ \n  ⋅   13.0     ⋅ \n  ⋅     ⋅   103.0\n\n\n\nBidiagonal, Tridiagonal, SymTridiagonal\n\\(m\\times n\\) 행렬 \\(A_{ij}\\) 에서 \\(A_{ii},\\, i=1,\\ldots,\\, \\min \\{m,\\,n\\}\\) 을 대각(diagonal) 성분이라 한다. 대각 성분의 바로 오른쪽 성분, 즉 \\(A_{i,\\, i+1}\\) 을 상대각 (superdialgonal) 성분이라 하며, 왼쪽 성분, 즉 \\(A_{i+1,\\,i}\\) 을 하대각(subdiagonal) 성분이라 한다.(적절한 번역어를 못찾아서 임의로 붙인 이름이다.)\n대각행렬과 대각성분 근처의 일부만을 제외한 모든 성분이 \\(0\\) 인 행렬을 띠행렬 (Band matrix)이라 한다. 대각성분과 상대각 성분/하대각 성성분 가운데 하나를 제외한 모든 성분이 \\(0\\) 인 행렬을 각각 상쌍대각 행렬/하쌍대각 행렬이라 한다. 아래의 보기에서 \\(\\boldsymbol{B}_U\\) 는 상쌍대각행렬, \\(\\boldsymbol{B}_L\\) 은 하쌍대각행렬이다.\n\\[\n\\boldsymbol{B}_U = \\begin{bmatrix} 4 & 1 & 0 &0  \\\\ 0 & 3 & 1 & 0 \\\\ 0 & 0 & 2 &  2 \\\\ 0 & 0 & 0 & 5 \\end{bmatrix} ,\\qquad \\boldsymbol{B}_L = \\begin{bmatrix} 4 & 0 & 0 &0  \\\\ 1 & 3 & 0 & 0 \\\\ 0 & 1 & 2 &  0 \\\\ 0 & 0 & 2 & 5 \\end{bmatrix}\n\\]\n쌍대각 행렬은 Bidiagonal 객체로 표현되며, 두가지 방법으로 구성 할 수 있다. 첫번째 방법은 생성자에(객체의 생성자이므로 객체와 같은 이름을 가진다.) 첫번째 인자로 대각성분을, 두번째 인자로 \\(0\\) 이 아닌 대각성분의 위/아래 성분을, 세번째 인자로 상쌍대각행렬이면 :U 를 하쌍대각행렬이면 :L 을 입력하는 방법이다.\nIn [6]: Bu = Bidiagonal([4, 3, 2, 5], [1, 1, 0], :U)\nOut[6]: 4×4 Bidiagonal{Int64, Vector{Int64}}:\n 4  1  ⋅  ⋅\n ⋅  3  1  ⋅\n ⋅  ⋅  2  0\n ⋅  ⋅  ⋅  5\n\nIn [7]: Bl = Bidiagonal([4, 3, 2, 5], [1, 1, 2], :L)\nOut[7]: 4×4 Bidiagonal{Int64, Vector{Int64}}:\n 4  ⋅  ⋅  ⋅\n 1  3  ⋅  ⋅\n ⋅  1  2  ⋅\n ⋅  ⋅  2  5\nBidiangoal 타입 행렬을 구성하는 두번째 방법은 이미 존재하는 행렬로부터 대각성분을 포함한 \\(0\\) 이 아닌 성분을 가져오는 방법이다. 다음을 보면 쉽게 이해 할 수 있을 것이다.\nIn [9]: A = [1 1 1 1; 2 2 2 2; 3 3 3 3; 4 4 4 4]; Bidiagonal(A, :U)\nOut[9]: 4×4 Bidiagonal{Int64, Vector{Int64}}:\n 1  1  ⋅  ⋅\n ⋅  2  2  ⋅\n ⋅  ⋅  3  3\n ⋅  ⋅  ⋅  4\n\nIn [10]: Bidiagonal(A, :L)\nOut[10]: 4×4 Bidiagonal{Int64, Vector{Int64}}:\n 1  ⋅  ⋅  ⋅\n 2  2  ⋅  ⋅\n ⋅  3  3  ⋅\n ⋅  ⋅  4  4\n\n대각성분, 상대각성분, 하대각성분을 제외한 성분이 모두 \\(0\\) 인 성분을 삼중대각행렬 (Tridiagonal matrix) 라 하며 LinearAlgebra.jl 에서는 Tridiagonal 타입으로 정의된다. 아래의 행렬 \\(\\boldsymbol{C}\\) 는 삼중 대각행렬이다.\n\\[\n\\boldsymbol{C} = \\begin{bmatrix} 4 & 1 & 0 & 0 & 0 \\\\ 2 & 3 & 1 & 0 & 0 \\\\ 0 & 3 &1& 2 &  0 \\\\ 0 & 0 & 2& 5 & 1 \\\\ 0 & 0 & 0 &3 & 6 \\end{bmatrix}\n\\]\n삼중대각행렬을 구성하는 방법은 쌍대각행렬을 구성하는 방법과 비슷하게 두가지 방법이다. 하나는 대각행렬 아래 성분들을 나타내는 벡터, 대각성분벡터, 대각성분 위의 성분들을 나타내는 벡터를 차례로 입력하는 것이다.\n위의 행렬 \\(\\boldsymbol{C}\\) 를 구성한다면\nIn [11]: C = Tridiagonal([2, 3, 2, 3], [4, 3, 1, 5, 6], [1, 1, 2, 1])\nOut[11]: 5×5 Tridiagonal{Int64, Vector{Int64}}:\n 4  1  ⋅  ⋅  ⋅\n 2  3  1  ⋅  ⋅\n ⋅  3  1  2  ⋅\n ⋅  ⋅  2  5  1\n ⋅  ⋅  ⋅  3  6\n와 같이 할 수 있다.\n다른 방법은 이미 존재하는 행렬로부터 구성하는 것이다.\nIn [13]: D = [1 2 3 4 5]' *ones(Int64, 5)'\nOut[13]: 5×5 Matrix{Int64}:\n 1  1  1  1  1\n 2  2  2  2  2\n 3  3  3  3  3\n 4  4  4  4  4\n 5  5  5  5  5\n\nIn [13]: Tridiagonal(D)\nOut[13]: 5×5 Tridiagonal{Int64, Vector{Int64}}:\n 1  1  ⋅  ⋅  ⋅\n 2  2  2  ⋅  ⋅\n ⋅  3  3  3  ⋅\n ⋅  ⋅  4  4  4\n ⋅  ⋅  ⋅  5  5\nSymTridiagonal 은 대칭인 삼중대각행렬을 나타내는 타입이다. 대칭행렬이므로 대각행렬의 위 띠와 아래띠가 같다. 이것은 어떻게 구성할까? 여러분이 짐작할 수 있듯이 대각성분과, 띠 성분을 벡터로 입력하면 된다.\nIn [14]: SymTridiagonal([1,2, 3, 4, 5], [-1, -2, -3, -4])\nOut[14]: 5×5 SymTridiagonal{Int64, Vector{Int64}}:\n  1  -1   ⋅   ⋅   ⋅\n -1   2  -2   ⋅   ⋅\n  ⋅  -2   3  -3   ⋅\n  ⋅   ⋅  -3   4  -4\n  ⋅   ⋅   ⋅  -4   5\n\n\n\nUpperTriangular, LowerTriangular\n상삼각행렬과 하삼각행렬을 다루기 위한 타입은 각각 UpperTriangular, LowerTriangular 이다. 거기에 대각성분이 \\(1\\) 인 행렬로 특별히 UnitUpperTriangular, UnitLowerTriangular 타입이 준비되어 있다. 이미 존재하는 행렬로부터 필요한 만큼의 행렬 원소를 취하여 행렬을 구성 할 수 있다.\nIn [15]: A = [1.0 2.0 3.0; 4.0 5.0 6.0; 7.0 8.0 9.0]\nOut[15]: 3×3 Matrix{Float64}:\n 1.0  2.0  3.0\n 4.0  5.0  6.0\n 7.0  8.0  9.0\n\nIn [16]: UpperTriangular(A)\nOut[16]: 3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 1.0  2.0  3.0\n  ⋅   5.0  6.0\n  ⋅    ⋅   9.0\n\nIn [17]: LowerTriangular(A)\nOut[17]: 3×3 LowerTriangular{Float64, Matrix{Float64}}:\n 1.0   ⋅    ⋅ \n 4.0  5.0   ⋅ \n 7.0  8.0  9.0\n\nIn [18]: UnitUpperTriangular(A)\nOut[18]: 3×3 UnitUpperTriangular{Float64, Matrix{Float64}}:\n 1.0  2.0  3.0\n  ⋅   1.0  6.0\n  ⋅    ⋅   1.0\n\nIn [19]: UnitLowerTriangular(A)\nOut[19]: 3×3 UnitLowerTriangular{Float64, Matrix{Float64}}:\n 1.0   ⋅    ⋅ \n 4.0  1.0   ⋅ \n 7.0  8.0  1.0\n\n\n\n헤센베르그 행렬\n헤센베르그 행렬은 상헤센베르그 행렬(Upper Hessenberg matrix) 과 하헤센베르그 행렬(Lower Hessenberg matrix) 로 이루어진다. 상헤센베르그 행렬(Upper Hessenberg matrix) 은 정사각행렬이며 행렬의 좌하단 모서리 부근이 모두 \\(0\\) 인 행렬이다. 하헤센베르그 행렬(Lower Hessenberg matrix) 는 정사각 행렬이며 행렬의 우하단 모서리 부근이 모두 \\(0\\) 인 행렬이다. 아래의 보기에서 \\(\\boldsymbol{H}_U\\) 는 상헤센베르그 행렬, \\(\\boldsymbol{H}_L\\) 은 하헤센베르그 행렬이다.\n\\[\n\\boldsymbol{H}_U = \\begin{bmatrix} 3 & 4 & 2 & 3\\\\ 1 & 4 & 2 & 3\\\\ 0 & 2 & 3 & 5 \\\\ 0 & 0 & 1 & 1\\\\\\end{bmatrix}\n,\\qquad \\boldsymbol{H}_L = \\begin{bmatrix} 1 & 2 & 0 & 0 \\\\ 7 & 2 & 3 & 0 \\\\ 3 & 4 & 3 & 7 \\\\ 5 & 3 & 1 &2 \\end{bmatrix}\n\\]\nJulia 에서는 상헤센베르그 행렬 가운데 첫번째 subdiagonal elements 아래의 성분을 0 으로 하는 행렬만 따로 UpperHessenberg 타입으로 정의한다. 임의의 정사각 행렬에 대해 UpperHessenberg 생성자를 취하면 다음과 같다.\nIn [20]: HH = [3  4  2  3; 1  4  2  3; 1  2  3  5 ; 1  2  1  1]\nOut[20]: 4×4 Matrix{Int64}:\n 3  4  2  3\n 1  4  2  3\n 1  2  3  5\n 1  2  1  1\n\nIn [21]: HU = UpperHessenberg(HH)\nOut[21]: 4×4 UpperHessenberg{Int64, Matrix{Int64}}:\n 3  4  2  3\n 1  4  2  3\n ⋅  2  3  5\n ⋅  ⋅  1  1\n\n\n\n대칭행렬과 에르미트 행렬\n어떤 행렬 \\(\\boldsymbol{A}\\) 의 행과 열을 바꾼 행렬을 그 행렬의 전치행렬 (transposed matrix) 라 하고 \\(\\boldsymbol{A}^T\\) 이라 한다. 즉 \\((\\boldsymbol{A}^T)_{ij} = (\\boldsymbol{A})_{ji}\\) 이다. 행렬 \\(\\boldsymbol{A}\\) 행과 열을 바꾸고 켤레복소수를 취한것을 켤레전치 행렬(conjugate transpose matrix) 혹은 에르미트 전치 행렬(Hermite transpose matrix)라 하고 \\(\\boldsymbol{A}^\\ast\\) 나 \\(\\boldsymbol{A}^\\dagger\\) 라 쓴다. 전자는 수학에서, 후자는 물리학에서 주로 쓰는 표현이다. \\(\\boldsymbol{A}=\\boldsymbol{A}^T\\) 이면 대칭행렬 이라 한다. \\(\\boldsymbol{A}=\\boldsymbol{A}^\\ast\\) 이면 에르미트 행렬(Hermitian matrix), 혹은 자기수반 행렬(Self-adjoint matrix) 라 한다. 예를 들어 \\(\\boldsymbol{A} =\\begin{bmatrix} 2+3i & i \\\\ 2i & 3\\end{bmatrix}\\) 에 대해,\n\\[\n\\boldsymbol{A}^T =\\begin{bmatrix} 2+3i & 2i \\\\ i & 3\\end{bmatrix},\\qquad \\boldsymbol{A}^\\ast = \\begin{bmatrix} 2-3i & -i \\\\ -2i & 3\\end{bmatrix}\n\\]\n이다. 행렬 \\(\\boldsymbol{A}\\) 가 실수 성분만을 가진다면 \\(\\boldsymbol{A}^T = \\boldsymbol{A}^\\ast\\) 이다.\n대칭행렬에 대한 타입은 Symmetric 이다. 이미 존재 하는 행렬에서 하삼각부분이나 상삼각부분을 취하여 대칭행렬로 만든다.\nIn [22]: A = [1 0 2 0 3; 0 4 0 5 0; 6 0 7 0 8; 0 9 0 1 0; 2 0 3 0 4]\nOut[22]: 5×5 Matrix{Int64}:\n 1  0  2  0  3\n 0  4  0  5  0\n 6  0  7  0  8\n 0  9  0  1  0\n 2  0  3  0  4\n\nIn [23]: Supper = Symmetric(A, :U)\nOut[23]: 5×5 Symmetric{Int64, Matrix{Int64}}:\n 1  0  2  0  3\n 0  4  0  5  0\n 2  0  7  0  8\n 0  5  0  1  0\n 3  0  8  0  4\n\nIn [24]: Slower = Symmetric(A, :L)\nOut[24]: 5×5 Symmetric{Int64, Matrix{Int64}}:\n 1  0  6  0  2\n 0  4  0  9  0\n 6  0  7  0  3\n 0  9  0  1  0\n 2  0  3  0  4\n행렬 A 의 하삼각 부분을 취하여 대칭행렬로 만들 때는 Symmetric(A, :U) 라 하고, 하삼각 부분을 취하여 대칭행렬로 만들 때는 Symmetric(A, :L) 라 한다. 이 때 :U 나 :L 을 입력하지 않으면 :U 를 기본값으로 하여 대칭행렬을 만든다.\n에르미트 행렬에 대한 타입은 Hermite 이며 마찬가지로 Hermite(A, :U) 나 Hermite(A, :L) 로 에르미트행렬을 만든다. 역시 기본값은 :U 이다.\nIn [25]: A = [1 0 2+2im 0 3-3im; 0 4 0 5 0; 6-6im 0 7 0 8+8im; 0 9 0 1 0; 2+2im 0 3-3im 0 4];\n\nIn [26]: Hupper = Hermitian(A)\nOut[26]: 5×5 Hermitian{Complex{Int64}, Matrix{Complex{Int64}}}:\n 1+0im  0+0im  2+2im  0+0im  3-3im\n 0+0im  4+0im  0+0im  5+0im  0+0im\n 2-2im  0+0im  7+0im  0+0im  8+8im\n 0+0im  5+0im  0+0im  1+0im  0+0im\n 3+3im  0+0im  8-8im  0+0im  4+0im\n\nIn [27]: Hlower = Hermitian(A, :L)\nOut[27]: 5×5 Hermitian{Complex{Int64}, Matrix{Complex{Int64}}}:\n 1+0im  0+0im  6+6im  0+0im  2-2im\n 0+0im  4+0im  0+0im  9+0im  0+0im\n 6-6im  0+0im  7+0im  0+0im  3+3im\n 0+0im  9+0im  0+0im  1+0im  0+0im\n 2+2im  0+0im  3-3im  0+0im  4+0im",
    "crumbs": [
      "수치해석 I",
      "Julia 에서의 행렬 계산"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/04_matrix_algebra.html#linearalgebra.jl-의-기본-연산자와-함수",
    "href": "src/numerical_analysis_using_julia/04_matrix_algebra.html#linearalgebra.jl-의-기본-연산자와-함수",
    "title": "Julia 에서의 행렬 계산",
    "section": "3 LinearAlgebra.jl 의 기본 연산자와 함수",
    "text": "3 LinearAlgebra.jl 의 기본 연산자와 함수\n우리는 앞서 가우스 소거법과 LU 분해 그리고 그람-그람 슈미트 과정을 통한 QR 분해를 구현하였다. 그러나 앞으로는 이것을 버리고 Julia 에서 제공하는 LinearAlgebra 모듈을 사용하도록 하자. Julia 의 LinearAlgebra 모듈은 선형대수학에서 사용하는 많은 기능을 포함하고 있다. 여기서는 중요한 것만 설명할 것이며 자세한 것은 LinearAlgbra.jl 공식 문서 를 참고하자. LinearAlgebra 를 우리가 지금껏 작성한 함수와 그 속도를 비교해보면 LinearAlgebra 쪽이 훨씬 빠르다. 그리고 LinearAlgebra module 은 많은 프로그래밍 언어에서 사용중이며, 수십년의 역사를 가진 BLAS(Basic Linear Algebra Subprograms) 과 LAPACK(Linear Algebra PACKage) 에 기반하여 만들어졌으므로 믿을만 하다. 많은 함수는 희소행렬과 밀집행렬에 대해 공통적으로 사용 할 수 있다.\n\n\n3.1 기본 연산자와 함수\nLininearAlgebra.jl 은 행렬 A(\\(\\boldsymbol{A}\\)), B(\\(\\boldsymbol{B}\\)) 와 벡터 u(\\(\\boldsymbol{u}\\)), v(\\(\\boldsymbol{v}\\)), 스칼라 a(\\(a\\)), b(\\(b\\)) 에 대해 다음과 같은 함수를 지원한다. (LinearAlgebra.jl 이 지원하는 함수중 일부이다.)\n\n연산자\n\n\n\n\n\n\n\n연산자\n설명\n\n\n\n\n+, -, *\n수학적으로 정의된 스칼라, 벡터, 행렬 사이의 더하기, 빼기, 곱하기 연산\n\n\n\\\nA \\ B 일 경우 AX==B 를 만족하는 행렬 A 를 구한다. 구하는 방법은 A 가 어떤 행렬인지에 따라 달라진다.\n\n\n/\n행렬간의 연산일 경우 (B' / A') 를 구한다.\n\n\nu⋅v (\\cdot+[tab])\n\\(\\boldsymbol{u} \\cdot \\boldsymbol{v}\\)\n\n\n×(u, v) (\\times+[tab])\n\\(\\boldsymbol{u \\times v}\\)\n\n\n\n\n\n\n기본적인 함수\n\n\n\n함수\n설명\n비고\n\n\n\n\ntr(A)\n\\(\\text{tr}(\\boldsymbol{A})\\)\n\n\n\ndet(A)\n\\(\\det(\\boldsymbol{A})\\)\n\n\n\ninv(A)\n\\(\\boldsymbol{A}^{-1}\\)\nA 가 정사각행렬일 경우\n\n\ntranspose(A)\n\\(\\boldsymbol{A}^{T}\\)\nA 의 전치행렬\n\n\nA' or adjoint(A)\n\\(\\boldsymbol{A}^\\dagger\\)\nA 의 에르미트 전치행렬\n\n\ndiag(A)\n\\([A_{11},\\,A_{22},\\ldots]\\)\n대각성분\n\n\ndot(u, v)\nu⋅v 와 같다\n\n\n\ndot(u, A, v)\ndot(u, A*v) 와 같다\n\n\n\ncross(u, v)\n×(u, v) 와 같다\n벡터의 외적\n\n\nnorm(u, p) or norm(A, p)\n벡터의 \\(L_p\\)-노름\n벡터와 행렬의 노름과 조건수 참고\n\n\nopnorm(A, p::Real=2)\n행렬의 \\(L_p\\)-노름\n벡터와 행렬의 노름과 조건수 참고\n\n\nnormalize(a, p::Real=2)\n벡터, 행렬의 정규화\nnorm(a, p)==1 되도록 normalize 한다.\n\n\nexp(A)\nexponential of \\(\\boldsymbol{A}\\)\n\\(\\displaystyle \\sum_{k=1}^\\infty \\dfrac{\\boldsymbol{A}^k}{k!}\\)",
    "crumbs": [
      "수치해석 I",
      "Julia 에서의 행렬 계산"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/04_matrix_algebra.html#선형-시스템",
    "href": "src/numerical_analysis_using_julia/04_matrix_algebra.html#선형-시스템",
    "title": "Julia 에서의 행렬 계산",
    "section": "4 선형 시스템",
    "text": "4 선형 시스템\n우리는 앞서 LU 분해를 통해 선형 시스템을 푸는 코드를 작성하였다. 이제는 LinearAlgebra.jl 이 제공하는 \\ 연산자를 이용하면 쉽게 풀 수 있다. 일반적인 정사각 행렬에 대해서는 성긴 행렬이든 밀집 행렬이든 LU 분해를 통해 선형 시스템을 쉽게 풀 수 도록 해 준다. 예를 들어,\nIn [1]: using LinearAlgebra\n\nIn [2]: A=[4 2 3;3 1 2;3 3 4];b=[2; 1 ;-1]\nOut[2]: 3-element Vector{Int64}:\n  2\n  1\n -1\n\nIn [3]: A\\b\nOut[3]: 3-element Vector{Float64}:\n  2.0\n  3.0000000000000013\n -4.000000000000001\n와 같이 풀 수 있다.",
    "crumbs": [
      "수치해석 I",
      "Julia 에서의 행렬 계산"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/05_interpolation.html",
    "href": "src/numerical_analysis_using_julia/05_interpolation.html",
    "title": "다항식을 이용한 1차원 보간법",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 I",
      "다항식을 이용한 1차원 보간법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/05_interpolation.html#바이어슈트라스-근사정리",
    "href": "src/numerical_analysis_using_julia/05_interpolation.html#바이어슈트라스-근사정리",
    "title": "다항식을 이용한 1차원 보간법",
    "section": "1 바이어슈트라스 근사정리",
    "text": "1 바이어슈트라스 근사정리\n이산적인 데이터 \\((x_1,\\,y_1),\\,(x_2,\\,y_2),\\ldots,\\, (x_N,\\, y_N)\\) 가 주어졌으며 \\(x_1&lt;x_2&lt;\\cdots &lt;x_N\\) 으로 정렬되어 있다고 하자. 이 때 주어진 데이터를 바탕으로 임의의 \\(x\\) 값을 추측할 경우, \\(x_1 \\le x \\le x_N\\) 의 \\(x\\) 값에 대해 추측하는 것을 보간법(interpolation) 이라 하고, \\(x&lt;x_1\\) 이거나 \\(x &gt;x_N\\) 일 경우 추축하는것을 외삽법(extrapolation) 이라 한다. 여기서는 보간법에 대해 다루기로 한다. 기본적으로 여기서 다루는 보간법은 데이터를 다항식으로 근사시키는데, 이 방법은 아래의 바이어스트라스 근사 정리(Weierstrass approximation theorem)로 정당화된다.\n\n\n\n정리 1 (바이어슈트라스 근사 정리) 함수 \\(f : X \\subset \\mathbb{R} \\to \\mathbb{R}\\) 가 \\([a,\\,b] \\subset I\\) 구간에서 연속일 때, 임의의 \\(\\varepsilon &gt; 0\\) 에 대해\n\\[\n|f(x)-P(x)|&lt;\\varepsilon,\\qquad \\forall x\\in [a,\\,b]\n\\]\n인 다항식 \\(P(x)\\) 가 항상 존재한다.\n\n\n\n이 정리에 대한 증명은 해석학(Analysis) 교과서(예를 들면, Terrence Tao의 해석학 교과서 2권) 를 참고하라.",
    "crumbs": [
      "수치해석 I",
      "다항식을 이용한 1차원 보간법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/05_interpolation.html#sec-global_interpolation_using_polynomial",
    "href": "src/numerical_analysis_using_julia/05_interpolation.html#sec-global_interpolation_using_polynomial",
    "title": "다항식을 이용한 1차원 보간법",
    "section": "2 다항식을 이용한 전역적 보간법",
    "text": "2 다항식을 이용한 전역적 보간법\n전역적 보간법은 전체 데이터를 하나의 다항식으로 표현하는 방법이며, 후술할 국소적 보간법은 구하고자 하는 값 주위의 몇개의 데이터만을 이용하여 국소적으로 다항식으로 표현한다. 여기서는 \\(n+1\\) 개의 데이터 \\(\\{ (x_i,\\, y_i): i=1,\\ldots,\\,n\\}\\) 이 주어졌으며 \\(i\\ne j \\implies x_i \\ne x_j\\) 일 때, 즉 어떤 두 \\(x_i,\\,x_j\\) 도 같지 않을 경우에 대해, 이 점을 모두 지나는 \\(n-1\\) 차 다항식은 유일하게 정해진다는 사실을 이용하여 \\(n-1\\) 차 다항식을 구하는 방법을 설명한다. 이 다항식을 구하는 방법은 방데르몽드(Vandermond) 방법, 라그랑쥬(Lagrange) 방법, 뉴턴(Newton) 방법 등이 있다\n\n\n2.1 발데르몽드 방법\n다항식을 다음과 같이 놓는다. \\[\nV_{n} (x) = a_0 + a_1 x + \\cdots + a_{n-1}x^{n-1}\n\\]\n이 때 \\(V_{n} (x_i)= y_i\\) 이므로 아래와 같은 선형방정식 \\(\\boldsymbol{Va}=\\boldsymbol{y}\\) 을 통해 계수 \\(a_0,\\ldots,\\,a_{n-1}\\) 을 구할 수 있다.\n\\[\n\\underbrace{\\begin{bmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} \\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^{n-1} \\\\ \\vdots & & & & \\vdots \\\\ 1 & x_n & x_n^2 & \\cdots & x_n^{n-1}\\end{bmatrix}}_{{\\Large \\boldsymbol{V}}} \\underbrace{\\begin{bmatrix}a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_{n-1}\\end{bmatrix}}_{{\\Large \\boldsymbol{a}}} = \\underbrace{ \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{bmatrix}}_{{\\Large\\boldsymbol{y}}}\n\\tag{1}\\]\n\n\n\n연습문제 1 위의 \\(\\boldsymbol{V}\\) 와 같이 주어진 발데르몽드 행렬의 행렬식은 다음과 같음이 잘 알려져 있다.\n\\[\n\\det(\\boldsymbol{V}) = \\prod_{1\\le i&lt;j\\le n} (x_j-x_i)\n\\]\n즉 어떤 두 \\(x_i,\\,x_j\\) 같지 않으면 행렬식은 \\(0\\) 이 아니므로 가역행렬이다. 따라서 \\(a_0,\\ldots,\\,a_{n-1}\\) 이 유일하게 정해진다.\n\n\n\n데이터로 부터 행렬 \\(\\boldsymbol{V}\\) 와 벡터 \\(\\boldsymbol{y}\\) 를 구할 수 있으며, 어떤 두 \\(x_i,\\,x_j\\) 도 같지 않으므로 선형방정식의 풀이를 통해 다항식의 계수 벡터 \\(\\boldsymbol{a}\\) 를 구할 수 있다.\n\n\n구현\n발데르몽드 행렬을 생성하는 가장 직관적이고 이해하기 쉬운 코드는 다음과 같다. xp 가 배열 \\(x_1,\\ldots,\\,x_n\\) 에 대한 변수라고 하자.\nV = Array{Float64}(undef, length(xp), length(xp))\n\nfor (i, x) in enumerate(xp), j in eachindex(xp)\n    V[i, j] = x^(j-1)\nend\n그러나 가장 효율적인 코드, 특히 xp 의 크기가 클 때 효율적인 코드는 array comprehension 을 사용하는 것이다.\nV = [x^(j-1) for x in xp, j in 1:length(xp)]\n이렇게 생성된 V 와 데이터로 알고 있는 \\(y_1,\\ldots,\\, y_n\\) 에 대한 배열 yp 에 대해 다항식은\nSimplePolynomial(V\\yp)\n로 구할 수 있다. 이것은 NAJ.jl 의 valdermond_polynomial 함수로 구현되었다.\nfunction valdermond_polynomial(\n    xp::AbstractVector{T1}, \n    yp::AbstractVector{T2}\n    ) where {T1&lt;:Number, T2&lt;:Number}\n    \n    N = length(xp)\n    @assert length(xp) == length(yp)\n    V = [x^(j-1) for x in xp, j in 1:length(xp)]\n    return SimplePolynomial(V\\yp)\nend\nIn [1]: using NAJ\n\nIn [2]: p1 = valdermond_polynomial([1,2,3,4], [1,3,4,7])\nOut[2]: Simple Polynomial{Float64}( -5.0  + 9.0 x^1 - 3.5 x^2 + 0.5 x^3)\n\n\n\n\n2.2 라그랑쥬 다항식을 이용한 보간법\n가장 직관적으로 이해하기 쉬운 라그랑쥬 방법을 알아보자. \\(n\\) 개의 데이터 \\(\\{(x_i,\\,y_i):i=1,\\ldots\\, n\\}\\) 에 대해 함수 \\(l_k(x)\\) 를 다음과 같이 정의하자.\n\\[\n\\begin{aligned}\nl_k (x) &:= \\prod_{i=1,\\, i \\ne k}^n \\dfrac{(x-x_i)}{(x_k-x_i)} \\\\\n&= \\dfrac{(x-x_1)\\cdots (x-x_{k-1})(x-x_{k+1})\\cdots (x-x_n)}{(x_k-x_1)\\cdots (x_k-x_{k-1})(x_k-x_{k+1})\\cdots (x_k-x_n)}.\n\\end{aligned}\n\\tag{2}\\]\n이 다항식은 다음을 만족한다.\n\\[\nl_k (x_j) = \\delta_{xj} = \\left\\{ \\begin{array}{ll} 1 \\qquad & j=k,\\\\ 0 \\qquad &\\text{otherwise} \\end{array}\\right.\n\\]\n다항식 \\(L_n(x)\\) 를 다음과 같이 정의하자.\n\\[\nL_{n}(x) := \\sum_{k=1}^n y_k \\, l_k(x)\n\\tag{3}\\]\n그렇다면 \\(i=1,\\ldots,\\,n\\) 에 대해 \\(L_{n}(x_i)=y_i\\) 인 것은 쉽게 알 수 있다. 즉 \\(L_{n} (x)\\) 는 주어진 점들을 모두 지나는 다항식이다. 이 다항식 \\(L_{n}(x)\\) 를 라그랑쥬 다항식이라고 한다.\n\n구현\n라그랑쥬 다항식은 다음과 같이 구현 할 수 있다.\nfunction lagrange_polynomial(\n    xp::AbstractVector{T1}, \n    yp::AbstractVector{T2}) where {T1&lt;:Number, T2&lt;:Number}\n\n    N = length(xp)\n    @assert length(xp) == length(yp)\n    \n    r = SimplePolynomial([zero(T2), ])\n    for i in 1:N\n        coef = yp[i]\n        rt = one(T2)\n        for j in 1:N\n            if i ≠ j\n                @inbounds coef = coef/(xp[i]-xp[j])\n                @inbounds rt = rt*SimplePolynomial([-xp[j], 1.0])\n            end\n        end\n        r += rt*coef\n    end\n    return r\nend\n\nIn [1]: using NAJ\n\nIn [2]: p2 = lagrange_polynomial([1,2,3,4], [2,4,1,8])\nOut[2]: Simple Polynomial{Float64}( -20.0  + 37.0 x^1 - 17.5 x^2 + 2.5 x^3)\n\n\n\n\n2.3 뉴턴 보간법\n뉴턴 보간법은 \\(n\\) 개의 데이터 \\(\\{ (x_i,\\, y_i): i=1,\\ldots,\\,n\\}\\) 를 지나는 \\(n-1\\) 차 다항식을 \\[\n\\begin{aligned}\nN_{n}(x) & = a_0 + a_1 (x-x_1)+ a_2 (x-x_1)(x-x_2)+ \\cdots \\\\\n& \\qquad + a_{n-1}(x-x_1)\\cdots (x-x_{n-1})\n\\end{aligned}\n\\tag{4}\\]\n으로 두고 그 계수 \\(a_0,\\,a_1,\\ldots,\\,a_{n-1}\\) 을 찾는다. 우선 \\(n_k(x)\\) 를 아래와 같이 정의하자. \\[\nn_k (x) := \\left\\{ \\begin{array}{ll} 1 & \\text{if }k=0 \\\\ \\displaystyle \\prod_{i=1}^k (x-x_i) &\\text{if }k\\ne 0\\end{array}\\right.\n\\tag{5}\\]\n이 \\(n_k(x)\\) 를 뉴턴 기저 다항식 이라 한다. 뉴턴 다항식 \\(N_n (x)\\) 는 다음과 같다.\n\\[\nN_{n}(x) = \\sum_{k=0}^{n-1} a_k n_k (x).\n\\]\n이 때 \\(N_n(x_1)=a_1= y_1\\) 이며, \\(k=1,\\ldots,\\, n-1\\) 에 대해\n\\[\na_k = \\dfrac{y_{k+1} - a_1 - a_2(x_{k+1}-x_1) - \\cdots - a_{k-1}(x_{k+1}-x_1)\\cdots (x_{k+1}-x_k)}{(x_{k+1}-x_1)\\cdots (x_{k+1}-x_k)}\n\\]\n임을 알 수 있다. 이것을 아래와 같은 선형방정식 \\(\\boldsymbol{Na}=\\boldsymbol{y}\\) 를 통해 표현 할 수 있다.\n\\[\n\\underbrace{\\begin{bmatrix} 1 & 0 & 0 & \\cdots & 0 \\\\ 1 & n_1(x_2) & 0 & \\cdots & 0 \\\\ 1 & n_1(x_3) & n_2(x_3) & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & n_{n-1}(x_n) & \\cdots & \\cdots & n_{n-1}(x_n)  \\end{bmatrix}}\n_{{\\Large \\boldsymbol{N}}}\n\\underbrace{\\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_{n-1}\\end{bmatrix}}_{{\\Large \\boldsymbol{a}}} =\n\\underbrace{\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\y_n \\end{bmatrix}}_{{\\Large \\boldsymbol{y}}}.\n\\tag{6}\\]\n뉴턴 보간법을 행렬로 표현하는 \\(\\boldsymbol{N}\\) 은 하삼각행렬로 방데르몽드 방법이나, 라그랑쥬 방법에 의한 보간법보다 그 계수를 구하는 것이 빠르고 수치해석적으로 안정적이다.\n\n\n구현\nfunction newton_polynomial(\n    xp::AbstractVector{T1}, \n    yp::AbstractVector{T2}) where {T1&lt;:Number, T2&lt;:Number}\n    n = length(xp)    \n    @assert n == length(yp)\n    T = promote_type(T1, T2)\n    N = LowerTriangular(ones(T, n, n))\n    for j in 2:n, i in j:n\n        @inbounds N[i, j] = N[i, j-1]*(xp[i] - xp[j-1]) \n    end\n    a = N\\yp\n    r = SimplePolynomial([a[1], ])\n    for i in 2:(n)\n        @inbounds r += a[i] * polynomial_from_roots(xp[1:i-1])\n    end\n    return r\n\nend\nIn [1]: using NAJ\n\nIn [2]: p3 = newton_polynomial([1,2,3,4], [2,4,1,8])\nOut[2]: Simple Polynomial{Float64}( -20.0  + 37.0 x^1 - 17.5 x^2 + 2.5 x^3)\n\n\n\n세 방법의 비교\n주어진 점에 대해 보간하는 세가지 방법은 수학적으로 동등하지만 수치해석적으로는 뉴턴 방법을 권장하는데 그 이유는 다음과 같다.\n\n일반적으로 라그랑쥬 다항식보다 뉴턴 다항식을 빠르게 계산 할 수 있다.\n\\(n\\) 개의 데이터에 대해 다항식을 구한 후 데이터를 추가할 때 라그랑쥬 다항식은 모든 계수를 다시 계산해야 하지만 뉴턴 다항식은 위의 이미 알고 있는 계수와 점들을 를 이용하여 계산할 수 있다. 여기서는 새로운 점을 추가하는 것을 구현하지는 않았다.\n데이터가 어느 정도 많이 주어졌을 때 라그랑쥬 다항식보다 뉴턴 다항식이 수치해석적으로 안정하다.\n\n\n\n\n\n2.4 다항식을 이용한 보간법의 오차\n\n\n명제 1 \\(f\\in C^{n}[a,\\,b]\\) 이고 \\(\\{x_1,\\ldots,\\,x_n\\}\\subset [a,\\,b]\\) 이며 \\(i\\ne j \\implies x_i \\ne x_j\\) 라고 하자. \\(P_{n-1}(x)\\) 가 \\(P_{n-1}(x_i) = f(x_i)\\) (\\(i=1,\\ldots,\\,n\\)) 인 \\(n-1\\) 차 다항식이라면\n\\[\nf(x) - P_{n-1}(x) = \\dfrac{f^{(n)}(\\xi)}{n!} \\prod_{j=1}^n (x-x_j)\n\\]\n를 만족하는 \\(\\xi \\in [a,\\,b]\\) 가 존재한다.\n\n\n\n\n(증명). \\(F(t)\\) 를 다음과 같이 정의하자.\n\\[\nF(t) = f(t) - P_{n-1} (t) - (f(x)-P_{n-1} (x)) \\prod_{i=1}^n \\dfrac{(t-x_i)}{(x-x_i)} \\tag{1}\n\\]\n이 때 \\(t=x_k,\\, k=1,\\ldots,\\, n\\) 에 대해 \\(F(x_k)=0\\) 이다. 또한\n\\[\nF(x) = f(x)-P_{n-1}(x) - (f(x)-P_{n-1}(x)) = 0\n\\]\n이므로 \\(x,\\,x_1,\\ldots,\\,x_n\\) 의 \\(n+1\\) 개의 점에서 \\(F(t)=0\\) 이 된다. 일반화된 롤의 정리 를 이용하면 \\(n\\) 개의 점에서 \\(F'(t)=0\\) 이며, 이것을 반복하면 \\(F^{(n)}(\\xi)=0\\) 을 만족하는 \\(\\xi\\in [a,\\,b]\\) 가 존재한다. 그런데 \\(P_{n-1}(x)\\) 는 \\(n-1\\) 차 다항식이므로 \\(P_{n-1}^{(n)} (x)=0\\) 이며 (\\(1\\)) 의 \\(\\displaystyle \\prod_{i=1}^n \\dfrac{(t-x_i)}{(x-x_i)}\\) 에 대한 \\(n\\) 차 미분은\n\\[\n\\dfrac{d^n}{dt^n} \\left(\\prod_{i=1}^n \\dfrac{(t-x_i)}{(x-x_i)}\\right) = \\dfrac{n!}{\\prod_{i=1}^n (x-x_i)}\n\\]\n이므로,\n\\[\nF^{(n)}(\\xi) = f^{(n)}(\\xi) - (f(x)-P_{n-1} (x))\\dfrac{n!}{\\prod_{i=1}^n (x-x_i)} = 0\n\\]\n이다. 즉\n\\[\nf(x) = P_{n-1}(x) + \\dfrac{f^{(n)}(\\xi)}{n!}\\prod_{i=1}^n (x-x_i)\n\\]\n를 만족하는 \\(\\xi \\in [a,\\,b]\\) 가 존재한다. \\(\\square\\)\n\n\n\n\n2.5 전역적 보간법의 문제점\n데이터 갯수가 많을 경우 고차다항식으로 보간을 하게 되는데, 이 경우 다항식이 심하게 진동하는 경우가 있을 수 있다. 아래 그림의 데이터는 15개의 포인트로 가우스 분포 \\(e^{-(x-x_0)/2\\sigma^2}\\) 를 그린 것이며, 라그랑쥬나 뉴턴 방법을 통해 14차 다항식으로 보간한 결과를 같이 보여준다.\n\n\n\n\n\n\n그림 1: Oscilating behavior\n\n\n\n게다가 데이터 갯수가 상당히 많으면 수치적으로 불안정해 진다. 예를 들어, 25개의 데이터에 대한 가우스 분포를 다항식으로 보간하면 아래와 같이 더 이상 사용할 수 없는 결과가 발생한다.\n\n\n\n\n\n\n그림 2: Oscilating behavior2\n\n\n\n참고로 위의 그림에서 라그랑제 방법이나 뉴턴 방법에 의한 차이는 방법의 고유한 성질에 의한 차이가 아니라 계산 과정에서 발생하는 것으로 같은 방법을 사용했다고 하더라도 구현에 따라 다른 모습으로 나타 날 수 있다.",
    "crumbs": [
      "수치해석 I",
      "다항식을 이용한 1차원 보간법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/05_interpolation.html#sec-Neville_method",
    "href": "src/numerical_analysis_using_julia/05_interpolation.html#sec-Neville_method",
    "title": "다항식을 이용한 1차원 보간법",
    "section": "3 네빌 방법 (Neville’s Method)",
    "text": "3 네빌 방법 (Neville’s Method)\n우리가 이산적인 데이터를 다항식으로 보간한다고 할 때, 많은 경우 보간된 다항식 자체에 관심있는 것이 아니라 데이터에 포함되지 않지만 데이터의 범위 안에 들어가는 몇몇 점에서 보간된 값만을 얻고 싶을 때가 있다. 이 경우 사용할 수 있는 것이 네빌 방법이다. 즉 다항식을 이용한 전역적인 보간법에서의 세 방법이 \\(\\{(x_i,\\,y_i): i=1,\\ldots,\\,n\\}\\) 를 모두 지나는 \\(n-1\\) 차 다항식을 구하는 방법이라면 네빌 방법은 다항식을 직접 구한다기 보다는 다항식으로 보간했을 때의 값만을 구하는 방법이다.\n\\(n\\) 개의 데이터 \\(\\{(x_i,\\,y_i): i=1,\\ldots,\\,n\\}\\) 가 주어졌다고 하자. 우리가 어쨋든 \\(x_k,\\ldots,\\,x_{m-1}\\) 에서 각각 \\(y_k,\\ldots,\\,y_{m-1}\\) 과 일치하는 \\(P_{k,\\ldots,\\,m-1}(x)\\) 과 \\(x_{k+1},\\ldots,\\,x_{m}\\) 에서 각각 \\(y_{k+1},\\ldots,\\,y_{m}\\) 과 일치하는 \\(P_{k+1,\\ldots,\\,m}\\) 을 알고 있다고 하자. 그렇다면 아래의 식으로부터 구한 \\(P_{k,\\ldots,\\,m}\\) 은 \\(x_k,\\ldots,\\,x_m\\) 에서 \\(y_k,\\ldots,\\,y_m\\) 과 일치한다.\n\\[\nP_{k,\\ldots,\\,m}(x) = \\dfrac{(x-x_k)P_{k+1,\\ldots,m}(x) - (x-x_m)P_{k,\\ldots,\\,m-1}(x)}{x_m-x_k}\n\\tag{7}\\]\n4 개의 점 \\(\\{(x_1,\\,y_1),\\ldots,\\,(x_4,\\,y_4)\\}\\) 으로 이루어진 데이터를 다항식으로 보간한다고 생각하자. 각 \\(P_i(x)\\) 는 \\(y_i\\) 의 값을 갖는 상수함수로 놓는다. \\(P_1,\\,P_2,\\,P_3,\\,P_4\\) 를 이용하여 \\(P_{1,\\,2}\\), \\(P_{2,\\,3}\\), \\(P_{3,4}\\) 를 계산하며, 이를 이용하여 \\(P_{1,2,3}\\) 와 \\(P_{2,3,4}\\) 를 계산한다. 마지막으로 이를 이용하여 \\(P_{1,2,3,4}\\) 를 계산하면 종료된다.\n\n\n\n\n\n\n\n\n\n\n회차\n1\n2\n3\n4\n\n\n\n\n\\(x_1\\)\n\\(P_1 (x)\\)\n\n\n\n\n\n\n\n\\(P_{1, 2} (x)\\)\n\n\n\n\n\\(x_2\\)\n\\(P_2 (x)\\)\n\n\\(P_{1, 2, 3} (x)\\)\n\n\n\n\n\n\\(P_{2, 3} (x)\\)\n\n\\(P_{1, 2, 3, 4} (x)\\)\n\n\n\\(x_3\\)\n\\(P_3 (x)\\)\n\n\\(P_{2, 3, 4} (x)\\)\n\n\n\n\n\n\\(P_{3, 4} (x)\\)\n\n\n\n\n\\(x_4\\)\n\\(P_4 (x)\\)\n\n\n\n\n\n\n\n\n3.1 구현\n네빌 방법에 대한 코드는 아래와 같다. Julia 의 배열 comprehension 방법을 사용하였기 때문에 한번에 이해하기 어려울 수 있다는 것에 주의하라.\nfunction neville(\n    xp::AbstractVector{T1}, \n    yp::AbstractVector{T2}, \n    x::Real) where {T1&lt;:Real, T2&lt;:Real}\n    @assert length(xp) == length(yp)\n    P = copy(yp)\n    for i in 1:(length(xp)-1)\n        P = [((x-xp[j+i])*P[j] - (x-xp[j])*P[j+1])/(xp[j]-xp[j+i]) for j in 1:(length(xp)-i)]\n    end\n    return P[1]\nend",
    "crumbs": [
      "수치해석 I",
      "다항식을 이용한 1차원 보간법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/05_interpolation.html#sec-local_interpolation_using_polynomial",
    "href": "src/numerical_analysis_using_julia/05_interpolation.html#sec-local_interpolation_using_polynomial",
    "title": "다항식을 이용한 1차원 보간법",
    "section": "4 다항식을 이용한 구간별 보간법",
    "text": "4 다항식을 이용한 구간별 보간법\n앞서의 전역적 보간법은 주어진 전체 데이터를 이용한 보간법인데 반해 지금 소개하는 보간법은 구하고자 하는 변수값 근처의 몇개의 점만을 이용하여 다항식으로 보간한다. 따라서 전역적 보간법에서 보이는 진동이 없으며, 데이터가 많더라도 사용 할 수 있다.\n\n\n4.1 최근접 보간법 (Nearest Neighbor Interpolation)\n가장 간단한 보간법 은 최근접 보간법 (nearest neighbor interpolation) 으로 \\(x_1,\\ldots,\\,x_N\\) 가운데 \\(x\\) 값에 가장 가까운 값을 취하는 방법이다. 즉 \\(x_i\\le x&lt;x_{i+1}\\) 일 때 \\(x_i\\) 와 \\(x_{i+1}\\) 중 \\(x\\) 와의 거리가 더 가까운 값을 \\(f(x)\\) 로 선택하는 방법이다. 가장 단순하면서도 빠르지만 보간법의 결과로 나온 함수는 연속함수가 아니다.\n\n\n\n4.2 선형 보간법 (Linear Interpolation)\n\\(x_{i}\\le x &lt; x_{i+1}\\) 일 때 \\((x_i,\\,y_i)\\) 와 \\((x_{i+1},\\, y_{i+1})\\) 의 두 점을 지나는 직선 \\(l_i (x)\\) 는 다음과 같다.\n\\[\nl_i (x) = \\dfrac{y_{i+1}-y_i}{x_{+1}-x_i} (x-x_i) + y_i\n\\tag{8}\\]\n이 직선 \\(l_i(x)\\) 값으로 \\(f(x)\\) 값을 추산하는 것을 선형보간법이라 한다. 선형 보간법으로 구한 함수는 연속이지만 일반적으로 미분가능하지 않다.\n\n\n\n4.3 삼차함수 보간법 (Cubic interpolation)\n\\(x_{i}\\le x&lt; x_{i+1}\\) 일 때 \\((x_{i-1},\\, y_{i-1})\\), \\((x_i,\\,y_{i})\\), \\((x_{i+1},\\,y_{i+1})\\), \\((x_{i+2},\\, y_{i+2})\\) 를 이용하여 3차함수를 구하여 보간한다. 보간하는 함수는 앞서 언급한 발데르몽드, 라그랑쥬, 뉴턴 방식중에 사용할 수 있으며, 여기서는 뉴턴 다항식을 이용하여 보간하도록 한다. \\((x_{i-1},\\, y_{i-1})\\), \\((x_i,\\,y_{i})\\), \\((x_{i+1},\\,y_{i+1})\\), \\((x_{i+2},\\, y_{i+2})\\) 를 이용하여 구한 3차 다항식을 \\(N_i(x)\\) 라고 하면 \\(x_1\\le x&lt;x_2\\) 일 경우에는 \\(x_0\\) 가 없으므로 \\(N_1(x)\\) 를 이용하며, \\(x_{N-1}\\le x \\le x_{N}\\) 일 경우에는 \\(N_{N-2}\\) 를 이용하여 보간한다. 삼차함수 보간법을 사용하여 구한 함수는 연속이며 선형 보간법보다 나은 결과를 보여주지만 역시 미분 가능하지 않다.\n일단 위의 세가지를 julia 로 구현해 보자. \\(x_1,\\ldots,\\,x_n\\) 과 \\(y_1,\\ldots,\\,y_n\\) 이 각각 벡터 xp, yp 라고 하고 \\(x_1 &lt; x_2 &lt; \\cdots &lt; x_n\\) 임을 가정하자.\n\"\"\"\n    nearest neighbor interpolation\n\"\"\"\nfunction interp_nearest(\n    xp::AbstractVector{T}, \n    yp::Vector{S}, \n    x::AbstractVector{R}\n    ) where {T&lt;:Real, S&lt;:Real, R&lt;:Real}\n    \n    @assert length(xp) == length(yp)\n    N = length(xp)\n    result = zeros(x)\n    for (i, v) in enumerate(x)\n        if v&lt;xp[1] || v&gt;xp[end]\n            result[i] = zero(T)\n        else \n            ind = findfirst(xs-&gt;(xs&gt;=v), xp)-1\n            if ind == 0\n                result[i]=yp[1]\n            elseif v-xp[ind] &gt; xp[ind+1]-v\n                result[i] = yp[ind+1]\n            else \n                result[i] = yp[ind]\n            end\n        end\n    end\n    return result\nend\n\n\"\"\"\n    linear interpolation\n\"\"\"\nfunction interp_linear(xp::AbstractVector{T}, yp::Vector{S}, x::AbstractVector{R}) where {T&lt;:Real, S&lt;:Real, R&lt;:Real}\n    @assert length(xp) == length(yp)\n    N = length(xp)\n    result = zero(x)\n    for (i, v) in enumerate(x)\n        if v&lt;xp[1] || v&gt;xp[end]\n            result[i] = zero(T)\n        else \n            ind = findfirst(xs-&gt;(xs&gt;=v), xp)-1\n            if ind == 0\n                result[i] = yp[1]\n            else\n                result[i] = (yp[ind+1]-yp[ind])/(xp[ind+1]-xp[ind])*(v-xp[ind]) + yp[ind]\n            end\n        end\n    end\n    return result\nend\n\n\"\"\"\n    cubic interpolation\n\"\"\"\nfunction interp_cubic(xp::AbstractVector{T}, yp::Vector{S}, x::AbstractVector{R}) where {T&lt;:Real, S&lt;:Real, R&lt;:Real}\n    @assert length(xp) == length(yp)\n    N = length(xp)\n    result = zero(x)\n    for (i, v) in enumerate(x)\n        \n        if v&lt;xp[1] || v&gt;xp[end]\n            result[i] = zero(T)\n        else \n            ind = findfirst(xs-&gt;(xs&gt;=v), xp)-1\n            if ind ∈ (0, 1)\n                xs, ys = xp[1:4], yp[1:4]\n            elseif ind ∈ (N, N-1)\n                xs, ys = xp[end-3:end], yp[end-3:end]\n            else \n                xs, ys = xp[ind-1:ind+2], yp[ind-1:ind+2]\n            end\n            result[i] = newton_polynomial(xp, yp)(v)\n        end\n    end\n    return result\nend\n\n이것을 다음 코드로 확인해 보았다.\nusing Plots\nxp = 0.0:1.0:15.0 \nyp = cos.(xp) .* exp.(-xp/10.0)\nxt = collect(0.0:0.1:15.0) \nscatter(xp, yp, label = L\"\\cos \\theta\", dpi=300)\nplot!(xt, interp_nearest(xp, yp, xt), lw=2, lc=:red, label=L\"\\mathrm{Nearest}\")\nplot!(xt, interp_linear(xp, yp, xt), lw=2, lc=:blue, label=L\"\\mathrm{Linear}\")\nplot!(xt, interp_cubic(xp, yp, xt), lw=2, lc=:green, label=L\"\\mathrm{Cubic}\")\n\n\n\n\n\n\n\n그림 3: Interpolation\n\n\n\n\n\n\n4.4 Cubic Spline Interpolation\n위에서 설명한 세가지 보간법은 연속이 아니거나 연속이더라도 미분 가능하지 않았다. Spline 보간법은 보간하고자 하는 구간 내에서 미분가능한 함수를 구해준다.\n\\(n\\) 개의 데이터 \\(\\{(x_i,\\,y_i): i=1,\\ldots,\\,n\\}\\) 에 대해 \\([x_i,\\,x_{i+1}]\\) 구간을 보간하는 다항식을 \\(s_i(x)\\) 라 하자. 이 때 각각의 \\(s_i(x)\\) 를 3차 다항식으로 하고\n\\[\n\\begin{aligned}\ns_i(x_{i}) & = y_{i}\\\\\ns_i(x_{i+1}) &= y_{i+1}\\\\\ns'_i(x_{i+1}) &= s'_{i+1}(x_{i+1}) \\\\\ns''_i(x_{i+1}) &= s''_{i+1}(x_{i+1})\n\\end{aligned}\n\\tag{9}\\] 을 만족하도록 보간하는 것을 cubic spline interpolation 이라 한다. \\([x_i,\\,x_{i+1}]\\) 구간에서의 삼차함수 \\(s_i(x)\\) 를 \\[\ns_{i}(x) = a_i + b_i x + c_ix^2 + d_i x^3\n\\]\n이라 하자. \\(s_i(x)\\) 에 대한 일계 도함수와 이계 도함수는\n\\[\n\\begin{aligned}\ns'_i (x) &= b_i + 2c_i x + 3d_i x^2,\\\\\ns''_{i}(x) &= 2c_i + 6d_ix\n\\end{aligned}\n\\]\n이므로, \\(a_{i-1},\\,b_{i-1},\\, c_{i-1},\\, d_{i-1}\\) 로부터\n\\[\n\\begin{array}{c}\na_i + b_i x_{i}+ c_i(x_{i})^2+ d_{i} (x_{i})^3 = y_{i} \\\\\na_i + b_i x_{i+1}+ c_i(x_{i+1})^2+ d_{i} (x_{i+1})^3 = y_{i+1} \\\\\nb_i + 2c_i (x_{i+1})+ 3d_i (x_{i+1})^2 =b_{i+1} + 2c_{i+1} (x_{i+1})+ 3d_{i+1} (x_{i+1})^2 \\\\\n2c_i + 6d_ix_{i+1} = 2c_{i+1}+ 6d_{i+1}x_{i+1}\n\\end{array}\n\\tag{10}\\]\n의 조건을 얻었으며, 이를 통해 \\(a_i,\\,b_i,\\,c_i,\\,d_i\\) 를 구할 수 있다. 위 식에서 아래의 두 식은 \\(i=1\\) 일 때 사용 할 수 없으므로 우리는 추가로 조건을 두어야 한다. 이 때 보통 아래와 같은 2가지의 경계조건 가운데 선택한다.\n\nFree or natural boundary: \\(s''_1 (x_1) = s''_{n-1} (x_n) = 0\\)\nClamped boundary: \\(s'_1(x_1) = f'(x_0),\\, s'_{n-1} (x_n) = f'(x_n)\\)\n\nClampled boundary 일 경우 \\(f'(x_0),\\, f'(x_n)\\) 값은 주어져야 한다.\n\n\\(n-1\\) 개의 구간에 대해 각각 4 개의 변수를 결정해야 하므로 미지수 행렬을 \\(4(n-1)\\times 1\\) 행렬 \\(\\boldsymbol{a}\\) 으로 두자. \\(j=1,\\ldots,\\,n-1\\) 에 대해\n\\[\na_{4j-3} = a_j, \\, a_{4j-2} = b_j, \\, a_{4j-1} = c_j,\\, a_{4j} = d_j\n\\]\n라고 하면 미지수 전체에 대한 행렬이다. 이제 주어진 데이터와 미지수와의 관계를 이용하여 선형방정식 \\(\\boldsymbol{Ma}=\\boldsymbol{b}\\) 를 만들자. 이 때 \\(\\boldsymbol{M}\\) 은 \\(4(n-1) \\times 4(n-1)\\) 행렬이다. \\(j=1,\\ldots,\\, n-1\\) 에 대해 \\(\\boldsymbol{M}\\) 의 \\(4(j-1)+1\\) 행부터 \\(4(j-1)+4\\) 행까지, \\(4(j-1)+1\\) 열부터 \\(4(j-1)+8\\) 열까지의 \\(4 \\times 8\\) 부분행렬을 \\(\\boldsymbol{M}_j\\) 라고 하자. 위의 조건 식 10 으로부터 \\(\\boldsymbol{M}_j\\) 가 포함하지 않는 성분은 모두 \\(0\\) 임을 알 수 있다. \\(i=1,\\ldots, n-2\\) 에 대해 \\(\\boldsymbol{M}_i\\) 는 다음과 같다. \\[\n\\boldsymbol{M}_j = \\begin{bmatrix}\n1 & x_i & (x_i)^2 & (x_i)^3 & 0 & 0 & 0 & 0\\\\\n1 & x_{i+1} & (x_{i+1})^2 & (x_{i+1})^3 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 2x_{i+1} & 3(x_{i+1})^2 & 0 & -1 & -2 x_{i+1} & -3(x_{i+1})^3\\\\\n0 & 0 & 2 & 6x_{i+1} & 0 & 0 & -2 & -6x_{i+1}\n\\end{bmatrix}\n\\]\n\\(x_{n+1}\\) 은 주어져 있지 않으므로 \\(\\boldsymbol{M}_{n-1}\\) 은 \\(2 \\times 8\\) 행렬이며 다음과 같다.\n\\[\\boldsymbol{M}_{n-1} = \\begin{bmatrix}\n1 & x_{n-1} & (x_{n-1})^2 & (x_{n-1})^3 & 0 & 0 & 0 & 0\\\\\n1 & x_{n} & (x_{n})^2 & (x_{n})^3 & 0 & 0 & 0 & 0\\\\\n\\end{bmatrix}\n\\]\n또한 역시 식 10 로 부터 \\(4(n-1)\\times 1\\) 행렬 \\(\\boldsymbol{y}\\) 는 \\(i=1,\\ldots,\\,n\\) 에 대해 다음과 같이 정할 수 있다.\n\\[\nb_{4j-3}= y_i, \\, b_{4j-2}= y_{i+1},\\, b_{4j-1} = 0,\\, b_{4j}= 0\n\\]\n이제 \\(\\boldsymbol{M}\\) 의 마지막 두 행을 경계조건에 따라 정해야 한다.\n\n\nFree boundary\n우선 첫번째 경계조건이 주어졌을 경우 \\[\n\\begin{aligned}\ns''_1 (x_1)=0 &\\implies 2c_1+ 6 d_1 (x_1)=0, \\\\\ns''_{n-1}(x_n) = 0 &\\implies 2c_{n-1} + 6d_{n-1}(x_n) = 0\n\\end{aligned}\n\\]\n를 얻는다. 이 조건으로부터,\n\\[\nM_{4n-5, 3}=2,\\, M_{4n-5, 4}= 6x_1,\\, M_{4n-4, 4n-5} = 2,\\, M_{4n-4, 4n-4} = 6x_n\n\\]\n을 얻는다.\n\n\n\nClamped boundary\n\\[\n\\begin{aligned}\ns'_1 (x_1)=f'(x_0) &\\implies b_1 + 2c_1(x_1)+ 3d_1 (x_1)^2=f'(x_0), \\\\\ns'_{n-1}(x_n) = f'(x_n) &\\implies b_{n-1}+ 2c_{n-1}(x_n) + 3d_{n-1}(x_n)^2 = f'(x_n)\n\\end{aligned}\n\\]\n를 얻는다. 이 조건으로부터,\n\\[\n\\begin{aligned}\nM_{4n-5, 2}=1,\\, M_{4n-5, 3}=2x_1,\\, M_{4n-5, 4} = 3(x_1)^2,\\, b_{4n-5}=f'(x_0), \\\\\nM_{4n-4, 4n-6}=1,\\, M_{4n-4, 4n-5}=2x_n,\\, M_{4n-4,4n-4}=3(x_n)^2,\\, b_{4n-4}=f'(x_n)\n\\end{aligned}\n\\]\n을 얻는다.\n\n이제 행렬 \\(\\boldsymbol{M}\\) 과 벡터 \\(\\boldsymbol{b}\\) 를 얻었으므로 \\(\\boldsymbol{Ma}=\\boldsymbol{b}\\) 를 만족하는 \\(\\boldsymbol{a}\\) 를 구할 수 있다. \\(\\boldsymbol{a}\\) 는 이 보간법의 각 구간에서의 3차 다항식의 계수를 결정하기 때문에 우리는 보간법을 사용하여 정해진 구간 내의 모든 값을 계산 할 수 있다.\n여기에 사용된 모든 보간법은 interpolation.jl 에 Interpolation1D 객체로 구현되었다.\nusing Plots, LaTeXStrings, LinearAlgebra, Plots, SparseArrays, Printf\ninclude(\"../lib/interpolation.jl\")\n\nxp = 0.0:0.75:15.0 # change step to vector\nyp = cos.(3 .* xp) .* exp.(-xp/10.0)\nyp[8:9]=[-0.15 -0.3]# \nxt = 0.0:0.05:15.0 # change step to vector\n\ninterp1=Interpolator1D(xp, yp, :cubic)\ninterp2=Interpolator1D(xp, yp, :cubic_spline_naturalbc)\ninterp3=Interpolator1D(xp, yp, :cubic_spline_clampedbc, [0.0, 0.0])\n\nscatter(xp, yp, label = L\"\\mathrm{Data}\", dpi=300)\nplot!(xt, interp1.(xt), lw=2, lc=:green, label=L\"\\mathrm{Cubic}\")\nplot!(xt, interp2.(xt), lw=2, lc=:red, ls = :dash, label=L\"\\mathrm{Cubic\\;Spline\\;with\\;natural\\;BC}\")\nplot!(xt, interp3.(xt), lw=2, lc=:blue, ls = :dot, label=L\"\\mathrm{Cubic\\;Spline\\;with\\;clamped\\;BC}\")\n이 결과로 다음의 그래프를 얻었다.\n\n\n\n\n\n\n그림 4: Cubic Spline Interpolation",
    "crumbs": [
      "수치해석 I",
      "다항식을 이용한 1차원 보간법"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html",
    "href": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html",
    "title": "일변수 함수의 미분과 적분",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 I",
      "일변수 함수의 미분과 적분"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#일변수-함수의-미분",
    "href": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#일변수-함수의-미분",
    "title": "일변수 함수의 미분과 적분",
    "section": "1 일변수 함수의 미분",
    "text": "1 일변수 함수의 미분\n이 장에서는 수치해석적으로 주어진 함수의 미분을 구하는 방법을 알아본다. 앞서 설명했듯이함수 \\(f: X\\subset \\mathbb{R} \\to \\mathbb {R}\\) 이 \\(x_0 \\in X\\) 에서 미분가능하다는 것은 다음의 극한이 존재한다는 것이다.\n\\[\nf'(x_0)=\\lim_{h\\to 0} \\dfrac{f(x_0+h)- f(x_0)}{h}\n\\]\n여기서 \\(h\\) 는 음수도, 양수도 가능하다. 그리고 우리가 앞서 배웠듯이 컴퓨터가 다룰수 있는 가장 \\(0\\) 에 가깝지만 가장 작은 수에는 한계가 있으므로 이러한 극한을 다룰수 없다. 따라서 수치해석적으로 미분을 계산할 때는 우리가 관심있는 영역 \\([a,\\,b]\\subset X\\) 를 분할하여 수열을 만들어 다룰 수 밖에 없다. 즉 우리는 \\(x_1=a&lt;x_2&lt;\\cdots &lt;x_n=b\\) 에 대해 \\(f(x_i) = y_i\\) 로 정하고 수열 \\((x_i)\\) 와 \\((y_i)\\) 를 이용하여 수치해석적으로 미분을 구한다. 이것은 미분가능한 함수가 주어졌을 때 뿐만 아니라 이산적인 값 \\(\\{(x_i,\\, y_i)\\}\\) 가 주어졌을 때도 동일하게 사용 할 수 있다. 우리는 \\([a,\\,b]\\) 를 일정한 간격으로 분할한, 즉 \\(x_{i+1}-x_i =h\\) 가 상수일 경우만 생각하기로 한다.\n\n\n1.1 전방차분과 후방차분\n수열 \\(y_1,\\,y_2,\\ldots\\) 에 대해 전방 차분 (forward difference) 는 다음과 같이 정의된다.\n\\[\n\\Delta_F y_i := y_{i+1}-y_i\n\\tag{1}\\]\n비슷한 방법으로 후방 차분 (backward difference) 은 다음과 같이 정의된다.\n\\[\n\\Delta_B y_i := y_{i}-y_{i-1}\n\\tag{2}\\]\n함수 \\(f(x)\\) 에 대해 어떤 특정한 간격 \\(h&gt;0\\) 가 주어졌을 때 전방 차분은 \\(\\Delta_F f(x) = f(x+h)-f(x)\\) 이며 후방차분은 \\(\\Delta_B f(x) = f(x)-f(x-h)\\) 이다. 앞서 말했듯이 수치해석에서는 \\(h\\to 0\\) 극한을 사용할 수 없으므로 차분을 이용하여 미분값을 계산한다.\n\n\n\n1.2 두 점을 이용한 미분\n함수 \\(f\\) 가 \\(C^2\\) 함수일 때 \\(f\\) 에 대한 테일러 전개는 아래와 같다.\n\\[\n\\begin{aligned}\nf(x-h) &= f(x) - f'(x)h + \\dfrac{f''(\\xi_1)}{2} h^2,  \\\\\nf(x+h) &= f(x) + f'(x)h + \\dfrac{f''(\\xi_2)}{2} h^2.\n\\end{aligned}\n\\tag{3}\\]\n\\(h&gt;0\\) 일 때 전방차분과 후방차분을 이용하여\n\\[\n\\begin{aligned}\nf'(x) &= \\dfrac{f(x+h)-f(x)}{h} + O(h) = \\dfrac{\\Delta_F f(x)}{h} + O(h) \\\\\n&= \\dfrac{f(x)-f(x-h)}{h} + O(h) = \\dfrac{\\Delta_B f(x)}{h} + O(h)\n\\end{aligned}\n\\tag{4}\\]\n로 근사 할 수 있다.\n\n\n\n1.3 세 점을 이용한 미분\n2차와 3차 테일러 다항식 까지 전개한다면 \\[\n\\begin{aligned}\nf(x-h) &= f(x) - f'(x)h + \\dfrac{f''(x)}{2} h^2 - \\dfrac{f^{(3)}(\\xi_1)}{6} h^3,  \\\\\n&= f(x) - f'(x)h + \\dfrac{f''(x)}{2} h^2 - \\dfrac{f^{(3)}(x)}{6} h^3 + \\dfrac{f^{(4)}(\\xi'_1)}{24}h^4,  \\\\\nf(x+h) &= f(x) + f'(x)h + \\dfrac{f''(x)}{2} h^2 + \\dfrac{f^{(3)}(\\xi_2)}{6} h^3, \\\\\n&= f(x) + f'(x)h + \\dfrac{f''(x)}{2} h^2 + \\dfrac{f^{(3)}(x)}{6} h^3 + \\dfrac{f^{(4)}(\\xi'_2)}{24}h^4\n\\end{aligned}\n\\tag{5}\\]\n를 만족하는 \\(\\xi_1,\\,\\xi'_1\\in (x-h,\\, x),\\, \\xi_2,\\,\\xi'_2 \\in (x,\\,x+h)\\) 가 존재한다.\n이로부터\n\\[\n\\begin{aligned}\nf(x+h)-f(x-h) &= 2f'(x) h + \\dfrac{1}{6} \\left(f^{(3)}(\\xi_2) +f^{(3)}(\\xi_1) \\right)h^3, \\\\\nf(x+h) -2 f(x) + f(x-h) &= f''(x) h^2 + \\dfrac{1}{24}\\left(f^{(4)}(\\xi'_1) +f^{(4)}(\\xi'_2) \\right)h^4\n\\end{aligned}\n\\]\n이므로 우리는 이것을 이용하여 1계 뿐만 아니라 2계 도함수에 대한 근사값도 구할 수 있다. 여기서 \\(f^{(3)}(\\xi_2) +f^{(3)}(\\xi_1)\\) 를 보자. \\(\\xi_1\\in [x-h,\\,x],\\, \\xi_2 \\in [x,\\,x+h]\\) 인데, \\(f\\in C^3_{[x-h,x+h]}\\) 이면 따름정리 : 여러 점의 경우 에 의해 \\(f^{(3)}(\\xi) = \\frac{1}{2}\\left(f^{(3)}(\\xi_2) +f^{(3)}(\\xi_1) \\right)\\) 를 만족하는 \\(\\xi\\in[x-h,\\,x+h]\\) 가 존재하며, 같은 이유로 \\(f\\in C^4_{[x-h,\\,x+h]}\\) 이면 \\(f^{(4)}(\\xi')=\\frac{1}{2}\\left(f^{(4)}(\\xi'_1) +f^{(4)}(\\xi'_2) \\right)\\) 를 만족하는 \\(\\xi'\\in [x-h,\\,x+h]\\) 가 항상 존재한다. 따라서 다음과 같이 도함수와 2계 도함수를 근사 할 수 있다.\n\\[\n\\begin{aligned}\nf'(x) &= \\dfrac{f(x+h)-f(x-h)}{2h} - \\dfrac{f^{(3)}(\\xi)}{6}h^2 ,\\\\\nf''(x) &=  \\dfrac{f(x+h) - 2f(x) + f(x-h)}{h^2} - \\dfrac{f^{(4)}(\\xi')}{12}h^2\n\\end{aligned}\n\\tag{6}\\]\n이 때 수치해석적으로 계산한 미분의 오차는 \\(h^2\\) 에 비례한다. 따라서 \\(h\\) 을 작게 잡을 수록 오차가 작아지며 충분히 작을 때 전방차분이나 후방차분보다 오차가 작을 것을 기대 할 수 있다. 위의 식에 의해 구한 미분의 추정값을 중앙 차분 (central difference) 라고 한다. 아래 그림은 함수 \\(f\\) 에 대한 전방 차분, 후방 차분, 중앙 차분값을 그림으로 표현하였다.\n\n\n\n\n\n\n그림 1: Finite difference\n\n\n\n\n\n\n1.4 Roundoff 에러\n\\(h\\) 값이 작을 수록 오차가 작아지지만 \\(h\\) 값이 어느 이하로 작아지면 에러가 발생한다. 아래는 쉬운 예를 보여준다. 우리는 \\(f(x)=e^x\\) 의 도함수는 자기 자신과 같으며, \\(f(0)=f'(0)=1\\) 임을 알고 있다. 앞의 전방, 후반 차분법과 보통의 차분법에 대해 \\(h\\) 값을 변화시키며 \\(f'(0)\\) 의 값을 계산하였다. \\(h\\) 값이 충분히 작지 않을 때는 오차가 발생하였으나 어느 정도 작아지면 그 값이 정확하고 안정적이다. 그러나 \\(h\\) 값이 \\(10^{-13}\\) 보다 작아지면 roundoff 에러가 발생하여 불안정해지는 것을 알 수 있다.\n\n\n\n\n\n\n그림 2: Roundioff-Finite difference\n\n\n\n\n\n\n1.5 다섯개의 점을 이용한 미분\n지금까지 1차와 2차 테일러 전개를 이용하여 미분계수를 \\(O(h)\\) 나 \\(O(h^2)\\) 의 오차로 구하는 방법을 소개하였다. 고차 테일러 전개를 이용하면 더 정확하게 미분값을 구할 수 있다. 4차 테일러 다항식과 나머지 값은 다음과 같다.\n\\[\n\\begin{aligned}\nf(x-2h) &= f(x) - f'(x) \\cdot (2h) + \\dfrac{f''(x)}{2}(2h)^2  \\\\\n&\\qquad \\qquad - \\dfrac{f'''(x)}{6} (2h)^3 + \\dfrac{f^{(4)}(x)}{24}(2h)^4 - \\dfrac{f^{(5)}(\\xi_1)}{120} (2h)^5\\,\\\\\nf(x-h) &= f(x) - f'(x) \\cdot (h) + \\dfrac{f''(x)}{2}(h)^2  \\\\\n&\\qquad \\qquad - \\dfrac{f'''(x)}{6} (h)^3 + \\dfrac{f^{(4)}(x)}{24}(h)^4 - \\dfrac{f^{(5)}(\\xi_2)}{120} (h)^5\\,\\\\\nf(x+h) &= f(x) + f'(x) \\cdot (h) + \\dfrac{f''(x)}{2}(h)^2 \\\\\n&\\qquad \\qquad + \\dfrac{f'''(x)}{6} (h)^3 + \\dfrac{f^{(4)}(x)}{24}(h)^4 + \\dfrac{f^{(5)}(\\xi_3)}{120} (h)^5\\,\\\\\nf(x+2h) &= f(x) + f'(x) \\cdot (2h) + \\dfrac{f''(x)}{2}(2h)^2 \\\\\n&\\qquad \\qquad + \\dfrac{f'''(x)}{6} (2h)^3 + \\dfrac{f^{(4)}(x)}{24}(2h)^4 + \\dfrac{f^{(5)}(\\xi_4)}{120} (2h)^5\\,\\\\\n\\end{aligned}\n\\tag{7}\\]\n이로부터,\n\\[\n\\begin{aligned}\nf(x-2h) -8f(x-h) + 8f(x+h)  - f(x+2h) &= 12 f'(x) h+ O (h^5) \\,,\\\\\n-f(x-2h) + 16f(x-h) -30 f(x) + 16 f(x+h) - f(x+2h) &= 12 f''(x) h^2 + O(h^5) .\n\\end{aligned}\n\\]\n임을 안다. 즉,\n\\[\n\\begin{aligned}\nf'(x) & = \\dfrac{f(x-2h) -8f(x-h) + 8f(x+h)  - f(x+2h)}{12h} + O(h^5)\\,,\\\\\nf''(x) &= \\dfrac{-f(x-2h) + 16f(x-h) -30 f(x) + 16 f(x+h) - f(x+2h)}{12h^2} + O(h^4)\n\\end{aligned}\n\\tag{8}\\]\n이다.\n다음 그림은 함수 \\(f(x) = x\\sin (x)\\) 에 대해 \\(h\\) 를 변화시켜 가면서로 잡고 위에서 설명한 두가지 방법을 이용하여 도함수를 구했을때의 그래프(“3 Points”, “5 Points”) 와 \\(f'(x) = \\sin (x) + x\\cos (x)\\) 의 그래프 (“Analytical”) 를 같이 그린 것이다. \\(h=1\\times 10^{-1}\\) 에서는 첫번째 수치미분보다 두번째 수치미분이 좀 더 실제 값에 가까우며 \\(1 \\times 10^{-2}\\) 부터 \\(1.0 \\times 10^{-14}\\) 까지는 두 방법 모두 상당히 정확한 값을 보여준다. 그러나 \\(h\\) 값이 \\(1.0\\times 10^{-15}\\) 보다 작아지면서부터는 뚜렷하게 오차를 보여준다. 이것은 수치 미분을 구할 때 분모를 \\(h\\) 로 나누게 되는데, 이 값이 작을수록 round-off 에러가 발생하기 때문이다. 따라서 적당한 \\(h\\) 값을 잡는 것이 중요하다.\n\n\n\n\nDerivative",
    "crumbs": [
      "수치해석 I",
      "일변수 함수의 미분과 적분"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#sec-Autodiff",
    "href": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#sec-Autodiff",
    "title": "일변수 함수의 미분과 적분",
    "section": "2 자동 미분",
    "text": "2 자동 미분\n\n참고자료\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2018. “Automatic Differentiation in Machine Learning: A Survey.” Journal of Machine Learning Research 18 (153): 1–43. http://jmlr.org/papers/v18/17-468.html.\n\n\nMargossian, Charles C. 2019. “A Review of Automatic Differentiation and Its Efficient Implementation.” WIREs Data Mining and Knowledge Discovery 9 (4). https://doi.org/10.1002/widm.1305.\n\n\nVerma, Arun. 2000. “An Introduction to Automatic Differentiation.” Current Science 78 (7): 804–7. http://www.jstor.org/stable/24103956.\n\n\n\n\n2.1 편미분과 합성함수의 편미분\n\\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) 에 대해 \\(f(\\boldsymbol{x})\\) 의 \\(\\boldsymbol{a}\\in \\mathbb{R}^n\\) 에서의 \\(i\\) 번째 변수에 대한 편미분 \\(\\dfrac{\\partial f(\\boldsymbol{a})}{\\partial x_i}\\) 은 다음과 같이 정의된다.\n\\[\n\\dfrac{\\partial f(\\boldsymbol{a})}{\\partial x_i} := \\lim_{h \\to 0}\\dfrac{f(\\boldsymbol{a}+h\\boldsymbol{e}_i) - f(\\boldsymbol{a})}{h}\n\\]\n이것은 \\(\\boldsymbol{x}\\) 에 대해 \\(x_i\\) 를 제외한 \\(x_j\\) 에 대해 \\(x_j=a_j (j\\ne i)\\) 로 고정시킨 함수를 \\(\\overline{f}\\) 라 고 했을 때의 \\(\\dfrac{d \\overline{f}(a_j)}{dx}\\) 와 같다. 즉 편미분을 구하는 것은 본질적으로 일변수 함수의 미분을 구하는 것과 다르지 않다. 또 하나 중요한 값은 헤시안(Hessian) 이라 불리는 함수로 \\(f\\) 에 대한 \\(\\boldsymbol{a}\\) 에서의 헤시안 \\(\\boldsymbol{H}_f(\\boldsymbol{a})\\) 은 다음과 같이 정의된다.\n\\[\n\\boldsymbol{H}_F(\\boldsymbol{a}) = \\begin{bmatrix} \\dfrac{\\partial^2 f(\\boldsymbol{a})}{\\partial x_1\\partial x_1} & \\cdots & \\dfrac{\\partial^2 f(\\boldsymbol{a})}{\\partial x_1 \\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\dfrac{\\partial^2 f(\\boldsymbol{a})}{\\partial x_n \\partial x_1} & \\cdots & \\dfrac{\\partial^2 f(\\boldsymbol{a})}{\\partial x_n \\partial x_n} \\end{bmatrix}  \n\\tag{9}\\]\n\\(f(\\boldsymbol{x})\\) 가 \\(C^2\\) 급 함수라면, 즉 2차 편미분이 모든 독립변수에 대해 존재하며 모두 연속이라면 헤시안은 대칭행렬이다.\n이제 다변수 함수의 경우를 보자. \\(F:\\mathbb{R}^n \\to \\mathbb{R}^m\\) 함수 \\(F(\\boldsymbol{x}) = \\begin{bmatrix} f_1(\\boldsymbol{x}) & \\cdots & f_m(\\boldsymbol{x})\\end{bmatrix}^T\\) 에 대해\n\\[\n\\dfrac{\\partial F(\\boldsymbol{a})}{\\partial x_i} = \\lim_{h \\to 0} \\dfrac{F(\\boldsymbol{a} + h \\boldsymbol{e}_i)- F(\\boldsymbol{x})}{h} = \\begin{bmatrix} \\dfrac{\\partial f_1(\\boldsymbol{a})}{\\partial x_i} \\\\ \\vdots \\\\ \\dfrac{\\partial f_m(\\boldsymbol{a})}{\\partial x_i}\\end{bmatrix}\n\\tag{10}\\]\n이며 야코비 행렬 \\(\\boldsymbol{J}_F(\\boldsymbol{a})\\) 는\n\\[\n\\boldsymbol{J}_F(\\boldsymbol{a}) = \\begin{bmatrix} \\dfrac{\\partial f_1(\\boldsymbol{a})}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1(\\boldsymbol{a})}{\\partial x_1} \\\\ \\vdots & \\ddots &\\vdots \\\\ \\dfrac{\\partial f_m(\\boldsymbol{a})}{\\partial x_n} & \\cdots & \\dfrac{\\partial f_m(\\boldsymbol{a})}{\\partial x_n}\\end{bmatrix}\n\\tag{11}\\]\n이다.\n이제 함성함수를 생각해보자. \\(F=F^{(N)}(F^{(N-1)} \\cdots F^{(2)}(F^{(1)}(\\boldsymbol{x}))\\cdots )\\) 이며 \\(F^{(k)} = \\begin{bmatrix} f^{(k)}_1 & \\cdots f^{(k)}_{m_k}\\end{bmatrix}^T\\) 라고 하자. \\(\\boldsymbol{z}^{(1)} = F^{(1)}(\\boldsymbol{a})\\), \\(\\boldsymbol{z}^{(2)} = F^{(2)}(F^{(1)}(\\boldsymbol{a})),\\ldots\\), \\(\\boldsymbol{y}=F(\\boldsymbol{a})\\) 라고 하면\n\\[\n\\begin{aligned}\n\\dfrac{\\partial y_j}{\\partial x_i} = \\dfrac{\\partial f_j(\\boldsymbol{x})}{\\partial x_i} &= \\sum_{k_{N-1}=1}^{m_{N-1}}\\sum_{k_{N-2}=1}^{m_{N-2}} \\cdots \\sum_{k_1=1}^{m_1}\\dfrac{\\partial f_j^{(N)}(\\boldsymbol{z}^{(N-1)})}{\\partial z^{(N-1)}_{k_{N-1}}} \\dfrac{\\partial f^{(N-1)}_{k_{N-1}}(\\boldsymbol{z}^{(N-2)})}{\\partial z^{(N-2)}_{k_{N-2}}} \\cdots \\dfrac{\\partial f^{(1)}_{k_{1}}(\\boldsymbol{x})}{\\partial x_i} \\\\\n&=\\sum_{k_{N-1}=1}^{m_{N-1}}\\sum_{k_{N-2}=1}^{m_{N-2}} \\cdots \\sum_{k_1=1}^{m_1}\\dfrac{\\partial z_j^{(N)}(\\boldsymbol{z}^{(N-1)})}{\\partial z^{(N-1)}_{k_{N-1}}} \\dfrac{\\partial a^{(N-1)}_{k_{N-1}}(\\boldsymbol{z}^{(N-2)})}{\\partial z^{(N-2)}_{k_{N-2}}} \\cdots \\dfrac{\\partial z^{(1)}_{k_{1}}(\\boldsymbol{x})}{\\partial x_i}\n\\end{aligned}\n\\tag{12}\\]\n이며, 이로부터\n\\[\n\\boldsymbol{J}_F(\\boldsymbol{a}) = \\boldsymbol{J}_{F^{(N)}}(\\boldsymbol{z}^{(N-1)}) \\boldsymbol{J}_{F^{(N-1)}}(\\boldsymbol{z}^{(N-2)}) \\cdots \\boldsymbol{J}_{F^{(1)}}(\\boldsymbol{a})\n\\tag{13}\\]\n임을 안다. 이것이 소위 chain rule 이다.\n\n\n\n2.2 컴퓨터를 이용한 미분 계산\n특히 현재의 인공지능의 발전에 있어 위에 언급한 대량의 함수와 변수를 사용하는 미분과 편미분을 정확하고 빠른게 계산하는 것이 중요해 졌다. 전통적으로 함수의 미분을 컴퓨터를 이용하여 계산하는 방법에는 세가지가 있었다.\n\n[수동] 미분/편미분 함수를 손으로 계산하고, 그 결과를 코딩한다.\n[차분] 앞서 배운 유한 차분법을 이용하여 수치해석적으로 계산한다.\n[기호] 메쓰메티카(Mathematica) 프로그램과 같이 기호를 이용하여 미분한다.\n\n이 방법은 모두 치명적인 단점이 존재한다. ‘수동’ 방법은 일반적인 함수에 대해 사용 할 수 없고, 인적 오류가 나기 쉽다. ‘차분’ 법은 항상 truncation error 와 round-off error 에 노출된다. 게다가 변수가 많고 여러 단계로 합성된 함수의 경우(인공지능의 신경망이 정확히 이런 경우이다) 그 오차가 계속 누적되기 때문에 오차가 필연적으로 커질 수 밖에 없다. ‘기호’ 법은 느리며 함수가 복잡할 때 만족할 만한 깔끔한 결과를 낼 수 없고 컴퓨팅 파워를 많이 사용한다.\n이를 극복할 수 있는 방법이 자동미분이다. 어떤 함수는 그 도함수가 간단하다. 예를 들어 \\(x^n\\) 이나 삼각함수, 지수-로그 함수 같은 경우이다. 그리고 함수간의 사칙연산과 합성에 대해 그 도함수는 간단한 규칙, 즉 chain rule 로 계산된다. 자동 미분에서는 함수값과 미분값을 같이 계산하여 전달한다. 예를 들어 julia 에서\nsin(π)\n는 0.0 을 반환하지만 자동미분에서 사용하는 함수는 함수값과 함께 미분값 cos(π) 도 같이 반환한다.\n(sin(π), cos(π))\n또한 함수간의 연산과 합성에 대해서도 그 chain rule 을 이용하여 자동으로 계산해준다.\n\n\n\n2.3 전진법과 후진법\n자동 미분의 chain rule 을 구현하는 방법에는 전진법(forwared AD)과 후진법(backward AD) 이 있다. (참고로 전진법, 후진법은 앞서 수치해석적 미분에서의 전방차분, 후방차분과는 아무 관계 없다.)\n\n전진법\n전진법은 \\(F^{(1)}(\\boldsymbol{a}),\\, \\boldsymbol{J}_{F^{(1)}}(\\boldsymbol{a})\\) 부터 차근차근 \\(i\\) 를 증가시켜 가면서 \\(F^{(i)}(\\boldsymbol{z}^{(i-1)})\\) 과 \\(\\boldsymbol{J}_{F^{(i)}}(\\boldsymbol{z}^{i-1})\\) 을 계산한다. Chain rule 로 표현해 보면 다음과 같다.\n\\[\n\\begin{aligned}\n\\dfrac{\\partial z^{(m)}_i}{\\partial x_j} &= \\sum_{k_{m-1}}\\cdots \\sum_{k_1}\\dfrac{\\partial z_{k_1}^{(1)}(\\boldsymbol{a})}{\\partial x_j} \\cdots \\dfrac{\\partial z_{k_{m-1}}^{(m-1)}(\\boldsymbol{z}^{(m-2)})}{\\partial z^{(m-2)}_{k_{m-2}}}\\dfrac{\\partial z_i^{(m)}(\\boldsymbol{z}^{(m-1)})}{\\partial z^{(m)}_{k_{m-1}}} \\\\[0.3em]\n&=\\sum_{k_{1}} \\cdots \\sum_{k_{m-1}}  \\dfrac{\\partial z_i^{(m)}(\\boldsymbol{z}^{(m-1)})}{\\partial z^{(m)}_{k_{m-1}}} \\dfrac{\\partial z_{k_{m-1}}^{(m-1)}(\\boldsymbol{z}^{(m-2)})}{\\partial z^{(m-2)}_{k_{m-2}}} \\cdots \\dfrac{\\partial z_{k_1}^{(1)}(\\boldsymbol{a})}{\\partial x_j} \\\\[0.3em]\\\\\n&= \\left[\\boldsymbol{J}_{F^{(m)}}(\\boldsymbol{z}^{(m-1)}) \\cdots \\boldsymbol{J}_{F^{(1)}}(\\boldsymbol{a})\\right]_{ij}\n\\end{aligned}\n\\tag{14}\\]\n\n\n\n후진법\n아래의 식을 사용한다.\n\\[\n\\begin{aligned}\n\\dfrac{\\partial y_j}{\\partial z^{(m)}_i} &= \\sum_{k_{m+1}}\\cdots \\sum_{k_{N-1}} \\dfrac{\\partial y_j(\\boldsymbol{z}^{(N-1)})}{\\partial z^{(N-1)}_{k_{N-1}}} \\dfrac{\\partial z^{(N-1)}_{k_{N-1}}(\\boldsymbol{z}^{(N-2)})}{\\partial z^{(N-2)}_{k_{N-2}}} \\cdots \\dfrac{\\partial z^{(m+1)}_{k_{m+1}}(\\boldsymbol{z}^{m})}{\\partial z^{(m)}_i} \\\\[0.3em]\n&=\\sum_{k_{N-1}} \\cdots\\sum_{k_{m+1}} \\dfrac{\\partial z^{(m+1)}_{k_{m+1}}(\\boldsymbol{z}^{m})}{\\partial z^{(m)}_i} \\cdots  \\dfrac{\\partial z^{(N-1)}_{k_{N-1}}(\\boldsymbol{z}^{(N-2)})}{\\partial z^{(N-2)}_{k_{N-2}}}\\dfrac{\\partial y_j(\\boldsymbol{z}^{(N-1)})}{\\partial z^{(N-1)}_{k_{N-1}}} \\\\[0.3em]\n&= \\left[\\boldsymbol{J}_{F^{(m+1)}}(\\boldsymbol{z}^m)^T \\cdots \\boldsymbol{J}_{F^{(N-1)}}(\\boldsymbol{z}^{(N)})^T \\right]_{ij}\n\\end{aligned}\n\\tag{15}\\]\n\\(m=(N-1)\\), 즉 \\(\\dfrac{\\partial y_j(\\boldsymbol{z}^{(N-1)})}{\\partial z^{(N-1)}_{k_N}}\\) 부터 계산한다. \\(m=N-2,\\,N-3,\\ldots,\\,1\\) 까지 감소시키면 \\(\\boldsymbol{J}_F\\) 를 계산 할 수 있다… 고 쉽게 넘어 갈 수 있지만 우리는 아직 \\(\\boldsymbol{z}^{(m)}= F^{(m)}\\circ \\cdots \\circ F^{(1)}(\\boldsymbol{a})\\) 를 계산하지 않았다는 것을 기억하라. 후진법은 전진법과는 달리 두가지 과정으로 수행된다.\n (\\(1\\)) 일단 \\(\\boldsymbol{z}^{(1)}=F^{(1)}(\\boldsymbol{a})\\) 부터 \\(\\boldsymbol{y} = F(\\boldsymbol{a})= F^{N}\\circ \\cdots F^{(1)}(\\boldsymbol{a})\\) 까지 계산하며 이 값을 기억한다.\n (\\(2\\)) (\\(1\\)) 에서 기억한 값을 이용하여 \\(m=N-1\\) 부터 \\(m=1\\) 까지의 편미분을 계산한다.\n즉 후진법은 전진하면서 함수값을 계산하고 후진하면서 앞서 계산한 함수값을 이용하여 편미분을 계산한다. 일반적으로 \\(F:\\mathbb{R}^n \\to \\mathbb{R}^m\\) 에 대해 \\(m&gt;n\\) 이면 전진법이 유리하며 \\(n&gt;m\\) 이면 후진법이 유리하다는 것이 잘 알려져 있으며 \\(n\\) 이 \\(m\\) 보다 더 클수록 더 유리하다는 것이 알려져 있다. 신경망과 같은 기계학습이 전형적으로 이런 경우이므로 기계학습에서는 거의 후진법이 사용된다.\n\nJulia 에서의 미분 관련 패키지에 대해서는 JuliaDiff 를 참고하라. 앞으로는 자동 미분을 사용 할 수 있을경우, 전진법에는 ForwardDiff.jl 을, 후진법에는 ReverseDiff.jl 을 사용하도록 한다.",
    "crumbs": [
      "수치해석 I",
      "일변수 함수의 미분과 적분"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#일변수-함수의-적분-뉴턴-코츠-방법",
    "href": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#일변수-함수의-적분-뉴턴-코츠-방법",
    "title": "일변수 함수의 미분과 적분",
    "section": "3 일변수 함수의 적분 : 뉴턴-코츠 방법",
    "text": "3 일변수 함수의 적분 : 뉴턴-코츠 방법\n수치해석적으로 적분을 구하는 방법을 알아보도록 한다. 다항식이나 삼각함수, 지수함수를 포함한 몇면 함수에 대해서는 우리가 이미 프로그래밍 언어에 내장되어 있는 기본적인 함수로 존재하므로 쉽게 구할 수 있다. 예를 들어 \\([0, \\theta]\\) 영역에서의 \\(\\sin x\\) 함수의 적분은\n\\[\n\\int_0^\\theta \\sin x \\, dx = -\\cos \\theta  +1\n\\]\n이라는 것을 안다. 그러나 대부분의 함수는 수치해석적으로 구할 수 밖에 없다. 수학적으로는 우리는 어떤 구간에서 연속인 함수라면 그 구간에서 적분이 존재한다는 것을 알고 있다. 또한 \\([a,\\,b]\\) 구간에서 불연속인 점이 유한개이면 적분이 존재한다는 것도 알고 있다1. 만약 적분구간이 부분적으로 연속인 유한개의 구간으로 이루어져 있다면, 각각의 유한개의 구간에 대한 적분의 합으로 생각 할 수 있으므로, 여기서는 연속인 경우만 다루기로 한다. 미분과 마찬가지로 \\([a,\\,b]\\) 구간이 \\(x_1=a&lt;x_2&lt;\\cdots &lt;x_{n-1}&lt;x_n=b\\) 인 \\(n\\) 개의 점을 이용하여 \\(n-1\\) 개의 부분구간(subintervals)로 분할되었다고 가정한다. 이렇게 적분을 유한개의 구간으로 나누어 구간의 폭과 구간마다 정해지는 값의 합으로 계산하는 방법을 구적법(quadrature) 라고 한다.\n\n\n3.1 뉴턴-코츠 방법\n함수 \\(f:[a,\\,b]\\to\\mathbb{R}\\) 과 \\([a,\\,b]\\) 구간을 같은 간경의 \\(n\\) 개의 점으로 나눈 \\(x_1,\\ldots,\\,x_n\\) 을 생각하자.\n\\[\na=x_1 &lt;x_2&lt; \\cdots &lt; x_{n-1}&lt;x_n=b,\\qquad x_{k}-x_{k-1}= h=\\dfrac{b-a}{n-1}=\\text{constant}.\n\\]\n여기에 대해 구적법으로 적분을 구하는 것을 뉴턴-코츠 방법(Newton-Cotes method) 이라고 한다. 구간에서 뉴턴-방법으로 적분을 수행할 때 양 끝점을 포함하여 구간을 나눌 수도 있고, 양 끝점을 빼고 구간을 나눌 수도 있다. 양 끝점을 포함하는 것을 닫힌 Newton-Cotes 방법(closed Newton-Cotes method) 이라고 하고, 양 끝점을 빼는 것을 열린 Newton-Cotes 방법(open Newton-Cotes method) 이라고 한다.\n뉴턴-코츠 방법은 적분값을 \\(f(x_1), \\ldots,\\,f(x_n)\\) 의 선형결합으로 다음과 같이 표현한다.\n\\[\n\\begin{aligned}\n\\int_{a}^b f(x)\\, dx &\\approx \\sum_{k=1}^n w_i f(x_i)\\qquad & & \\text{closed Newton-Cotes method}\\\\\n&\\approx \\sum_{i=2}^{n-1} w_i f(x_i) \\qquad & & \\text{open Newton-Cotes method}\n\\end{aligned}\n\\]\n즉 우리가 여기서 구해야 할 것은 각 구간에서의 weight factor \\(w_i\\) 이다. 뉴턴-코츠 방법을 이용한 대표적인 적분 방법이 사다리꼴 방법과 Simpson 방법이다.\n\n\n\n\nTrapzoidal method 와 Simpson method\n\n\n\n중점법은 구간 \\([x_k,\\, x_{k+1}]\\) 의 중점 \\(\\dfrac{1}{2}(x_k + x_{k+1})\\) 에서의 함수값을 이용하여 적분값을 계산하는 방법이다. 사다리꼴 방법은 \\([x_k,\\,x_{k+1}]\\) 의 적분을 \\((x_k,\\, f(x_k))\\) 와 \\((x_{k+1},\\, f(x_{k+1}))\\) 을 1차 함수로 근사하여 적분값을 계산한다. Simpson 방법은 두가지가 있는데 각각은 이어지는 세 점이나 네 점을 각각 2차함수와 3차함수로 근사하여 적분하는 방법이다.\n\n\n\n3.2 중점을 이용한 적분\n\\([x_{k},\\, x_{k+1}]\\) 구간에 대한 적분을 구할 때 구간의 양 끝점에 대한 중점 \\(z_k = \\dfrac{x_{k+1}+x_{k}}{2}\\) 에서의 함수값을 사용하는 방법이다. 즉,\n\\[\n\\int_{x_{k}}^{x_{k+1}} f(x)\\,dx\\approx f\\left(\\dfrac{x_{k+1}+x_i}{2}\\right) h = f(z_k) h\n\\]\n로 근사하는 방법이다. 테일러 정리에 의해\n\\[\nf(x) = f\\left(z_k\\right)+f'\\left(z_k\\right)\\left(x-z_k\\right) + \\dfrac{1}{2}f^{(2)}(\\xi_k) \\left(x-z_k\\right)^2\n\\]\n을 만족하는 \\(\\xi_k\\in [x_{k},\\, x_{k+1}]\\) 이 존재한다는 것을 안다. 위 식을 적분하면\n\\[\n\\int_{x_k}^{x_{k+1}} f(x)\\, dx = f(z_k)h + \\dfrac{1}{24}f^{(2)}(\\xi_k)h^3\n\\]\n이며, 따라서 \\([a,\\,b]\\) 구간에서의 적분은\n\\[\n\\int_a^b f(x) \\,dx= \\sum_{k=1}^{n-1} f\\left(\\dfrac{x_{k}+x_{k+1}}{2}\\right) h + \\dfrac{h^3}{24} \\sum_{k=1}^{n-1}f^{(2)}(\\xi_k)\n\\]\n이다. \\(f\\in C^2_{[a,\\,b]}\\) 라면 따름정리 : 여러 점의 경우 에 의해 \\(\\displaystyle \\sum_{k=1}^{n-1}f^{(2)}(\\xi_k) = (n-1)f^{(2)}(\\xi)\\) 를 만족하는 \\(\\xi\\in [a,\\,b]\\) 가 존재하므로,\n\\[\n\\int_a^b f(x) \\, dx =  \\sum_{k=1}^{n-1} f\\left(\\dfrac{x_{k}+x_{k+1}}{2}\\right) h + \\dfrac{(n-1)h^3}{24} f^{(2)}(\\xi)\n\\]\n를 만족한다. 중간값에 의한 적분 계산값을 \\(I_M[a, b, n]\\) 라고 하면\n\\[\nI_M [f,\\,a,\\,b,\\,n] = \\sum_{k=1}^{n-1}f\\left(\\dfrac{x_{k}+x_{k+1}}{2}\\right) h\n\\]\n이라고 할 수 있으며, \\(M=\\max \\{ |f^{(2)}(x)| : x\\in [a,\\,b] \\}\\) 일 때 중점을 이용한 적분의 오차는 \\(E_M [f,\\,a,\\,b,\\,n]=\\dfrac{(n-1)h^3}{24} M=\\dfrac{(b-a)M}{24}h^2\\) 보다 작거나 같다. 즉 \\(O(E_M) =h^2\\) 이다.\n\n\n\n3.3 사다리꼴 방법 를 이용한 적분\n함수 \\(f(x)\\) 를 \\([x_k,\\, x_{k+1}]\\) 구간에서 \\((x_k,\\, y_k)\\) 와 \\((x_{k+1},\\, y_{k+1})\\) 을 지나는 1차 다항식 \\(\\tilde{f}_k(x) = \\dfrac{y_{k+1}-y_k}{x_{k+1}-x_k}(x-x_k)+y_k\\) 으로 근사하여 적분하는 것을 사다리꼴 방법 (trapzoidal method) 이라고 한다. 이 근사에서는 다항식을 이용한 보간법의 오차 의 정리 1 에서 보았듯이 다음을 만족하는 \\(\\xi_k\\in [x_{k-1},\\, x_k]\\) 가 존재한다. \\[\n\\begin{aligned}\nf(x) = \\tilde{f}(x) + \\dfrac{f^{(2)}(\\xi_k)}{2}(x-x_{k-1})(x-x_k).\\\\\n\\end{aligned}\n\\]\n이것을 \\([x_{k-1},\\, x_k]\\) 구간에서 적분하면 다음을 얻는다.\n\\[\n\\begin{aligned}\n\\int_{x_{k-1}}^{x_k} f(x)\\, dx = \\dfrac{h}{2}(f(x_{k-1})+f(x_k)) -\\dfrac{f^{(2)}(\\xi_k)}{12} h^3\n\\end{aligned}\n\\]\n전체 구간 \\([a,\\,b]\\) 에 대해 적분하면\n\\[\n\\int_{a}^b f(x)\\, dx= \\dfrac{h}{2}(f(x_1)+ f(x_n)) + \\sum_{k=2}^{n-1} hf(x_k) - \\dfrac{h^3}{12}\\sum_{k=1}^{n-1} f^{(2)}(\\xi_k)\n\\]\n를 얻는다. 사다리꼴 방법에 의한 적분값 \\(I_T\\) 는\n\\[\nI_T[f, a, b, n] = \\dfrac{h}{2}\\left(f(x_1)+ f(x_n) + \\sum_{k=2}^{n-1} f(x_k)\\right)\n\\tag{16}\\]\n이다. 이제 적분의 오차를 알아보자. \\(f^{(2)}(x)\\) 가 연속이므로 \\(\\displaystyle \\dfrac{1}{n-1}\\sum_{k=1}^{n-1} f^{(2)}(\\xi_k)= f^{(2)}(\\xi)\\) 를 만족하는 \\(\\xi\\in [a,\\,b]\\) 가 존재하며2 따라서 사다리꼴 방법에 의한 적분에 대한 에러 \\(E_T\\) 는\n\\[\nE_T[f, a, b, n] = \\dfrac{h^3}{12}(n-1)f^{(2)}(\\xi) = \\dfrac{(b-a)f^{(2)}(\\xi)}{12}h^2 = O(h^2)\n\\tag{17}\\]\n이다.\n\n\n\n3.4 Simpson 1/3 적분\n\\([x_{k-1},\\,x_{k+1}]\\) 구간에서의 적분을 생각하자. 테일러 정리에 의해\n\\[\nf(x) = f(x_k) + f'(x_k)(x-x_k) + \\dfrac{f''(x_k)}{2}(x-x_k)^2 + \\dfrac{f^{(3)}(x_k)}{6}(x-x_k)^3 + \\dfrac{f^{(4)}(\\xi_k)}{24}(x-x_k)^4\n\\]\n를 만족하는 \\(\\xi_k \\in [x_{k-1},\\, x_{k+1}]\\) 이 존재한다. 양변을 적분하면\n\\[\n\\begin{aligned}\n\\int_{x_{k-1}}^{x_{k+1}}f(x)\\,dx &= f(x_k)(2h) + \\dfrac{f''(x_k)}{3}h^3 + \\dfrac{f^{(4)}(\\xi_k)}{60}h^5\n\\end{aligned}\n\\]\n이다. 또한 식 6 로부터\n\\[\nf''(x) =  \\dfrac{f(x+h) - 2f(x) + f(x-h)}{h^2} - \\dfrac{f^{(4)}(\\xi_k')}{36}h^5\n\\]\n를 만족하는 \\(\\xi_k'\\in [x_{k-1},\\,x_{k+1}]\\) 가 존재한다는 것을 알고 있다. 따라서\n\\[\n\\begin{aligned}\n\\int_{x_{k-1}}^{x_{k+1}}f(x)\\,dx &= f(x_k)(2h) + \\dfrac{h}{3}\\left[ f(x_{k+1}) -2f(x_k) + f(x_{x-k}) \\right]  + \\left[\\dfrac{f^{(4)}(\\xi_k)}{60} - \\dfrac{f^{(4)}(\\xi_k')}{36} \\right] h^5 \\\\\n&=\\dfrac{1}{3} \\left[f(x_{k-1}) + 4f(x_{k}) + f(x_{k+1})\\right] -  \\left( \\dfrac{f^{(4)}(\\xi_k')}{36} - \\dfrac{f^{(4)}(\\xi_k)}{60} \\right)h^5\n\\end{aligned}\n\\]\n이다. 이로부터 우리는 Simpson 1/3 적분에서의 오차가 \\(O(h^5)\\) 임을 알 수 있다. 여기서는 \\(\\xi_k\\) 와 \\(\\xi_k'\\) 에서의 4계 도함수값이 필요했지만 실제로는\n\\[\n\\begin{aligned}\n\\int_{x_{k-1}}^{x_{k+1}}f(x)\\,dx =\\dfrac{h}{3} \\left[f(x_{k-1}) + 4f(x_{k}) + f(x_{k+1})\\right] -  \\dfrac{f^{(4)}(\\overline{\\xi}_k)}{90}h^5\n\\end{aligned}\n\\]\n를 만족하는 \\(\\overline{\\xi}_k \\in [x_{k-1},\\,x_{k+1}]\\) 가 존재한다(Atkinson, Kendall E. (1989). An Introduction to Numerical Analysis (2nd ed.). John Wiley & Sons. 을 참고하라)\n이제 \\([a,\\,b]\\) 구간을 \\(a=x_1&lt;x_2&lt;\\cdots&lt;x_{2m}&lt;x_{2m+1}=b\\) 의 \\(2m\\) 개의 구간으로 나누어 적분하여 합치는 것을 생각하자. Simpson 1/3 적분은 2개의 구간을 한꺼번에 적분하므로 \\(2m+1\\) 개의 위치가 필요하다. 에러도 \\(2m+1\\) points 에 대한 중간값 정리를 사용하면 다음을 만족하는 \\(\\xi \\in [x_1,\\,x_{2m+1}]\\) 이 존재한다.\n\\[\n\\begin{aligned}\n\\int_{a}^b f(x)\\, dx &= \\dfrac{h}{3} \\left[f(x_1) + 4f(x_2) + 2f(x_3) + 4f(x_4) + 2f(x_5)+\\cdots \\right.\\\\\n&\\qquad \\cdots \\left.+ 4f(x_{2m-2})+2f(x_{2m-1}) + 4f(x_{2m})+f(x_{2m+1})\\right] - m\\dfrac{f^{(4)}(\\xi)}{90}h^5\n\\end{aligned}\n\\]\n따라서 Simpson 1/3 적분값은 다음과 같다. \\[\n\\mathcal{I}_{1/3}[f,\\,a,\\,b,\\, 2n+1] = \\dfrac{h}{3} \\left[f(x_1) + f(x_{2m+1}) + 4\\left(\\sum_{k=1}^m  f(x_{2k})\\right) + 2\\left(\\sum_{k=1}^{m-1} f(x_{2k+1})\\right)\\right]\n\\tag{18}\\]\n\n\n\n3.5 Simpson 3/8 적분\nSimpson 1/3 적분이 전체 \\(n-1\\) 개의 구간을 2개씩 묶어서 적분하여 합쳤다면 3/8 적분은 3개씩 묶어서 합친다. 라그랑쥬 다항식을 이용하면,\n\\[\nL_4(x) = f(x_{i-1})l_{i-1}(x) + f(x_i)l_i(x) + f(x_{i+1})l_{i+1}(x) + f(x_{i+2})l_{i+2}(x)\n\\]\n에 대해\n\\[\nf(x) = L_4 (x) + \\dfrac{f^{(4)}(\\xi_i)}{4!}\\prod_{j=1}^4(x-x_{j-2})\n\\]\n를 만족하는 \\(\\xi_i\\in [x_{i-1},\\, x_{i+2}]\\) 가 존재한다.\n\\[\n\\begin{aligned}\n\\int_{x_{i-1}}^{x_{i+2}} f(x)\\, dx = \\dfrac{3h}{8} \\left( f(x_{i-1}) + 3 f(x_{i}) + 3f(x_{i+1}) + f(x_{i+2})\\right) - \\dfrac{3}{80}f^{(4)}(\\xi) h^5\n\\end{aligned}\n\\]\n를 얻는다. 이것을 \\([a,\\,b]\\) 구간을 \\(a=x_1&lt;\\cdots &lt;x_{3m+1}=b\\) 이며 \\(x_{i+1}-x_i = h=\\text{const.}\\) 라면\n\\[\n\\begin{aligned}\n\\int_a^b f(x)\\, dx &= \\dfrac{3h}{8} \\left[f(x_1) + f(x_{3m+1}) + 3\\sum_{i=0}^{m-1} \\left(f(x_{3i+2}) + f(x_{3i+3})\\right)  + 2\\sum_{i=1}^{m-1} f(x_{3i+1})\\right] - \\dfrac{3m}{80}f^{(4)} (\\xi) h^5\n\\end{aligned}\n\\]\n를 만족하는 \\(\\xi \\in [a,\\,b]\\) 가 존재한다. 여기서\n\\[\n\\mathcal{I}_{3/8}[f,\\,a,\\,b,\\, 3m+1]=\\dfrac{3h}{8} \\left[f(x_1) + f(x_{3m+1}) + 3\\sum_{i=0}^{m-1} \\left(f(x_{3i+2}) +f(x_{3i+3})\\right) +  + 2\\sum_{i=1}^{m-1} f(x_{3i+1})\\right]\n\\tag{19}\\]\n는 Simpson 3/8 적분값이며 그 오차는 \\(- \\dfrac{3m}{80}f^{(4)} (\\xi) h^5\\) 이다.\n\n\n아래 그림은 \\(\\displaystyle \\int_0^1 \\exp(x)\\, dx\\) 를 세가지 방법으로, 구간을 변화시켜 가며 구하여 실제값인 \\((e-1)\\) 로 나눈 비율을 표현하였다. \\(N_{points}\\) 가 작을 때는 계산값간의 다소간의 차이가 있으며 실제 값과도 차이가 있지만 \\(N_{points}\\) 값이 커질수록 세 방법 모두 실제 값에 수렴해 나간다는 것을 볼 수 있다. 또 하나 유의할 것은 Simpson 3/8 방법이 Simpson 1/3 방법보다 다소 복잡함에도 불구하고 일반적으로 실제 값과의 차이는 Simpson 1/3 방법이 더 작다. 이런 이유로 단순이 Simpson 방법이라고 할 때는 Simpson 1/3 방법을 말한다.\n\n\n\n수치 적분\n\n\nfunction integrate_trapzoidal(\n    f::Function, \n    a::Number, \n    b::Number, \n    n::Integer)\n    \n    a, b = minmax(a, b)\n    h = (b-a)/(n-1)\n    x = range(a, b, length = n)\n    ff = f.(x)\n    result = 0.5*(ff[1]+ff[end])\n    result += sum(ff[2:end-1])\n    return result*h\nend\n\nfunction integrate_simpson_1_3(\n    f::Function, \n    a::Number, \n    b::Number, \n    n::Integer)\n    \n    @assert n %2 == 1\n    a, b = minmax(a, b)\n    h = (b-a)/(n-1)\n    x = range(a, b, length = n)\n    ff = f.(x)\n    result = (ff[1]+ff[end])\n    result += 4*sum(ff[2:2:end-1])\n    result += 2*sum(ff[3:2:end-2])\n    return result * h/3\nend\n\nfunction integrate_simpson_3_8(\n    f::Function, \n    a::Number, \n    b::Number, \n    n::Integer)\n    \n    @assert n %3 == 1\n    a, b = minmax(a, b)\n    h = (b-a)/(n-1)\n    x = range(a, b, length = n)\n    println(\"h=$h, dx=$(x[10]-x[9])\")\n    ff = f.(x)\n    result = (ff[1]+ff[end])\n    result += 3*sum(ff[2:3:end-2])\n    result += 3*sum(ff[3:3:end-1])\n    result += 2*sum(ff[4:3:end-3])\n    return result * h * 3 / 8\nend",
    "crumbs": [
      "수치해석 I",
      "일변수 함수의 미분과 적분"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#리처드슨-외삽법과-롬버그-적분",
    "href": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#리처드슨-외삽법과-롬버그-적분",
    "title": "일변수 함수의 미분과 적분",
    "section": "4 리처드슨 외삽법과 롬버그 적분",
    "text": "4 리처드슨 외삽법과 롬버그 적분\n\n4.1 리처드슨 외삽법\n오차가 작은 알고리즘을 사용하는 것은 일반적으로 오차가 큰 알고리즘을 계산하는 것보다 자원을 많이 사용한다. 리차드슨 외삽법은 오차가 큰 알고리즘에 의한 계산값을 사용하여 오차가 작은 계산값을 얻는 방법으로 많은 경우 같은 오차의 알고리즘으로 직접 계산하는 것보다 자원을 덜 소모하는 경우가 많아 다양한 분야에 널리 사용된다.\n어떤 값 \\(M\\) 을 계산하는데 매개변수 \\(h\\) 에 대해 그 truncation 오차가 \\(O(h)\\) 인 방법을 사용하여 \\(N_1(h)\\) 를 얻었다고 하자. 이 때 우리는\n\\[\nM - N_1(h) = k_1 h + k_2 h^2 + O(h^3)\n\\tag{20}\\]\n라고 생각 할 수 있다. 이것을 약간 변형하면,\n\\[\nM-N_1\\left(\\dfrac{h}{2}\\right) = k_1 \\dfrac{h}{2} + k_2 \\dfrac{h^2}{4} + O(h^3)\n\\tag{21}\\]\n이며 \\(2\\times\\)(식 21)\\(-\\)(식 20) 를 계산하면,\n\\[\n\\begin{aligned}\nM&=\\left[2N_1\\left(\\dfrac{h}{2}\\right) - N_1(h) \\right] - k_2 \\dfrac{h^2}{2} + O(h^3) = N_2(h) + O(h^2) \\\\\n&\\qquad \\qquad \\text{where} \\; N_2(h) = \\left[2N_1\\left(\\dfrac{h}{2}\\right) - N_1(h) \\right]\n\\end{aligned}\n\\]\n이다. 즉 \\(O(h)\\) 인 방법을 이용하여 truncation 오차가 \\(O(h^2)\\) 인 추정값 \\(N_2(h)\\) 를 얻었다. 이렇게 정확도가 낮은 값을 이용하여 정확도가 높은 값을 얻는 것을 리처드슨 외삽법이라고 한다. 리처드슨 외삽법은 truncation 오차가 다항식 꼴인 방법에 대해 항상 사용 할 수 있다.\n\\(M\\) 에 대해 tuncation 오차가 \\(O(h^2)\\) 인 방법을 사용하여 \\(N_2(h)\\) 를 얻었다고 하자.\n\\[\n\\begin{aligned}\nM - N_2 (h) &= k_2h^2 + k^3 h^3 + O(h^4) , \\\\\nM - N_2 \\left(\\dfrac{h}{2}\\right) &= k_2\\dfrac{h^2}{4} + k_3 \\dfrac{h^3}{8} + O(h^4)\n\\end{aligned}\n\\]\n를 이용하여,\n\\[\n\\begin{aligned}\nM &= \\dfrac{1}{3}\\left[4N_2 \\left(\\dfrac{h}{2}\\right)-N_2(h)\\right] - k_3 \\dfrac{h^3}{6} + O(h^4) = N_3(h) + O(h^3) \\\\\n&\\qquad \\qquad \\text{where} \\; N_3(h)=\\dfrac{1}{3}\\left[4N_2 \\left(\\dfrac{h}{2}\\right)-N_2(h)\\right]\n\\end{aligned}\n\\]\n를 얻는다. 즉 \\(O(h^2)\\) 인 방법을 이용하여 \\(O(h^3)\\) 인 값을 얻었다.\n이것을 일반화하여 보자. \\(O(h^m)\\) 인 방법을 사용하여 \\(M\\) 의 값을 \\(N_m(h)\\) 로 얻었다고 하자.\n\\[\n\\begin{aligned}\nM - N_m(h) &=  k_m h^m + k_{m+1}h^{m+1} + O(h^{m+2}), \\\\\nM - N_m\\left(\\dfrac{h}{2}\\right) & = k_m \\left(\\dfrac{h}{2}\\right)^m + k_{m+1}\\left(\\dfrac{h}{2}\\right)^{m+1} + O(h^{m+2})\n\\end{aligned}\n\\]\n를 이용하여,\n\\[\n\\begin{aligned}\nM &= \\dfrac{1}{2^m-1}\\left[ 2^m N_m \\left(\\dfrac{h}{2}\\right)-N_m (h)\\right] - \\dfrac{k_{m+1}}{2(2^m-1)}h^{m+1} + O(h^{m+2}) \\\\\n&= N_{m+1}(h) + O(h^{m+1})\n\\end{aligned}\n\\tag{22}\\]\n을 얻는다.\n\n\n짝수승 오차의 리차드슨 외삽법\n수치해석에서 많은 경우 truncation error 가 변수의 짝수승에만 관련이 있다. 즉,\n\\[\nM = N_1(h) + k_2h^2 + k_4 h^4 + k_6 h^6 + \\cdots\n\\]\n라고 하자. \\(O(h^{2m})\\) 오차를 가진 \\(N_{2m}(h)\\) 를 얻었다고 하자.\n\\[\n\\begin{aligned}\nM - N_{2m}(h) &= k_{2m}h^{2m} + O(h^{2m+2}), \\\\\nM - N_{2m}\\left(\\dfrac{h}{2}\\right) &= k_{2m} \\left(\\dfrac{h}{2}\\right)^{2m} + O(h^{2m+2})\n\\end{aligned}\n\\]\n로부터,\n\\[\n\\begin{aligned}\nM &= \\dfrac{1}{4^{m} -1} \\left[4^{m}N_{2m} \\left(\\dfrac{h}{2}\\right) - N_{2m}(h)\\right] + O(h^{2m+2}), \\\\\n\\end{aligned}\n\\]\n를 얻는다. 즉\n\\[\nN_{2m+2}(h) =  \\dfrac{1}{4^{m} -1} \\left[4^{m}N_{2m} \\left(\\dfrac{h}{2}\\right) - N_{2m}(h)\\right]\n\\tag{23}\\]\n를 얻는다.\n\n\n\n\n4.2 롬버그 적분\n\n\n\n\n\n\n주의\n\n\n\n교과서적인 교재에서 \\(R_{k, j}\\) 로 쓰는 표기법은 리처드슨 외삽법의 표기법을 좀 꼬아서 사용하기 때문에 개인적으로 마음에 들지 않았다. 여기서는 리처드슨 외사법의 표기법을 거의 그대로 가지고 오는 방법으로 표기하도록 하겠다. 다른 교재들과는 다르므로 혼동을 줄 수 있다.\n\n\n앞서 사다리꼴 방법을 통해 \\(O(h^2)\\) 의 오차를 갖는 함수 \\(f\\) 의 적분값을 얻었다. 이것을 짝수승 오차의 리차드슨 외삽법과 결합하면 \\(O(h^4)\\) 의 오차를 갖는 적분값을 계산 할 수 있을 것이다.\n식 16 의 사다리꼴 적분 \\(I_T[f, a, b, n]\\) 에서 \\(n\\) 을 \\(2^k\\) 로 잡았을 때의 적분을 \\(N_{k, 1}\\) 이라고 하자. 즉,\n\\[\nN_{k, 1} = I_T[f, a, b, 2^k] = \\dfrac{h}{2}\\left(f(x_1)+ f(x_{2^k}) + \\sum_{i=2}^{2^k-1} f(x_i)\\right)\n\\tag{24}\\]\n이라고 하자. 이 때의 오차는 \\(O(h^2)\\) 이다. \\(k=1,\\ldots, k_M\\) 에 대해 \\(N_{k, 1}\\) 을 구할 수 있다. \\(m\\le k_M-1\\) 에 대해 \\(N_{1, m}, \\ldots, N_{k_M-m+1, m}\\) 의 값을 구하였다고 하자. 식 23 를 이용하여, \\(O(h^{2m+2})\\) 의 오차를 갖는 \\(2^k-1\\) 개의 구간을 갖는 적분 \\(N_{k, m+1}\\) 은 \\[\nN_{k, m+1} = \\dfrac{1}{4^{m} -1} \\left[4^{m}N_{k+1, m}  - N_{k, m}\\right] + O(h^{2m+2}), \\qquad k = 1, \\ldots k_M-m+1\n\\]\n를 통해 얻을 수 있다.\nfunction rhomberg(f::Function, a::Number, b::Number, order::Integer = 4)\n    @assert order &gt; 2\n    N = zeros(order, order) \n    N[:, 1]= [integrate_trapzoidal(f, a, b, 2^k) for k in 1:order]\n    for m = 1:order, k = 1:order-m\n        N[k, m+1] = 1.0/(4^m-1.0) *(4^m * N[k+1, m]- N[k, m]) \n    end\n    return N[1, end]\nend\n함수 \\(f(x)\\) 를 \\([a,\\,b]\\) 구간에서 \\(n=2^{k_{M}}-1\\) 개의 구간, 즉 \\(a=x_1 &lt; \\cdots &lt; x_n=b\\) 의 구간에서의 사다리꼴 적분을 수행했다면 \\(h= \\dfrac{b-a}{n-1}\\) 에 대해 \\(O(h^2)\\) 의 정확도를 갖는 적분이지만 롬버그 적분을 통해 \\(O(h^{2{k_M}+2})\\) 의 정확도를 갖는 적분값을 구할 수 있다. 더구나. 실제로 계산하는 함수값의 갯수는 \\(n\\) 개이며 나머지는 이미 계산된 값들에 대한 단순한 사칙연산이므로 훨씬 빠르게 계산 할 수 있다.\n예를 들어 우리는 \\(\\displaystyle \\int_0^1 \\sqrt{x}\\, dx=\\frac{2}{3}\\) 임을 안다. 롬버그 적분을 통해 \\(k_M=4\\) 으로 계산한 결과는 \\(0.665693\\) 이며 이것보보다 오차가 작은 사다리꼴 적분을 얻기 위해서는 구간을 35개의 구간으로, 즉 \\(x_1=0,\\, x_{36}=1\\) 로 나누어야 한다.",
    "crumbs": [
      "수치해석 I",
      "일변수 함수의 미분과 적분"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#가우스-구적법",
    "href": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#가우스-구적법",
    "title": "일변수 함수의 미분과 적분",
    "section": "5 가우스 구적법",
    "text": "5 가우스 구적법\n함수 \\(f(x)\\) 를 \\([a,\\,b]\\) 구간에서 적분 할 때 \\([a,\\,b]\\) 구간에서 \\(n\\) 개의 점 \\(x_1,\\ldots,\\,x_n\\) 이 굳이 등간격이 아니더라도 선택하여\n\\[\n\\int_a^b f(x) \\, dx \\approx a_1 f(x_1)+ \\cdots + a_n f(x_n)\n\\]\n로 근사하는 것을 가우스 구적법 (Gaussian quadrature) 라고 한다. 뉴턴-코츠 방법은 점 \\(x_1,\\ldots,\\,x_n\\) 이 등간격인데 반해 가우스 구적법은 등간격일 필요가 없으며, 적분을 잘 근사하는 점을 찾아야 한다는 것이다. 이 \\(x_1,\\ldots,\\,x_n\\) 을 노드(nodes) 라고 한다.\n\n본격적으로 들어가기 전에 이미 미적분학에서 배웠을 한가지를 확인하고 가자. 우리는 임의의 구간 \\([a,\\,b]\\) 에 대한 함수 \\(f(x)\\) 의 적분을 \\([-1,\\,1]\\) 구간에 대한 적분으로 바꿀 수 있다. 즉,\n\\[\n\\int_{a}^b f(x)\\, dx = \\int_{-1}^1 f\\left(\\dfrac{(b-a)t+ (b+a)}{2}\\right)\\,\\left(\\dfrac{b-a}{2}\\right) dt\n\\tag{25}\\]\n이다. 따라서 \\([-1,\\,1]\\) 구간에서의 수치해석적 적분을 잘 정한다면 임의의 구간에서의 적분도 잘 계산 할 수 있다.\n\n\n5.1 르장드르 다항식을 이용한 가우스 구적법\n\\(n\\) 차 르장드르 다항식 \\(P_n(x)\\) 은 \\([0,\\,1]\\) 구간에서 정의된 다항식으로 다음과 같은 성질을 가진다.\n (\\(1\\)) \\(P_n(x)\\) 는 \\(n\\) 차 다항식으로 최고차항의 계수는 항상 \\(1\\) 이다.\n (\\(2\\)) \\(\\displaystyle \\int_{-1}^1 P_n (x) \\, P_m (x)\\, dx = \\dfrac{\\delta_{mn}}{2n+1}\\) 이다.\n (\\(3\\)) \\(P_0(x) = 1,\\, P_1(x) = x\\) 이며 아래의 점화식으로 고차 다항식을 구할 수 있다.\n\\[\n(n+1)P_{n+1}(x) - (2n+1)xP_{n}(x) + nP_{n-1}(x)=0\n\\]\n (\\(4\\)) \\(P_n(-x) = (-1)^nP_n(x)\\), \\(P_n(1) = 1\\).\n (\\(5\\)) \\(n\\ge 1\\) 에 대한 \\(P_n(x)\\) 는 \\([-1,\\,1]\\) 구간에서 \\(n\\) 개의 서로 다른 근을 갖는다.\n (\\(6\\)) 임의의 \\(n\\) 차 다항식은 \\(P_1(x),\\ldots,\\,P_n(x)\\) 의 선형결합으로 표현 할 수 있다.\n\n\n\n명제 1 \\(P_n(x)\\) 에 대한 \\(n\\) 개의 근을 \\(x_1,\\ldots,\\,x_n\\) 이라고 하고 \\(c_i = \\displaystyle \\int_{-1}^1 \\prod_{j=1,\\, j\\ne i}^n \\dfrac{x-x_j}{x_i-x_j}\\, dx\\) 라고 하면 임의의 다항식 \\(p(x)\\) 에 대해 다음이 성립한다.\n\\[\n\\int_{-1}^1 p(x) \\, dx = \\sum_{i=1}^n c_i p(x_i).\n\\]\n\n\n\n\n(증명). (\\(1\\)) 우선 \\(p(x)\\) 의 차수가 \\(n\\) 보다 작은 경우를 보자. \\(p(x)\\) 를 라그랑쥬 다항식에 대해 표현하면\n\\[\np(x) = \\sum_{i=1}^n p(x_i)l_i(x) = \\sum_{i=1}^n \\prod_{j=1,\\, j\\ne i}^n \\dfrac{x-x_j}{x_i-x_j} p(x_i)\n\\]\n이며,\n\\[\n\\begin{aligned}\n\\int_{-1}^1 p(x)\\, dx &=\\int_{-1}^1 \\left[\\sum_{i=1}^n \\prod_{j=1,\\, j\\ne i}^n \\dfrac{x-x_j}{x_i-x_j} p(x_i)\\right]\\, dx \\\\\n&= \\sum_{i=1}^n  \\left[\\prod_{j=1,j\\ne i}^n \\int_{-1}^1 \\dfrac{x-x_j}{x_i-x_j}\\, dx \\right] p(x_i)  = \\sum_{i=1}^n c_i p(x_i)\n\\end{aligned}\n\\]\n이다.\n(\\(2\\)) 이제 \\(p(x)\\) 의 차수 \\(m\\) 이 \\(n\\) 보다 크거나 같고 \\(2n\\) 보다 작은 경우를 보자. 다항식의 나누기를 생각하면,\n\\[\np(x) =  q(x)P_n (x) + r(x)\n\\]\n를 생각 할 수 있다. 이 때 \\(x_i,\\, i=1,\\ldots,\\,n\\) 은 \\(P_n(x)\\) 의 근이므로\n\\[\np(x_i) = r(x_i)\n\\]\n이다. 여기서 \\(q(x)\\) 는 \\(m-n\\) 차 다항식이고 \\(r(x)\\) 는 \\(n\\) 보다 차수를 갖는 다항식이다. 르장드르 다항식의 성질 (\\(6\\)) 으로 부터 우리는 \\(q(x)\\) 가 \\(P_1(x),\\ldots,\\,P_{m-n}\\) 의 선형결합으로 표현 될 수 있다는 것을 안다. \\(m&lt;2n\\) 이므로 \\(m&lt;n\\) 이며, 르장드르 다항식의 성질 (\\(2\\)) 에 의해 \\(\\int q(x)P_n(x) = 0\\) 이다. 따라서\n\\[\n\\int_{-1}^1 p(x) = \\int_{-1}^1 r(x) = \\sum_{i=1}^n\\left[\\prod_{j=1,j\\ne i}^n \\int_{-1}^1 \\dfrac{x-x_j}{x_i-x_j}\\, dx \\right] r(x_i) = \\sum_{i=1}^n c_i p(x_i)\n\\]\n이다.\n\n이제 \\(P_n(x)\\) 에 대한 근을 통해 \\(\\displaystyle C_n^i = \\left[\\prod_{j=1,j\\ne i}^n \\int_{-1}^1 \\dfrac{x-x_j}{x_i-x_j}\\, dx \\right]\\) 를 테이블로 가지고 있다면 우리는 적분을 쉽게 계산 할 수 있다. 아래는 \\(n=2\\) 부터 \\(n=7\\) 까지의 테이블이다.\n\n르장드르 함수를 이요한 가우스 구적법 {#tbl-gauss_quadrature} \n\n\norder\nPoint\nWeight\n\n\n\n\n2\n-0.577350269189626\n1.000000000000000\n\n\n2\n0.577350269189626\n1.000000000000000\n\n\n3\n0.000000000000000\n0.888888888888889\n\n\n3\n-0.774596669241483\n0.555555555555556\n\n\n3\n0.774596669241483\n0.555555555555556\n\n\n4\n-0.339981043584856\n0.652145154862546\n\n\n4\n0.339981043584856\n0.652145154862546\n\n\n4\n-0.861136311594053\n0.347854845137454\n\n\n4\n0.861136311594053\n0.347854845137454\n\n\n5\n0.000000000000000\n0.568888888888889\n\n\n5\n-0.538469310105683\n0.478628670499367\n\n\n5\n0.538469310105683\n0.478628670499367\n\n\n5\n-0.906179845938664\n0.236926885056189\n\n\n5\n0.906179845938664\n0.236926885056189\n\n\n6\n0.661209386466264\n0.360761573048139\n\n\n6\n-0.661209386466264\n0.360761573048139\n\n\n6\n-0.238619186083197\n0.467913934572691\n\n\n6\n0.238619186083197\n0.467913934572691\n\n\n6\n-0.932469514203152\n0.171324492379170\n\n\n6\n0.932469514203152\n0.171324492379170\n\n\n7\n0.000000000000000\n0.417959183673469\n\n\n7\n0.405845151377397\n0.381830050505119\n\n\n7\n-0.405845151377397\n0.381830050505119\n\n\n7\n-0.741531185599394\n0.279705391489277\n\n\n7\n0.741531185599394\n0.279705391489277\n\n\n7\n-0.949107912342758\n0.129484966168870\n\n\n7\n0.949107912342758\n0.129484966168870\n\n\n\n르장드르 함수를 이용한 가우스 구적법은 NAJ.jl 의 integrate_gauss_quadrature 함수로 구현되었다. 위의 테이블을 이용하여 쉽게 구한다. 간단한 예로 함수 \\(f(x)=x^4\\) 의 \\([0,\\,1]\\) 구간에서의 적분을 \\(n=3\\) 에서 구하는 방법은 아래와 같다. 답은 0.2 가 정확히 나온다.\nusing NAJ\nintegrate_gauss_quadrature(x-&gt;x^4, 3, 0, 1)\n르장드르 함수를 이용한 가우스 구적법은 적분하고자 하는 함수가 매끄럽고 적분 구간에서 발산하지 않을 때 사용할 수 있다.",
    "crumbs": [
      "수치해석 I",
      "일변수 함수의 미분과 적분"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#footnotes",
    "href": "src/numerical_analysis_using_julia/06_calculus_of_one_variable_function.html#footnotes",
    "title": "일변수 함수의 미분과 적분",
    "section": "각주",
    "text": "각주\n\n\n더 정확히 말하자면 불연속인 점이 무한개라도 가산(countable) 이면 적분이 존재하지만, 어짜피 불연속인 점이 무한개일 경우는 여기서 다루지 않는다.↩︎\n최대 최소 정리 을 생각하자. \\([a,\\,b]\\) 구간에서 연속인 함수 \\(g(x)\\) 는 최대값 \\(g_M\\) 과 최소값 \\(g_m\\) 을 가진다. \\(\\{x_1,\\ldots,\\,x_n\\}\\subset [a,\\,b]\\) 라면 \\(g_m \\le g(x_i) \\le g_M\\) 이며, \\[\ng_m \\le \\dfrac{g(x_1)+ \\cdots + g(x_n)}{n}  \\le g_M\n\\] 이다. 중간값 정리에 의해 \\(g(\\overline{\\xi}) =  \\dfrac{g(x_1)+ \\cdots + g(x_n)}{n}\\) 를 만족하는 \\(\\overline{\\xi}\\in [a,\\,b]\\) 가 존재한다.↩︎",
    "crumbs": [
      "수치해석 I",
      "일변수 함수의 미분과 적분"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/08_matrix_decomposition.html",
    "href": "src/numerical_analysis_using_julia/08_matrix_decomposition.html",
    "title": "행렬의 분해",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]\n선형 대수학에서 행렬 분해는 행렬을 다른 행렬들의 곱으로 분해하는 것을 말한다. 다양한 행렬 분해 방법이 존재하며, 각각은 특정 유형의 문제 해결에 활용된다. 수치 해석에서는 효율적인 행렬 알고리즘을 구현하는 데 다양한 분해 방법을 사용 할 수 있다. 우리는 이미 LU 분해를 하는 방법과 그 유용성을 알아 보았다. 여기서는 다양한 행렬의 분해에 대해 알아보고자 한다.",
    "crumbs": [
      "수치해석 II",
      "행렬의 분해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/08_matrix_decomposition.html#수학적-기초",
    "href": "src/numerical_analysis_using_julia/08_matrix_decomposition.html#수학적-기초",
    "title": "행렬의 분해",
    "section": "1 수학적 기초",
    "text": "1 수학적 기초\n\n대각 지배 행렬\n\n\n\n\n\n\n\n정의 1 (대각 지배 행렬) 행렬 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n\\times n}\\) 의 각 행의 대각성분의 절대값이 그 행의 대각성분을 제외한 성분의 절대값의 합보다 크면, 즉 각각의 \\(i=1,\\ldots,\\,n\\) 행에 대해\n\\[\n|A_{ii}| \\ge \\sum_{j=1,\\, j\\ne i}^n |A_{ij}|, \\qquad i=1,\\ldots,\\,n\n\\]\n이면 이 행렬 \\(\\boldsymbol{A}\\) 를 행에 대한 대각 지배 행렬 (row-wise diagonally dominant matrix)이라 한다. 또한 각 열에대해\n\\[\n|A_{ii}| \\ge \\sum_{j=1,\\, j\\ne i}^n |A_{ji}|, \\qquad i=1,\\ldots,\\,n\n\\]\n이면 이 행렬 \\(\\boldsymbol{A}\\) 를 열에 대한 대각 지배 행렬 (column-wise diagonally dominant matrix)이라 한다. 행/열 에 대한 대각 지배 행렬의 조건에서 등호 조건을 제외하고 성립하면 행/열 에 대한 엄격한 대각 지배 행렬 (strictly row-wise/column-wise diagonally dominant matrix) 이라고 한다.\n\n\n\n\n\n즉 각 행(열)에 대해 대각성분의 절대값이 나머지 그 행(열)의 성분의 절대값의 합보다 크거나 같으면 행(열)에 대한 대각지배행렬이다.\n\n\n\n명제 1 엄격한 대각 지배 행렬은 가역행렬이다.\n\n\n\n\n(증명). 행에 대해 엄격한 대각 지배 행렬 \\(\\boldsymbol{A}\\) 이 가역행렬이면, \\(\\boldsymbol{A}^T\\) 는 열에 대해 엄격한 대각 지배 행렬이며 가역행렬이다. 따라서 행에 대해 엄격한 대각 지배 행렬이 가역행렬임을 보이면 된다.\n\\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 이 행에 대한 엄격한 대각 지배 행렬이라고 하자. 어떤 \\(\\boldsymbol{x}(\\ne \\boldsymbol{0}) \\in \\mathbb{F}^n\\) 에 대해 \\(\\boldsymbol{Ax}=\\boldsymbol{0}\\) 이면 \\(\\boldsymbol{A}\\) 는 가역행렬이 아니다. 따라서 \\(\\boldsymbol{Ax}=\\boldsymbol{0}\\) 인 \\(\\boldsymbol{0}\\) 벡터가 아닌 \\(\\boldsymbol{x}\\) 가 존재한다고 가정하고 모순을 보이면 된다. \\(\\boldsymbol{x}\\) 의 성분 가운데 그 절대값이 가장 큰 \\(|x_k|\\ne 0\\) 을 찾을 수 있다. \\(\\boldsymbol{Ax}=\\boldsymbol{0}\\) 이므로, 각각의 \\(i=1,\\ldots,\\,n\\) 에 대해\n\\[\n\\sum_{j=1}^n A_{ij}x_j=0\n\\]\n이며, 따라서 \\(x_k\\) 의 \\(k\\) 에 대해\n\\[\nA_{kk}x_k = -\\sum_{j=1,\\,j \\ne k}^n A_{kj}x_j\n\\]\n이다. \\(\\boldsymbol{A}\\) 가 행에 대해 엄격한 대각 지배 행렬이므로 모든 대각성분은 \\(0\\) 이 아니다. 삼각부등식에 의해,\n\\[\n|A_{kk}| |x_k|  \\le \\sum_{j=1,\\,j\\ne k}^n |A_{kj}| |x_j|\n\\]\n이 성립하며 \\[\n|A_{kk}| \\le \\sum_{j= 1,\\, j \\ne k}^n \\dfrac{|x_j|}{|x_k|} |A_{kj}| &lt; \\sum_{j= 1,\\, j \\ne k}^n |A_{kj}|\n\\]\n이므로 \\(\\boldsymbol{A}\\) 가 행에 대해 엄격한 대각 지배 행렬이라는 가정에 위배된다. 따라서 \\(\\boldsymbol{A}\\) 는 가역행렬이다. \\(\\square\\)\n\n\n\n\n명제 2 행에 대해 엄격한 대각 지배 행렬은 피보팅 없이 가우스-요르단 소거법을 수행 할 수 있다.\n\n\n\n\n(증명). \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 가 행에 대해 엄격한 대각 지배 행렬일 때 induction 으로 증명한다. 우선 \\(n=1\\) 일 경우는 자명하다. \\(n\\) 에 대해 명제가 성립함을 가정하자. 가우스-요르단 소거법을 첫번째 행을 가준으로 하여 피보팅 없이 두번째 행부터 \\(n+1\\) 번째 행까지 첫번째 열을 소거한 행렬을 \\(\\boldsymbol{A}'\\) 이라 하면 \\[\nA'_{ij} = A_{ij}- \\dfrac{A_{i1}}{A_{11}}A_{1j},\\qquad i=2,\\ldots,\\,n+1 \\tag{1}\n\\]\n이다.\n\\[\n\\begin{aligned}\n\\sum_{j=2,\\, j\\ne i}^n |A'_{ij}| &= \\sum_{j=2,\\, j \\ne i} \\left|A_{ij}- \\dfrac{A_{i1}}{A_{11}}A_{1j}\\right| \\le \\sum_{j=2,\\, j \\ne i}^n |A_{ij}| + \\sum_{j=2,\\,j \\ne i} \\left|\\dfrac{A_{i1}}{A_{11}}A_{1j}\\right|\n\\end{aligned}\\tag{2}\n\\]\n이다. 행에 대한 엄격한 지배행렬이라는 조건으로 부터 다음을 얻는다.\n\\[\n\\begin{aligned}\n\\sum_{j=2,\\, j \\ne i}^n |A_{ij}| &lt; |A_{ii}| - |A_{i1}|,\\\\\n\\sum_{j=2,\\, j \\ne i}^n |A_{1j}| &lt; |A_{11}| - |A_{1i}|\n\\end{aligned} \\tag{3}\n\\]\n위 식을 이용하여 \\((3)\\) 를 보면 \\[\n\\sum_{j=2,\\, j\\ne i}^n |A'_{ij}| &lt; |A_{ii}|-|A_{i1}| + \\dfrac{|A_{i1}|}{|A_{11}|} \\left(|A_{11}| - |A_{1i}|\\right) = |A_{ii}| - \\dfrac{|A_{i1}|}{|A_{11}|} |A_{1i}| \\tag{4}\n\\]\n이다. \\((1)\\) 로 부터,\n\\[\n|A'_{ii}| = \\left|A_{ii}- \\dfrac{A_{i1}}{A_{11}}A_{1i}\\right| \\ge \\left|A_{ii}\\right|- \\left|\\dfrac{A_{i1}}{A_{11}}A_{1i}\\right| \\tag{5}\n\\]\n이므로, \\((4)\\) 와 \\((5)\\) 를 결합하면,\n\\[\n|A'_{ii}| &gt; \\sum_{j=2,\\, j\\ne i}^n |A'_{ij}|\n\\]\n이다. 즉 2행 2열부터 \\(n+1\\) 행 \\(n+1\\) 열까지의 \\(\\boldsymbol{A}'\\) 의 부분행렬은 행에 대해 엄격한 지배행렬이므로 가정에 의해 피보팅 없이 가우스-조르단 소거법을 수행 할 수 있다. 따라서 임의의 크기의 정사각 행렬이 행에 대해 엄격한 대각 지배 행렬일 때, 피보팅 없이 가우스-조르단 소거법을 수행 할 수 있다. \\(\\square\\)\n\n\n\n\nPositive Definite 행렬\n\n\n\n\n\n\n\n정의 2 (Positive definite 행렬) 행렬 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 이 에르미트 행렬이며 모든 \\(\\boldsymbol{x}\\in \\mathbb{F}^n\\), \\(\\boldsymbol{x}\\ne \\boldsymbol{0}\\) 에 대해 \\(\\langle \\boldsymbol{Ax},\\boldsymbol{x}\\rangle &gt; 0\\) 일 경우, 즉 \\(\\boldsymbol{x}^T\\boldsymbol{Ax}&gt;0\\) 일 경우 \\(\\boldsymbol{A}\\) 를 positive definite 라 한다.\n\n\n\n\n\nPositive definite 행렬은 복소수체에서 정의할 수도 있으나 여기서는 실수체에서 정의된 행렬만 생각하기로 한다. Positive definite 행렬은 정의상 대칭행렬이므로 Interude : 수학에 관련된 표기법과 명제들 의 실수체에서 정의된 행렬의 스펙트럼 정리 를 만족한다. 즉 모든 \\(\\boldsymbol{x}\\in\\mathbb{R}^n\\) 은 \\(\\boldsymbol{A}\\) 의 고유벡터의 선형결합으로 표현 할 수 있다. 이제 positive definite 행렬에 대한 성질을 알아보자.\n\n\n\n명제 3 \\(\\boldsymbol{A}\\in \\mathbb{R}^{n \\times n}\\) 이 positive definite 일 경우 다음이 성립한다.\n  (\\(1\\)) \\(\\boldsymbol{A}\\) 의 모든 고유값은 양수이다.\n  (\\(2\\)) \\(\\det (\\boldsymbol{A}) &gt;0\\) 이다.\n  (\\(3\\)) \\(\\boldsymbol{A}\\) 는 가역행렬이다.\n  (\\(4\\)) \\(\\boldsymbol{A}\\) 의 대각성분은 모두 양수이다.\n  (\\(5\\)) \\(|A_{ij}| \\le \\max \\{|A_{ii}| : i=1,\\ldots,\\,n\\}\\).\n  (\\(6\\)) 모든 \\(i\\ne j\\) 에 대해 \\((A_{ij})^2 &lt; A_{ii} A_{jj}\\) 이다.\n\n\n\n\n(증명). (\\(1\\)) \\(\\lambda\\) 가 \\(\\boldsymbol{A}\\) 의 고유값이며 \\(\\boldsymbol{v}\\) 가 \\(\\lambda\\) 에 대한 고유벡터일 경우 \\(\\lambda = \\langle \\boldsymbol{Av}, \\boldsymbol{v}\\rangle &gt;0\\) 이어야 한다.\n(\\(2\\)) 대칭행렬 \\(\\boldsymbol{A}\\) 의 행렬식은 모든 고유값의 곱이므로 양수이다.\n(\\(3\\)) \\(\\boldsymbol{x}\\ne 0\\) 이며 \\(\\boldsymbol{Ax}=\\boldsymbol{0}\\) 일 경우 \\(\\langle \\boldsymbol{Ax}, \\boldsymbol{x}\\rangle = 0\\) 인데 이는 \\(\\boldsymbol{A}\\) 가 positive definite 가 아니라는 의미이다.\n(\\(4\\)) \\(A_{ii} = \\langle \\boldsymbol{A}\\hat{\\boldsymbol{e}}_i,\\, \\hat{\\boldsymbol{e}}_i\\rangle  &gt; 0\\)\n(\\(5\\)) \\(i\\ne j\\) 에 대해\n\\[\n\\begin{aligned}\n0&lt; \\langle \\boldsymbol{A}(\\hat{\\boldsymbol{e}}_i-\\hat{\\boldsymbol{e}}_j), (\\hat{\\boldsymbol{e}}_i-\\hat{\\boldsymbol{e}}_j)\\rangle = A_{ii}+ A_{jj} - 2 A_{ij} \\\\\n0&lt; \\langle \\boldsymbol{A}(\\hat{\\boldsymbol{e}}_i+\\hat{\\boldsymbol{e}}_j), (\\hat{\\boldsymbol{e}}_i+\\hat{\\boldsymbol{e}}_j)\\rangle = A_{ii}+ A_{jj} + 2 A_{ij} \\\\\n\\end{aligned}\n\\]\n이므로 \\[\n|A_{ij}| &lt; \\dfrac{A_{ii}+A_{jj}}{2} \\le \\max \\{|A_{ii}| : i=1,\\ldots,\\,n\\}\n\\]\n이다.\n(\\(6\\)) 임의의 실수 \\(t\\) 에 대해 \\(\\boldsymbol{x} = \\hat{\\boldsymbol{e}}_i + t \\hat{\\boldsymbol{e}}_j\\) 라고 하자. \\(\\langle \\boldsymbol{Ax},\\,\\boldsymbol{x}\\rangle = t^2A_{ii} + 2tA_{ij} + A_{jj} &gt; 0\\) 이므로 \\((A_{ij})^2-A_{ii}A_{ij}&lt;0\\) 이어야 한다. \\(\\square\\)\n\n\n\n\n\n\n\n\n\n정의 3 (부분 행렬과 선행 주 부분 행렬) 행렬 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 에 대해 임의의 중복되지 않은 행 \\(I=\\{i_1,\\ldots,\\,i_p: 1\\le i_k\\le m\\}\\) 과 중복되지 않은 열 \\(J=\\{j_1,\\ldots,\\,j_q: 1 \\le j_l \\le n \\}\\) 을 모아 만든 행렬을 \\(\\boldsymbol{A}\\) 의 부분 행렬 (submatrix)이라고 하고 \\(\\boldsymbol{A}[I, J]\\) 라고 쓴다. \\(I=J=\\{1,\\,2,\\ldots,\\,k\\}\\)일 때 \\(\\boldsymbol{A}[I, J]\\) 를 선행 주 부분 행렬 (leading principal submatrix) 이라고 한다.\n\n\n\n\n\n이제 정사각 행렬 \\(\\boldsymbol{A}\\) 가 positive definite 인 것과 \\(\\boldsymbol{A}\\) 의 모든 선행 주 부분행렬의 행렬식이 \\(0\\) 보다 큰 것이 동치임을 보이고자 한다. 이를 위해 몇가지 미리 보여야 할 것이 있다.\n\n\n보조정리 1 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 이 에르미트행렬이고 \\(\\boldsymbol{Q} \\in \\mathbb{F}^{n \\times n}\\) 이 가역행렬 일 때 다음은 동치이다.\n  (\\(1\\)) \\(\\boldsymbol{A}\\) 가 positive definite 이고 \\(\\boldsymbol{Q}^\\ast\\boldsymbol{AQ}\\) 가 에르미트행렬이다.\n  (\\(2\\)) \\(\\boldsymbol{Q}^\\ast \\boldsymbol{AQ}\\) 가 positive definte 이다.\n\n\n\n\n(증명). (\\(1 \\implies 2\\)) \\(\\boldsymbol{0}\\) 벡터가 아닌 \\(\\boldsymbol{v}\\in \\mathbb{F}^n\\) 에 대해 \\(\\boldsymbol{Qv}\\ne \\boldsymbol{0}\\) 이다. \\(\\boldsymbol{A}\\) 가 positive definite 이므로,\n\\[\n0 &lt; (\\boldsymbol{Qv})^\\ast \\boldsymbol{A}(\\boldsymbol{Qv}) = \\boldsymbol{v}^\\ast \\left(\\boldsymbol{Q}^\\ast\\boldsymbol{AQ}\\right)\\boldsymbol{v}\n\\]\n이므로 \\(\\boldsymbol{Q}^\\ast\\boldsymbol{AQ}\\) 는 positive definite 이다.\n(\\(2 \\implies 1\\)) \\(\\boldsymbol{A}\\) 가 에르미트행렬이므로 \\((\\boldsymbol{Q}^\\ast\\boldsymbol{AQ})^\\ast = \\boldsymbol{Q}^\\ast \\boldsymbol{A}^\\ast\\boldsymbol{Q} = \\boldsymbol{Q}^\\ast\\boldsymbol{AQ}\\) 이다. 즉 \\(\\boldsymbol{Q}^\\ast\\boldsymbol{AQ}\\) 는 에르미트행렬이다. \\(\\boldsymbol{Q}\\) 가 가역이므로 임의의 \\(\\boldsymbol{x}\\in \\mathcal{M}_n(\\mathbb{F})\\) 에 대해 \\(\\boldsymbol{x}=\\boldsymbol{Qy}\\) 를 만족하는 \\(\\boldsymbol{y}\\in \\mathcal{M}_n(\\mathbb{F})\\) 이 존재한다.\n\\[\n\\boldsymbol{x}^\\ast\\boldsymbol{Ax} =  (\\boldsymbol{Qy})^\\ast \\boldsymbol{A}\\boldsymbol{Qy} = \\boldsymbol{y}^\\ast\\boldsymbol{Q}^\\ast\\boldsymbol{AQ}\\boldsymbol{y} &gt; 0\n\\]\n이므로 \\(\\boldsymbol{A}\\) 는 positive defnite 이다. \\(\\square\\)\n\n\n\n\n명제 4 (실베스터 판정법 (Sylvester’s criterion)) 에르미트 행렬 \\(\\boldsymbol{A}\\) 에 대해 다음은 동치이다.\n  (\\(1\\)) \\(\\boldsymbol{A}\\) 의 모든 선행 주 부분 행렬의 행렬식이 양수이다.\n  (\\(2\\)) \\(\\boldsymbol{A}\\) 는 positive definite 이다.\n\n\n 명제 4 가 실베스터 판정법이라고 불리우는 것은 임의의 에르미트 행렬이 positive definite 인지 아닌지를 확인하는 방법이기 때문이다.\n\n(증명). \\(\\boldsymbol{A}\\) 의 \\(k\\) 행 \\(k\\) 열 까지의 선 주 부분행렬을 \\(\\boldsymbol{A}_k\\) 라고 쓰자.\n(\\(1\\implies 2\\)) Inductinon 을 통해 증명한다. \\(1 \\times 1\\) 행렬의 경우는 trivial 하다. 이제 \\(n\\times n\\) 행렬에 대해 성립한다고 가정한다. \\(\\boldsymbol{A}\\) 가 \\((n+1)\\times (n+1)\\) 행렬이라 하자.Carl P.Simon & Lawrence E. Blume 의 Mathematics for Economists 에 나오는 증명이다.\n우선 모든 \\(\\boldsymbol{A}_i\\), \\(i=1,\\ldots,\\,n\\) 의 행렬식이 양수이면 \\(\\boldsymbol{A}\\) 가 positive definite 임을 보이자. 우선 \\(\\boldsymbol{A}\\) 를 \\(\\boldsymbol{A}_n\\) 에 대하여 아래와 같이 분할하여 보자.\n\\[\n\\boldsymbol{A} = \\left[\\begin{array}{c|c} \\boldsymbol{A}_n & \\boldsymbol{a} \\\\ \\hline \\boldsymbol{a}^\\ast & a\\end{array}\\right].\n\\]\n여기서 \\(\\boldsymbol{a}=\\begin{bmatrix} A_{1, n+1} & \\cdots & A_{n, n+1}\\end{bmatrix}^T\\) 이고 \\(a=A_{n+1, n+1}\\) 이다. 또한 \\(\\boldsymbol{A}\\) 가 에르미트행렬이므로 \\(\\boldsymbol{A}_n\\) 도 에르미트행렬이며 따라서 \\((\\boldsymbol{A}^\\ast)^{-1}=\\boldsymbol{A}^{-1}\\) 이다. . 그렇다면,\n\\[\n\\boldsymbol{A} = \\left[\\begin{array}{c|c} \\boldsymbol{I}_n & \\boldsymbol{0} \\\\ \\hline (\\boldsymbol{A}_n^{-1}\\boldsymbol{a})^\\ast & 1\\end{array}\\right] \\left[\\begin{array}{c|c} \\boldsymbol{A}_n & \\boldsymbol{0}_n \\\\ \\hline (\\boldsymbol{0}_n)^\\ast & d\\end{array}\\right] \\left[\\begin{array}{c|c} \\boldsymbol{I}_n & \\boldsymbol{A}_n^{-1}\\boldsymbol{a} \\\\ \\hline (\\boldsymbol{0}_n)^\\ast & 1\\end{array}\\right] = \\boldsymbol{Q}^\\ast\\boldsymbol{BQ}\n\\]\n이며 이 때 \\(d=a-\\boldsymbol{a}^\\ast(\\boldsymbol{A}_n)^{-1}\\boldsymbol{a}\\) 이다. 우리는 \\(\\det (\\boldsymbol{Q}^\\ast) = \\det (\\boldsymbol{Q})=1\\) 이며 \\(\\det (\\boldsymbol{B})=d \\cdot \\det (\\boldsymbol{A}_n)\\) 임을 안다. 즉\n\\[\n\\det (\\boldsymbol{A}) =  d \\cdot \\det (\\boldsymbol{A}_n)\n\\]\n이다. \\(\\boldsymbol{A}\\) 가 positive definite 이며 \\(\\det (\\boldsymbol{A})&gt;0\\) 이므로 \\(d&gt;0\\) 이다. 즉 모든 선행 주 부분행렬식이 양수이면 그 행렬은 positive definite 이다.\n(\\(2 \\implies 1\\)) \\(\\boldsymbol{A}\\) 가 positive definite 임을 가정하자. 임의의 \\(\\boldsymbol{x}\\in \\mathcal{M}_{n+1}(\\mathbb{R})\\) 에 대해 \\(\\boldsymbol{x} = \\begin{bmatrix} \\boldsymbol{x}_0 & x \\end{bmatrix}^T\\), \\(\\boldsymbol{x}_0 \\in \\mathcal{M}_{n}(\\mathbb{R})\\) 이라 하면,\n\\[\n\\boldsymbol{x}^\\ast \\boldsymbol{Bx} = \\boldsymbol{x}^\\ast \\boldsymbol{A}_n \\boldsymbol{x} + d\\cdot x^2\n\\]\n이다. \\(\\boldsymbol{A}_n\\) 이 가정에 의해 positive definite 이며, \\(d&gt;0\\) 이므로 \\(\\boldsymbol{x}^\\ast\\boldsymbol{Bx}&gt;0\\) 이다. 따라서 보조정리 1 에 의해 \\(\\boldsymbol{B}=\\boldsymbol{Q}^\\ast\\boldsymbol{AQ}\\) 일 때 \\(\\boldsymbol{A}\\) 는 positive definite 이다. \\(\\square\\)",
    "crumbs": [
      "수치해석 II",
      "행렬의 분해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/08_matrix_decomposition.html#숄레스키-분해",
    "href": "src/numerical_analysis_using_julia/08_matrix_decomposition.html#숄레스키-분해",
    "title": "행렬의 분해",
    "section": "2 숄레스키 분해",
    "text": "2 숄레스키 분해\n\n\\(\\boldsymbol{LL}^\\ast\\) 분해\n\n\n정리 1 Positive definite 행렬 \\(\\boldsymbol{A}\\in \\mathbb{F}^{n \\times n}\\) 은 어떤 모든 대각성분이 양수인 하삼각행렬 \\(\\boldsymbol{L} \\in \\mathbb{F}^{n \\times n}\\) 에 대해 \\(\\boldsymbol{A}= \\boldsymbol{LL}^\\ast\\) 이다.\n\n\n\n\n(증명). 수학적 귀납법으로 증명한다. \\(n=1\\) 일 때 경우 \\(\\boldsymbol{A}=\\begin{bmatrix}a \\end{bmatrix}\\) 라면 명제 3 의 (\\(4\\)) 에 의해 \\(a&gt;0\\) 이므로 \\(\\boldsymbol{L}=\\begin{bmatrix} \\sqrt{a}\\end{bmatrix}\\) 이다. \\(n\\) 일때 성립함을 가정하자. \\(\\boldsymbol{A}\\in \\mathbb{F}^{(n+1)\\times (n+1)}\\) 에 대해\n\\[\n\\boldsymbol{A} = \\left[\\begin{array}{c|c} \\boldsymbol{A}_n & \\boldsymbol{b} \\\\ \\hline \\boldsymbol{b}^\\ast & a\\end{array}\\right].\n\\]\n로 표현 할 수 있다. \\(\\boldsymbol{A}_n = \\boldsymbol{L}_n \\boldsymbol{L}_n^\\ast\\) 라면 일 때\n\\[\n\\boldsymbol{L} = \\left[\\begin{array}{c|c} \\boldsymbol{L}_n & \\boldsymbol{0} \\\\ \\hline \\boldsymbol{c}^\\ast & d\\end{array}\\right].\n\\]\n라 하자.\n\\[\n\\boldsymbol{LL}^\\ast = \\left[\\begin{array}{c|c} \\boldsymbol{L}_n\\boldsymbol{L}_n^\\ast & \\boldsymbol{L}_n \\boldsymbol{c} \\\\ \\hline (\\boldsymbol{L}_n \\boldsymbol{c})^\\ast & |d|^2\\end{array}\\right].\n\\]\n이므로 \\(\\boldsymbol{b}=\\boldsymbol{L}_n \\boldsymbol{c}\\), \\(d+ \\boldsymbol{c}^\\ast\\boldsymbol{c}=|a|^2\\) 을 만족하는 \\(\\boldsymbol{c}\\) 와 양수 \\(d\\) 가 존재한다면 \\(\\boldsymbol{A} = \\boldsymbol{LL}^\\ast\\) 이다.\n\\(\\boldsymbol{L}_n\\) 이 대각성분이 양수인 하삼각행렬이므로 가역행렬이다. 따라서 \\(\\boldsymbol{c}=(\\boldsymbol{L}_n)^{-1} \\boldsymbol{b}\\) 이다. 따라서 어쨋든 (그것이 양수가 아니고 복소수라고 하더라도) \\(d=\\sqrt{|a|^2-\\boldsymbol{c}^\\ast \\boldsymbol{c}}\\) 가 존재하며, \\(\\det(\\boldsymbol{A})=|\\det(\\boldsymbol{L}_n)|^2 d^2 &gt;0\\) 로부터 \\(d^2&gt;0\\) 임을 안다. 따라서 \\(d&gt;0\\) 를 얻는다. \\(\\square\\)\n\n\n\n\n\n\n\n\n\n정의 4 (숄레스키 분해) 정리 1 에 따라 Positive definite 행렬 \\(\\boldsymbol{A}\\in\\mathbb{F}^{n \\times n}\\) 를 모든 대각성분이 양수인 하삼각 행렬 \\(\\boldsymbol{L}\\in \\mathbb{F}^{n \\times n}\\) 에 대해 \\(\\boldsymbol{A}=\\boldsymbol{LL}^\\ast\\) 로 분해하는 것을 숄레스키 분해 (Cholesky decomposition) 라고 한다. \\(\\mathbb{F} =\\mathbb{R}\\) 인 경우 \\(\\boldsymbol{A}=\\boldsymbol{LL}^T\\) 이므로 \\(\\boldsymbol{LL}^T\\) 분해라고도 한다.\n\n\n\n\n\n\n\n구현\n\\(\\boldsymbol{A},\\,\\boldsymbol{L}\\in \\mathbb{F}^{n \\times n}\\) 을 생각하자. \\(\\boldsymbol{A} = \\boldsymbol{LL}^\\ast\\) 이므로,\n\\[\n\\begin{bmatrix} A_{11} & A_{12} & A_{13} & \\cdots & A_{1n} \\\\\nA_{21} & A_{22} & A_{23} & \\cdots & A_{2n}\\\\\n\\vdots & & & & \\vdots \\\\\nA_{n1} & A_{n2} & A_{n3} & \\cdots & A_{nn}\\end{bmatrix}\n= \\begin{bmatrix} L_{11} & 0 & 0 & \\cdots & 0 \\\\\nL_{21}& L_{22} & 0 & \\cdots & 0 \\\\\n\\vdots & & & & \\vdots \\\\\nL_{n1} & L_{n2} & L_{n3} & \\cdots & L_{nn} \\end{bmatrix}\n\\begin{bmatrix} L_{11} & \\overline{L_{21}} & \\overline{L_{31}} & \\cdots & \\overline{L_{n1}} \\\\\n0 & L_{22} & \\overline{L_{32}} & \\cdots & \\overline{L_{n2}}\\\\\n\\vdots & & & & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\overline{L_{nn}}\\end{bmatrix}\n\\]\n이다. 또한 \\(\\boldsymbol{L}\\) 이 하삼각행렬이므로 다음이 성립한다. \\[\nA_{ij} = \\boldsymbol{L}_{i:} (\\boldsymbol{L}^\\ast)_{:j} = \\sum_{k=1}^n L_{ik} \\overline{L_{jk}} = \\sum_{k=1}^{\\min (i, j)} L_{ik} \\overline{L_{jk}}\n\\]\n\\(A_{11}=|L_{11}|^2\\), \\(A_{i1} =  L_{i1}\\overline{L_{11}}=L_{i1}L_{11}\\) 이므로 \\(L_{11}= \\sqrt{A_{11}},\\, L_{i1} = A_{i1}/L_{11}\\) 으로 부터 \\(\\boldsymbol{L}\\) 의 1열을 얻을 수 있다. 이제 \\(\\boldsymbol{L}\\) 의 \\(j-1\\) 열까지 얻었다고 가정하자.\n\\[\n\\begin{aligned}\nA_{1, j} & = L_{11}\\overline{L_{j1}},\\\\\nA_{i, j} & = \\sum_{k=1}^{\\min (i, j)} L_{ik}\\overline{L_{jk}},\n\\end{aligned}\n\\]\n이다. \\(i\\le j\\) 인 경우,\n\\[\nA_{i, j} = L_{i1}\\overline{L_{j1}} + \\cdots + L_{ii}\\overline{L_{ji}}\n\\]\n이므로 다음을 얻는다.\n\\[\n\\begin{aligned}\n\\text{if }j \\ne i & &L_{ji} &= \\dfrac{1}{(\\overline{L_{ii}})}\\overline{A_{ij} - \\sum_{k=1}^{i-1} L_{ik}\\overline{L_{jk}}}, \\\\\n& &L_{jj} &= \\sqrt{A_{ij}- \\sum_{k=1}^{j-1} L_{jk}\\overline{L_{jk}}}\n\\end{aligned}\n\\]\n즉 \\(i\\le j\\) 일 경우 \\(L_{ji}\\) 는 \\(\\boldsymbol{L}\\) 의 \\(1\\) 열부터 \\(j-1\\) 열까지와 \\(j\\) 열의 \\(1\\) 행부터, \\(j-1\\) 행까지의 값을 이용하여 얻을 수 있다. Julia 로 구현한 것은 아래와 같다. 하지만 LinearAlgebra 모듈에 cholesky 함수로 존재하며(그래서 함수 이름을 굳이 mcholesky 로 붙였다), 앞으로 필요하다면 그것을 쓸 것이다.\n\nfunction mcholesky(A::Matrix{T}) where T&lt;:Number\n    M = size(A)[1]\n    L = zero(A)\n    L[1, 1] = sqrt(A[1, 1])\n    for i in 2:M\n        L[i, 1] = A[i, 1]/L[1, 1]\n    end\n    for j in 2:M, i in 1:j\n        if j == i \n            L[i, i] = sqrt(A[i, i] - dot(L[i, 1:i-1], L[i, 1:i-1]) )\n        else \n            L[j, i] = (A[i, j] - dot(L[j, 1:i-1], L[i, 1:i-1]))/L[i, i]\n        end\n    end\n    return L\nend\n\n\n\n\\(\\boldsymbol{LDL}^\\ast\\) 분해\n\n\n명제 5 대각성분이 모두 \\(0\\) 이 아닌 하삼각 행렬 \\(\\boldsymbol{L}\\) 은 어떤 대각행렬 \\(\\boldsymbol{\\Delta}\\) 와 대각성분이 \\(1\\) 인 하삼각 행렬 \\(\\boldsymbol{\\Lambda}\\) 에 대해 \\(\\boldsymbol{L}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Delta}\\) 로 분해된다.\n\n\n\n\n(증명). \\(\\boldsymbol{\\Delta}\\) 를 \\(\\boldsymbol{L}\\) 의 대각성분만으로 이루어진 대각행렬이라고 하고 \\(\\boldsymbol{\\Lambda}\\) 를 \\(\\Lambda_{ij} = L_{ij}/\\Delta_{ii}\\) 라고 하면,\n\\[\n(\\boldsymbol{\\Lambda \\Delta})_{ij} = \\sum_{k=1}^i \\Lambda_{ik}\\Delta_{kj} = L_{ij}\n\\]\n이므로 \\(\\boldsymbol{L}=\\boldsymbol{\\Lambda \\Delta}\\) 이다. \\(\\square\\)\n\n\n우리는 positive definite 행렬이 하삼각 행렬과 그 하삼각행렬의 에르미트 conjugarte 의 곱으로 표현된다는 것을 안다(정리 1). 여기에 명제 5 를 같이 생각하면 positive definite 행렬은 어떤 대각행렬이 \\(1\\) 인 하삼각행렬 \\(\\boldsymbol{L}\\) 과 대각 행렬 \\(\\boldsymbol{D}_0\\) 에 대해\n\\[\n\\boldsymbol{A} = (\\boldsymbol{LD}_0) (\\boldsymbol{LD}_0)^\\ast = \\boldsymbol{L}(\\boldsymbol{D}_0\\boldsymbol{D}_0^\\ast) \\boldsymbol{L}^\\ast\n\\]\n이다. \\(\\boldsymbol{D}_0\\) 는 모든 성분이 양수인 대각행렬이므로 \\(\\boldsymbol{D}=(\\boldsymbol{D}_0\\boldsymbol{D}_0^\\ast)\\) 도 그러하다. 따라서 우리는 다음의 결론을 얻을 수 있다.\n\n\n명제 6 positive definite 행렬 \\(\\boldsymbol{A}\\) 는 어떤 대각성분이 모두 \\(1\\) 인 하삼각 행렬 \\(\\boldsymbol{L}\\) 과 대각성분이 모두 양수인 대각행렬 \\(\\boldsymbol{D}\\) 에 대해 \\(\\boldsymbol{A} = \\boldsymbol{LDL}^\\ast\\) 로 분해된다.\n\n\n\n\n\n크라우트 분해\n숄레스키 분해와는 유사한 분해로 크라우트 분해가 있다. 삼중대각행렬 을 하쌍대각행렬 \\(\\boldsymbol{L}\\) 과 대각성분이 모두 \\(1\\) 인 상쌍대각행렬 \\(\\boldsymbol{U}\\) 의 곱으로 분해하는 것을 크라우트 분해 (Crount decomposition) 라고 한다.",
    "crumbs": [
      "수치해석 II",
      "행렬의 분해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/08_matrix_decomposition.html#qr-분해",
    "href": "src/numerical_analysis_using_julia/08_matrix_decomposition.html#qr-분해",
    "title": "행렬의 분해",
    "section": "3 QR 분해",
    "text": "3 QR 분해\n\n하우스홀더 행렬\n\n\n\n\n\n\n\n정의 5 (하우스홀더 행렬과 하우스홀더 변환) 벡터공간 \\(\\mathbb{C}^n\\) 에서의 벡터 \\(\\boldsymbol{v}\\) 에 대해 다음과 같이 정의된 \\(\\boldsymbol{H}_v\\) 를 하우스홀더 행렬 (Householder matrix) 이라 한다.\n\\[\n\\begin{aligned}\n\\boldsymbol{H}_{\\boldsymbol{v}} := I_n- \\dfrac{2\\boldsymbol{v}\\boldsymbol{v}^{\\ast}}{\\|\\boldsymbol{v}\\|^2}, \\qquad \\text{i. e. }\\quad\n(\\boldsymbol{H}_{\\boldsymbol{v}})_{ij} := \\delta_{ij} - \\dfrac{2 v_i \\overline{v_j}}{\\|\\boldsymbol{v}\\|^2}.\n\\end{aligned}\n\\tag{1}\\] 여기서 \\(\\boldsymbol{v}\\boldsymbol{v}^{\\ast}\\) 는 벡터의 내적이 아니라 \\(n\\times 1\\) 행렬 \\(\\boldsymbol{v}\\) 와 \\(1 \\times n\\) 행렬 \\(\\boldsymbol{v}^{\\ast}\\) 가 곱해진 \\(n \\times n\\) 행렬을 의미한다. 벡터 \\(\\boldsymbol{x}\\in \\mathbb{C}^n\\) 에 대해 \\(\\boldsymbol{H}_{\\boldsymbol{v}} \\boldsymbol{x}\\in \\mathbb{C}^n\\) 을 하우스홀더 변환 이라 한다. \\(\\boldsymbol{v}\\) 가 단위벡터일 경우, 즉 \\(\\|\\boldsymbol{v}\\|=1\\) 라면 좀 더 간단하게 쓸 수 있다.\n\\[\n\\boldsymbol{H}_{\\boldsymbol{v}} = I_n- 2\\boldsymbol{v}\\boldsymbol{v}^{\\ast},\\qquad \\text{where } \\|\\boldsymbol{v}\\|=1.\n\\tag{2}\\]\n\n\n\n\n\n\n\n명제 7 하우스홀더 행렬 \\(\\boldsymbol{H}_{\\boldsymbol{v}} \\in \\mathbb{C}^{n \\times n}\\) 은 다음의 특징을 가진다.\n  (\\(1\\)) 에르미트 행렬이다. 즉 \\(\\boldsymbol{H}_{\\boldsymbol{v}}=  \\boldsymbol{H}_{\\boldsymbol{v}}^\\ast\\).\n  (\\(2\\)) 유니타리 행렬이다. 즉 \\(\\boldsymbol{H}_{\\boldsymbol{v}} (\\boldsymbol{H}_{\\boldsymbol{v}})^\\ast = I\\).\n\n\n\n\n(증명). (\\(1\\)) \\(\\boldsymbol{v}\\) 가 단위벡터일 경우에만 보여도 된다. \\(\\boldsymbol{H} = \\boldsymbol{H}_{\\boldsymbol{v}}\\) 라 하면,\n\\[\n(\\boldsymbol{H}^\\ast)_{ij} = \\overline{H_{ji}}= \\delta_{ij}-2 \\overline{v_j \\overline{v_i}} = \\delta_{ij}-2 v_i \\overline{v_j} = (\\boldsymbol{H})_{ij}\n\\]\n이므로 \\(\\boldsymbol{H}_{\\boldsymbol{v}}\\) 는 에르미트 행렬이다.\n(\\(2\\)) 또한,\n\\[\n\\begin{aligned}\n\\left(\\boldsymbol{H} (\\boldsymbol{H}^\\ast)\\right)_{ij} &= \\left((\\boldsymbol{H})^2\\right)_{ij} = \\sum_{k}(\\delta_{ik}-2 v_i \\overline{v_k})(\\delta_{kj} -2 v_k \\overline{v_j}) \\\\\n&= \\sum_k \\delta_{ik}\\delta_{kj} - 2\\sum_k \\delta_{ik}v_k \\overline{v_j} - 2 \\sum_k \\delta_{kj} v_i \\overline{v_k} + 4 \\sum_{k} v_i \\overline{v_j} v_k \\overline{v_k} \\\\\n&= \\delta_{ij} - 2 v_i \\overline{v_{j}} - 2 v_i \\overline{v_j} + 4 v_i \\overline{v_j} = \\delta_{ij}\n\\end{aligned}\n\\]\n이다. \\(\\square\\)\n\n\n\n\n따름정리 1 실수성분의 하우스홀더 행렬 \\(\\boldsymbol{H}_{\\boldsymbol{v}} \\in \\mathbb{R}^{n\\times n}\\) 은 대칭행렬이며 직교행렬이다.\n\n\n\n\n\n\n하우스홀더 변환과 하우스홀더 반사\n하우스 홀더 행렬은 유니타리 행렬이므로 하우스홀더 변환은 가역변환이다. \\(\\mathbb{R}^n\\) 공간에서 벡터 \\(\\boldsymbol{v}\\) 만으로 \\(\\boldsymbol{v}\\) 와 수직이며 원점을 지나는 평면이 유일하게 정의될 수 있다. 임의의 벡터 \\(\\boldsymbol{x} \\in \\mathbb{F}^n\\) 에 대해,\n\\[\n\\begin{aligned}\n(\\boldsymbol{H}_{\\boldsymbol{v}}\\boldsymbol{x})_k &=  x_k - \\dfrac{2}{\\|\\boldsymbol{v}\\|^2}  \\sum_{j=1}^n v_k \\overline{v_j} x_j\\\\\n\\boldsymbol{H}_{\\boldsymbol{v}}\\boldsymbol{x} &= \\boldsymbol{x} - 2 \\dfrac{\\langle \\boldsymbol{x},\\,  \\boldsymbol{v} \\rangle \\boldsymbol{v}}{\\|\\boldsymbol{v}\\|^2}\n\\end{aligned}\n\\]\n이며, \\(\\boldsymbol{x}\\) 의 \\(\\boldsymbol{v}\\) 벡터에 대한 projection \\(\\text{Proj}_\\boldsymbol{v}\\boldsymbol{x}= \\langle \\boldsymbol{v},\\, \\boldsymbol{x}\\rangle \\boldsymbol{v}\\) 이므로,\n\\[\n\\begin{aligned}\n\\dfrac{1}{2 }(\\boldsymbol{x}+\\boldsymbol{H_vx}) &= \\boldsymbol{x} - \\langle \\boldsymbol{v,\\,  x}\\rangle \\boldsymbol{v} = \\boldsymbol{x} - \\text{Proj}_{\\boldsymbol{v}} \\boldsymbol{x} \\\\\n\\boldsymbol{x}-\\boldsymbol{H_vx} &= 2\\langle \\boldsymbol{v ,\\, x} \\rangle  \\boldsymbol{v} = 2\\, \\text{Proj}_{\\boldsymbol{v}}\\boldsymbol{x}\n\\end{aligned}\n\\]\n이다. 즉 \\(\\boldsymbol{x}\\) 와 \\(\\boldsymbol{Hx}\\) 는 \\(\\boldsymbol{v}\\) 에 의해 정의되는 평면에 대해 대칭이다. 이런 이유로 하우스홀더 변환을 하우스홀더 반사(Householder reflection) 라고도 한다.\n\n\n\n\n\n\n그림 1: Householder 반사\n\n\n\n\\(\\boldsymbol{x}\\) 를 \\(\\boldsymbol{v}\\) 와 평행한 부분과 수직한 부분으로 분리하자. 즉 \\(\\boldsymbol{x}_{\\|} = \\text{Proj}_{\\boldsymbol{v}}\\boldsymbol{x}\\), \\(\\boldsymbol{x}_{\\perp} = \\boldsymbol{x}-\\text{Proj}_{\\boldsymbol{v}}\\boldsymbol{x}\\) 라 하면, \\(\\boldsymbol{x} = \\boldsymbol{x}_{\\|} + \\boldsymbol{x}_{\\perp}\\) 이며 \\(\\boldsymbol{x}\\cdot \\boldsymbol{v} = \\boldsymbol{x}_{\\|}\\cdot \\boldsymbol{v}\\) 이다.\n\\[\n\\boldsymbol{H}_{\\boldsymbol{v}}\\boldsymbol{x} = \\boldsymbol{x} - 2 \\boldsymbol{x}_{\\|} = \\boldsymbol{x}_{\\perp} - \\boldsymbol{x}_{\\|}\n\\]\n이다.\n\n\n\n실벡터 공간에서의 하우스홀더 변환과 QR 분해\n\\(\\boldsymbol{x}_1,\\ldots,\\,\\boldsymbol{x}_n\\in \\mathbb{R}^n\\) 에 대해 \\(\\boldsymbol{X}=\\begin{bmatrix} \\boldsymbol{x}_1 & \\cdots &\\boldsymbol{x}_n\\end{bmatrix}\\) 라고 하자. \\(\\mathbb{R}^n\\) 에서의 벡터 \\(\\boldsymbol{x}\\) 과 표준 기저 \\(\\{\\hat{\\boldsymbol{e}}_1,\\ldots,\\,\\hat{\\boldsymbol{e}}_n\\}\\) 를 생각하자. 이 때 \\(\\boldsymbol{v}_1\\) 을 다음과 같이 정한다.\n\\[\n\\boldsymbol{v}_1 : = \\boldsymbol{x}_1-\\|\\boldsymbol{x}_1\\|_2\\hat{\\boldsymbol{e}}_1\n\\]\n그리고 \\(\\boldsymbol{Q}_1 = \\boldsymbol{H}_{\\boldsymbol{v}_1}\\) 이라고 하자. \\(\\boldsymbol{Q}_1\\boldsymbol{x_1} = \\|\\boldsymbol{x}_1\\|_2 \\hat{\\boldsymbol{e}}_1\\) 이므로,\n\\[\n\\boldsymbol{Q}_1\\boldsymbol{X} = \\begin{bmatrix} \\|\\boldsymbol{x}_1\\|_2 & x'_{12} & \\cdots & x'_{1n} \\\\ 0 & x'_{22} & \\cdots & x'_{2n}  \\\\  \\vdots  & \\vdots & \\cdots & \\vdots  \\\\ 0 & x'_{n2} & \\cdots & x'_{nn}  \\end{bmatrix}\n\\]\n가 된다. 하우스홀더 행렬의 정의에 의해 \\(\\boldsymbol{v}_1\\) 을 \\(0\\) 이 아닌 상수로 나누어 주어도 하우스홀더행렬에는 변화가 없는데 보통 하우스홀더 행렬의 맨 앞 성분을 \\(1\\) 로 만드는 것이 좋다고 한다(Golub and Van Loan (2013), pp.235). 여기서 문제가 되는게 만약 \\(\\boldsymbol{x}_1\\) 이 첫번째 성분을 제외한 성분이 모두 \\(0\\) 이고 첫번째 성분이 양수라면 \\(\\boldsymbol{v}_1 = \\boldsymbol{0}\\) 이 되며 이 경우 정규화 할수 없기 때문에 문제가 된다. 즉 코드에 \\(\\boldsymbol{v}/\\|\\boldsymbol{v}\\|_2\\) 가 들어가는데 \\(\\|\\boldsymbol{v}\\|_2=0\\) 이므로 \\(0\\) 으로 나누어지는 에러가 발생한다. 이 때는 하우스 홀더 행렬을 \\(\\boldsymbol{I}\\), \\(\\boldsymbol{v}_1 = \\hat{\\boldsymbol{e}}_1\\) 으로 놓고 계산하면 원하는 결과를 얻을 수 있다.\n이제 \\(\\boldsymbol{x}'_2 = [x'_{22}, \\ldots, x'_{n2}]^T\\) (\\(\\boldsymbol{Q}_1\\boldsymbol{x}_1\\) 의 두번째 행의 두번째 열부터 마지막 열까지로 이루어진 벡터) 를 이용하여 앞의 \\(\\boldsymbol{Q}_1\\) 과 같은 과정에서 \\(\\boldsymbol{v}_2\\in \\mathbb{R}^{n-1}\\) 을 정하고 \\(\\boldsymbol{Q}^0_2 = \\boldsymbol{H}_{\\boldsymbol{v}_2}\\in \\mathbb{R}^{(n-1) \\times (n-1)}\\) 을 구성하였다면 당연히 \\(\\boldsymbol{Q}^0_2 \\boldsymbol{x}'_2\\) 는 첫번째 행을 제외한 나머지 행은 \\(0\\) 일 것이다.\n\\[\n\\boldsymbol{Q}_2 = \\begin{bmatrix} 1 & \\boldsymbol{0}^T \\\\ \\boldsymbol{0} & \\boldsymbol{Q}'_2\\end{bmatrix}\n\\]\n라고 정하자. 그렇다면,\n\\[\n\\boldsymbol{Q}_2\\boldsymbol{Q}_1 \\boldsymbol{X} = \\begin{bmatrix} \\|\\boldsymbol{x}\\|_1 & x'_{12} & x'_{13} & \\cdots & x'_{1n} \\\\ 0 & x''_{22} & x''_{23} & \\cdots & 0  \\\\   0 & 0 & x_{33}'' & \\cdots & x''_{22} \\\\ \\vdots  & \\vdots & \\vdots & \\cdots & \\vdots  \\\\ 0 & 0 & x'_{n3}  & \\cdots & x'_{nn}  \\end{bmatrix}\n\\]\n이제 \\(\\boldsymbol{x}_3' = \\begin{bmatrix} x''_{33} & \\cdots & x''_{n3}\\end{bmatrix}^T \\in \\mathcal{M}_{n-2}\\) 부터 계속 같은 과정을 반복한다면,\n\\[\n\\boldsymbol{Q}_n \\cdots \\boldsymbol{Q}_1 \\boldsymbol{X} = \\boldsymbol{R}\n\\]\n이 된다. \\(\\boldsymbol{Q}_1,\\ldots,\\,\\boldsymbol{Q}_n\\) 은 직교행렬이므로 \\(\\boldsymbol{Q} = (\\boldsymbol{Q}_1 \\cdots \\boldsymbol{Q}_n)^T\\) 도 직교행렬이고 따라서\n\\[\n\\boldsymbol{X} = \\boldsymbol{Q} \\boldsymbol{R}\n\\]\n이 성립한다.\nfunction house(x::Vector{T}) where T&lt;:Real\n    m = length(x)\n    σ = norm(x[2:end])^2\n    v= [1 ; x[2:end]]\n    if σ ≈ zero(σ) && x[1] ≥ 0\n        β = 0\n    elseif σ ≈ 0\n        β = -2\n    else \n        μ = sqrt(x[1]^2+σ)\n        if x[1] ≤ 0\n            v[1] = x[1] - μ\n        else \n            v[1] = -σ/(x[1]+μ)\n        end\n        β = 2*v[1]^2 / (σ + v[1]^2)\n        v=v./(v[1])\n    end\n    return β, v\nend\n\nfunction householder(x::Vector{T}) where T&lt;:Real\n    β, v = house(x)\n    return I - β*v*v'\nend\n\nfunction qrdecom(mat::Matrix{T}) where T&lt;:Real\n    \n    m, n = size(mat)[1:2]\n    @assert m == n\n    \n    if T&lt;:Integer \n        A = Float64.(mat[:, :])\n    else \n        A = mat[:, :]\n    end\n\n    elt = eltype(A)\n    Q = Matrix{elt}(I, n, n)\n    for i in 1:(n)\n        Hm = householder(A[i:end, i])\n        if i == 1 \n            Q=Hm\n            A = Q*A\n        else\n            X= [Matrix{elt}(I, i-1, i-1) zeros(elt, i-1, n-i+1); zeros(elt, i-1, n-i+1)'  Hm] \n            Q = X* Q\n            A = X*A\n        end\n        \n    end\n    return Q', A\nend\n\n\n\n\n복소벡터공간에서의 QR 분해",
    "crumbs": [
      "수치해석 II",
      "행렬의 분해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/08_matrix_decomposition.html#linearalgebra.jl",
    "href": "src/numerical_analysis_using_julia/08_matrix_decomposition.html#linearalgebra.jl",
    "title": "행렬의 분해",
    "section": "4 LinearAlgebra.jl",
    "text": "4 LinearAlgebra.jl\nJulia 공식 메뉴얼의 Matrix Factorization 에는 LinearAlgebra.jl 에서 지원하는 행렬 분해에 대해 소개하고 있다.\n\n\n\n분해 종류\n함수\n\n\n\n\n숄레스키 분해\ncholesky()\n\n\n\\(LDL^T\\) 분해\nldlt()\n\n\nLU 분해\nlu()\n\n\nQR 분해\nqr()\n\n\n특이값 분해(SVD)\nsvd()\n\n\n\n그 외에도 행렬의 분해와 관련된 사항을 자세하게 적어 놓았으니 행렬의 분해에 관련된 일을 해야한다면 공식 문서를 숙독해야 한다.",
    "crumbs": [
      "수치해석 II",
      "행렬의 분해"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/10_least_square_problem.html",
    "href": "src/numerical_analysis_using_julia/10_least_square_problem.html",
    "title": "최소자승 문제",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 II",
      "최소자승 문제"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/10_least_square_problem.html#하우스홀더-행렬과-qr-분해",
    "href": "src/numerical_analysis_using_julia/10_least_square_problem.html#하우스홀더-행렬과-qr-분해",
    "title": "최소자승 문제",
    "section": "1 하우스홀더 행렬과 QR 분해",
    "text": "1 하우스홀더 행렬과 QR 분해\n\\(\\boldsymbol{v}\\in \\mathbb{C}^{n}\\) 에 대해 다음과 같이 정의된 \\(\\boldsymbol{H}_v\\) 를 하우스홀더 행렬 (Householder matrix) 이라 한다.\n\\[\n\\begin{aligned}\n\\boldsymbol{H}_{\\boldsymbol{v}} := I_n- \\dfrac{2\\boldsymbol{v}\\boldsymbol{v}^{\\ast}}{\\|\\boldsymbol{v}\\|^2}, \\qquad \\text{i. e. }\\quad\n(\\boldsymbol{H}_{\\boldsymbol{v}})_{ij} := \\delta_{ij} - \\dfrac{2 v_i \\overline{v_j}}{\\|\\boldsymbol{v}\\|^2}.\n\\end{aligned}\n\\]\n여기서 \\(\\boldsymbol{v}\\boldsymbol{v}^{\\ast}\\) 는 벡터의 내적이 아니라 \\(n\\times 1\\) 행렬 \\(\\boldsymbol{v}\\) 와 \\(1 \\times n\\) 행렬 \\(\\boldsymbol{v}^{\\ast}\\) 가 곱해진 \\(n \\times n\\) 행렬을 의미한다. 벡터 \\(\\boldsymbol{x}\\in \\mathbb{C}^n\\) 에 대해 \\(\\boldsymbol{H}_{\\boldsymbol{v}} \\boldsymbol{x}\\in \\mathbb{C}^n\\) 을 하우스홀더 변환 이라 한다. \\(\\boldsymbol{v}\\) 가 단위벡터일 경우, 즉 \\(\\|\\boldsymbol{v}\\|=1\\) 라면 좀 더 간단하게 쓸 수 있다.\n\\[\n\\boldsymbol{H}_{\\boldsymbol{v}} = I_n- 2\\boldsymbol{v}\\boldsymbol{v}^{\\ast},\\qquad \\text{where } \\|\\boldsymbol{v}\\|=1.\n\\]\n\n\n\n명제 1 위와 같이 정의된 하우스홀더 행렬 \\(\\boldsymbol{H}_{\\boldsymbol{v}}\\) 은 다음의 특징을 가진다.\n  (\\(1\\)) 에르미트 행렬이다. 즉 \\(\\boldsymbol{H}_{\\boldsymbol{v}}=  \\boldsymbol{H}_{\\boldsymbol{v}}^\\ast\\).\n  (\\(2\\)) 유니타리 행렬이다. 즉 \\(\\boldsymbol{H}_{\\boldsymbol{v}} (\\boldsymbol{H}_{\\boldsymbol{v}})^\\ast = I\\).\n\n\n\n\n(증명). (\\(1\\)) \\(\\boldsymbol{v}\\) 가 단위벡터일 경우에만 보여도 된다. \\(\\boldsymbol{H} = \\boldsymbol{H}_{\\boldsymbol{v}}\\) 라 하면,\n\\[\n(\\boldsymbol{H}^\\ast)_{ij} = \\overline{H_{ji}}= \\delta_{ij}-2 \\overline{v_j \\overline{v_i}} = \\delta_{ij}-2 v_i \\overline{v_j} = (\\boldsymbol{H})_{ij}\n\\]\n이므로 \\(\\boldsymbol{H}_{\\boldsymbol{v}}\\) 는 에르미트 행렬이다.\n(\\(2\\)) 또한,\n\\[\n\\begin{aligned}\n\\left(\\boldsymbol{H} (\\boldsymbol{H}^\\ast)\\right)_{ij} &= \\left((\\boldsymbol{H})^2\\right)_{ij} = \\sum_{k}(\\delta_{ik}-2 v_i \\overline{v_k})(\\delta_{kj} -2 v_k \\overline{v_j}) \\\\\n&= \\sum_k \\delta_{ik}\\delta_{kj} - 2\\sum_k \\delta_{ik}v_k \\overline{v_j} - 2 \\sum_k \\delta_{kj} v_i \\overline{v_k} + 4 \\sum_{k} v_i \\overline{v_j} v_k \\overline{v_k} \\\\\n&= \\delta_{ij} - 2 v_i \\overline{v_{j}} - 2 v_i \\overline{v_j} + 4 v_i \\overline{v_j} = \\delta_{ij}\n\\end{aligned}\n\\]\n이다. \\(\\square\\)\n\n\n따라서 \\(\\mathbb{R}^n\\) 에서 생각 할 경우 하우스홀더 행렬은 대칭행렬이며, 직교행렬(orthogonal matrix) 이다.\n\n\n하우스홀더 변환과 하우스홀더 반사\n하우스 홀더 변환이 리플렉션(reflection, 반사) 라고 불린다. 수학적으로 \\(\\mathbb{R}^n\\) 공간에서 벡터 \\(\\boldsymbol{v}\\) 만으로 \\(\\boldsymbol{v}\\) 와 수직이며 원점을 지나는 평면이 유일하게 정의될 수 있다. 임의의 벡터 \\(\\boldsymbol{x} \\in \\mathbb{F}^n\\) 에 대해,\n\\[\n\\begin{aligned}\n(\\boldsymbol{H}_{\\boldsymbol{v}}\\boldsymbol{x})_k &=  x_k - \\dfrac{2}{\\|\\boldsymbol{v}\\|^2}  \\sum_{j=1}^n v_k \\overline{v_j} x_j\\\\\n\\boldsymbol{H}_{\\boldsymbol{v}}\\boldsymbol{x} &= \\boldsymbol{x} - 2 \\dfrac{\\langle \\boldsymbol{x},\\,  \\boldsymbol{v} \\rangle \\boldsymbol{v}}{\\|\\boldsymbol{v}\\|^2}\n\\end{aligned}\n\\]\n이며,\n\\[\n\\begin{aligned}\n\\dfrac{1}{2 }(\\boldsymbol{x}+\\boldsymbol{Hx}) &= \\boldsymbol{x} - \\langle \\boldsymbol{v,\\,  x}\\rangle \\boldsymbol{v} = \\boldsymbol{x} - \\text{Proj}_{\\boldsymbol{v}} \\boldsymbol{x} \\\\\n\\boldsymbol{x}-\\boldsymbol{Hx} &= 2\\langle \\boldsymbol{v ,\\, x} \\rangle  \\boldsymbol{v} = 2\\, \\text{Proj}_{\\boldsymbol{v}}\\boldsymbol{x}\n\\end{aligned}\n\\]\n이다. 즉 \\(\\boldsymbol{x}\\) 와 \\(\\boldsymbol{Hx}\\) 는 \\(\\boldsymbol{v}\\) 에 의해 정의되는 평면에 대해 대칭이다.\n\n\n\n\n\n\n그림 1: Householder 반사\n\n\n\n\\(\\boldsymbol{x}\\) 를 \\(\\boldsymbol{v}\\) 와 평행한 부분과 수직한 부분으로 분리하자. 즉 \\(\\boldsymbol{x}_{\\|} = \\text{Proj}_{\\boldsymbol{v}}\\boldsymbol{x}\\), \\(\\boldsymbol{x}_{\\perp} = \\boldsymbol{x}-\\text{Proj}_{\\boldsymbol{v}}\\boldsymbol{x}\\) 라 하면, \\(\\boldsymbol{x} = \\boldsymbol{x}_{\\|} + \\boldsymbol{x}_{\\perp}\\) 이며 \\(\\boldsymbol{x}\\cdot \\boldsymbol{v} = \\boldsymbol{x}_{\\|}\\cdot \\boldsymbol{v}\\) 이다.\n\\[\n\\boldsymbol{H}_{\\boldsymbol{v}}\\boldsymbol{x} = \\boldsymbol{x} - 2 \\boldsymbol{x}_{\\|} = \\boldsymbol{x}_{\\perp} - \\boldsymbol{x}_{\\|}\n\\]\n이다.\n\n\n\n하우스홀더 변환과 QR 분해\n\n명제 2 \\(\\mathbb{F}^m\\) 에서의 벡터 \\(\\boldsymbol{x}\\) 과 표준 기저 \\(\\{\\boldsymbol{e}_1,\\ldots,\\,\\boldsymbol{e}_m\\}\\) 를 생각하자. \\(\\alpha\\) 를 \\(\\boldsymbol{x}\\) 의 첫번째 성분 \\(x_1 = re^{i\\theta}\\) 에 대해 다음과 같이 정의한다.\n\\[\n\\alpha = \\left\\{ \\begin{array}{ll} \\|\\boldsymbol{x}\\|_2  & \\text{where } \\mathbb{F}=\\mathbb{R}, \\\\ e^{i\\theta}\\|\\boldsymbol{x}\\|_2 \\qquad & \\text{where } \\mathbb{F} =\\mathbb{C}. \\end{array} \\right.\n\\]\n\\(\\boldsymbol{x}\\) 에 대해 \\(\\boldsymbol{v}\\) 를 다음과 같이 정하자.\n\\[\n\\boldsymbol{v} = \\boldsymbol{x} - \\alpha \\boldsymbol{e}_1, \\\\\n\\]\n이 때 \\(\\boldsymbol{x}\\) 의 \\(\\boldsymbol{v}\\) 에 대한 하우스홀더 변환 \\(\\boldsymbol{H}_{\\boldsymbol{v}}\\boldsymbol{x}\\) 는 다음과 같다.\n\\[\n\\boldsymbol{H}_{\\boldsymbol{v}}\\boldsymbol{x} = \\boldsymbol{x}-\\boldsymbol{v} = \\alpha \\boldsymbol{e}_1.\n\\]\n\n\n(증명). \\(\\alpha \\overline{x}_1 = \\overline{\\alpha}x_1 = |x_1| \\|\\boldsymbol{x}\\|_2\\) 이며 \\(|\\alpha|^2 = \\|\\boldsymbol{x}\\|_2^2\\) 이다. \\(a=\\alpha\\overline{x}_1=\\overline{\\alpha}x_1\\) 이라 놓고 다음을 계산해 보자. \\[\n\\begin{aligned}\n\\langle \\boldsymbol{x},\\,\\boldsymbol{v}\\rangle & = \\langle \\, \\boldsymbol{x} , \\boldsymbol{x}-\\alpha \\boldsymbol{e}_1\\rangle = \\|\\boldsymbol{x}\\|_2^2-\\overline{\\alpha} x_1  = \\|\\boldsymbol{x}\\|_2^2 - a\\\\\n\\langle \\boldsymbol{v},\\,\\boldsymbol{v}\\rangle &= \\langle \\boldsymbol{x} -\\alpha \\boldsymbol{e}_1,\\, \\boldsymbol{x} -\\alpha\\boldsymbol{e}_1 \\rangle  = \\|\\boldsymbol{x}\\|^2 - \\overline{\\alpha} x_1 -\\alpha \\overline{x_1} + |\\alpha|^2 = 2(\\|\\boldsymbol{x}\\|_2^2 -a) \\\\\n&= 2 \\langle \\boldsymbol{x},\\, \\boldsymbol{v} \\rangle\n\\end{aligned}\n\\]\n이므로, (잠시 \\(\\|\\boldsymbol{x}\\|_2\\) 를 \\(\\|\\boldsymbol{x}\\|\\) 라 하자.)\n\\[\n\\boldsymbol{H}_\\boldsymbol{v} \\boldsymbol{x} = \\boldsymbol{x} - 2\\dfrac{\\langle \\boldsymbol{x},\\, \\boldsymbol{v}\\rangle}{\\|\\boldsymbol{v}\\|^2} \\boldsymbol{v} = \\boldsymbol{x}-\\boldsymbol{v} = \\alpha \\boldsymbol{e}_1\n\\]\n이다. \\(\\square\\)\n\n\n이미 설명한 그람 슈미트 방법과 유사하게\n이제 \\(\\boldsymbol{Q}_1= \\boldsymbol{Q},\\, \\boldsymbol{A}=\\boldsymbol{A}_1\\) 이라 놓으면,\n\\[\n\\boldsymbol{Q}_1\\boldsymbol{A}_1 = \\begin{bmatrix}\\alpha _{1} & \\ast &\\cdots &\\ast \\\\0 & & &\\\\ \\vdots & & \\boldsymbol{A}_2 & \\\\ 0 & & & \\end{bmatrix}\n\\]\n꼴이 된다. 이제 행렬 \\(m \\times n\\) 행렬 \\(\\boldsymbol{A}_k\\) 가 \\(k\\) 번째 행까지는 상삼각 행렬의 모양을 따른다고 하자. 즉 \\(j\\le k\\) 이고 \\(i&gt;k\\) 이면 \\((\\boldsymbol{A}_k)_{ij}=0\\) 이라 하자. 이 때 \\(\\boldsymbol{A}_k\\) 의 \\(k\\) 번째 행부터 \\(m\\) 행, \\(k\\) 번째 열부터 \\(m\\) 열까지를 \\(\\boldsymbol{A}'_k\\) 라 하고,(julia 로 표현하면 Ak[k:end, k:end] 가 될 것이다) 이 \\(\\boldsymbol{A}'_k\\) 에 대해 앞서 \\(\\boldsymbol{A}\\) 에 했던 것과 똑같은 과정을 수행하는 하우스홀더 행렬을 \\(\\boldsymbol{Q}'_k\\) 라 하면 \\(\\boldsymbol{Q}'_k \\boldsymbol{A}'_k\\) 는 \\(\\boldsymbol{Q}_1\\boldsymbol{A}_1\\) 처럼 첫번째 열에서는 첫번째 행을 제외한 나며지 행의 값이 \\(0\\) 이 된다. 만약\n\\[\n\\boldsymbol{Q}_k = \\begin{bmatrix} \\boldsymbol{I}_{k-1} & 0 \\\\ 0 & \\boldsymbol{Q}_k'\\end{bmatrix}\n\\]\n이라 하면, \\((k-1)\\times (k-1)\\) 단위행렬 \\(I_{k-1}\\) 과 \\(\\boldsymbol{A}_k\\) 를 \\(k\\) 행 과 \\(k\\) 열 부터 잘라 \\(\\boldsymbol{A}_{k} = \\begin{bmatrix} B_{k} & C_{k} \\\\ 0 &\\boldsymbol{A}'_{k}\\end{bmatrix}\\) 로 만들자. \\(B_k\\) 는 \\((k-1) \\times (k-1)\\) 행렬이며 \\(\\boldsymbol{A}'_{k}\\) 는 \\((m-k+1)\\times (n-k+1)\\) 행렬이다. \\(\\boldsymbol{A}_k\\) 가 \\(k\\) 열까지 상삼각 행렬 모양이므로 \\(B_k\\) 아래는 \\(0\\) 행렬이다. 두 행렬의 곱은 \\[\n\\boldsymbol{Q}_k \\boldsymbol{A}_k = \\begin{bmatrix} \\boldsymbol{I}_{k-1} & 0 \\\\ 0 & \\boldsymbol{Q}_k' \\end{bmatrix} \\begin{bmatrix} B_{k} & C_{k} \\\\ 0 &\\boldsymbol{A}'_{k}\\end{bmatrix} = \\begin{bmatrix} B_k & C_k \\\\ 0 & \\boldsymbol{Q}'_k \\boldsymbol{A'}_k \\end{bmatrix}\n\\]\n이 되고 \\(\\boldsymbol{Q}'_k \\boldsymbol{A}'_k\\) 의 첫번째 열은 첫번째 행을 제외하면 모두 \\(0\\) 이므로 \\(\\boldsymbol{Q}_k \\boldsymbol{A}_k\\) 는 \\(k\\) 열까지 상삼각 행렬 꼴이 된다.\n\\(L = \\min\\{m,\\,n\\}\\) 이라 하면 \\(\\boldsymbol{Q}_L \\boldsymbol{Q}_{L-1} \\cdots \\boldsymbol{Q}_1 \\boldsymbol{A}\\) 는 상삼각행렬꼴이 된다. 이를 \\(\\boldsymbol{R}\\) 이라 하자. \\(\\boldsymbol{Q}'_k\\) 가 하우스홀더 행렬이므로\n\\[\n\\boldsymbol{Q}_k \\boldsymbol{Q}_k^\\ast = \\begin{bmatrix} \\boldsymbol{I}_{k-1} & 0 \\\\ 0 & \\boldsymbol{Q}_k'\\end{bmatrix} \\begin{bmatrix} \\boldsymbol{I}_{k-1} & 0 \\\\ 0 & (\\boldsymbol{Q}_k')^\\ast\\end{bmatrix} = \\begin{bmatrix} I_{k-1} & 0 \\\\0 & \\boldsymbol{Q}_k'\n(\\boldsymbol{Q}_k')^\\ast\\end{bmatrix} = I\n\\]\n이다. 즉 \\(\\boldsymbol{Q}_k\\) 도 직교행렬이다. \\(\\boldsymbol{Q}_k\\) 가 에르미트 행렬임은 쉽게 보일 수 있다. 이제,\n\\[\n\\boldsymbol{Q}_L \\cdots \\boldsymbol{Q}_1 \\boldsymbol{A} = \\boldsymbol{R} \\implies \\boldsymbol{A} = \\boldsymbol{Q}_1^\\ast \\cdots \\boldsymbol{Q}_L^\\ast \\boldsymbol{R}\n\\]\n임은 쉽게 보일 수 있다. 직교행렬의 곱은 직교행렬이므로 \\(\\boldsymbol{Q}_1^\\ast \\cdots \\boldsymbol{Q}_L^\\ast\\) 도 직교행렬이다. 따라서 QR 분해를 할 수 있다.\n\n\nQR 분해를 이용한 최소자승법\n\n부분공간에 대한 정사영\n\\(W\\) 가 내적벡터공간 \\(V\\) 의 부분공간이라 하자. 이 때 그람슈미트 과정을 통해 \\(W\\) 의 정규기저벡터 \\(\\{\\hat{\\boldsymbol{u}}_1,\\ldots,\\,\\hat{\\boldsymbol{u}}_m\\}\\) 을 구할 수 있다. 이 때 \\(\\boldsymbol{v} \\in V\\) 의 \\(W\\) 에 대한 정사영 \\(\\text{Proj}_W \\boldsymbol{v}\\) 는 다음과 같이 정의된다.\n\\[\n\\text{Proj}_W \\boldsymbol{v} = \\sum_{i=1}^m \\langle \\boldsymbol{v},\\,\\hat{\\boldsymbol{u}}_i \\rangle\\,\\hat{\\boldsymbol{u}}_i\n\\]\n\\(\\text{Proj}_W \\boldsymbol{v}\\) 는 \\(W\\) 에 속한 벡터이며, 벡터에 대한 정사영과 마찬가지로 임의의 \\(\\boldsymbol{w} \\in W\\) 에 대해\n\\[\n\\langle \\boldsymbol{w},\\,  \\boldsymbol{v}-\\text{Proj}_W \\boldsymbol{v}\\rangle = \\boldsymbol{0}\n\\]\n이다. \\(\\boldsymbol{v} = \\text{Proj}_W \\boldsymbol{v} + (\\boldsymbol{v} - \\text{Proj}_W\\boldsymbol{v} )\\) 이므로 벡터 \\(\\boldsymbol{v}\\) 를 \\(W\\) 에 속하는 벡터와, \\(W\\) 에 직교하는 벡터로 분해 할 수 있다.\n\n내적벡터공간 \\(V\\) 의 두 벡터 \\(\\boldsymbol{v}_1,\\,\\boldsymbol{v}_2\\) 의 거리는 \\(\\|\\boldsymbol{v}_1-\\boldsymbol{v}_2\\| = \\sqrt{\\langle \\boldsymbol{v}_1-\\boldsymbol{v}_2,\\,\\boldsymbol{v}_1-\\boldsymbol{v}_2 \\rangle}\\) 로 정의된다. \\(V\\) 와 그 부분공간 \\(W\\) 를 생각하자. \\(V\\) 의 벡터 \\(\\boldsymbol{v}\\) 와 \\(W\\) 사이의 거리 \\(d(\\boldsymbol{v},\\,W)\\) 는 \\(W\\) 에 속한 벡터 가운데 \\(\\boldsymbol{v}\\) 와의 거리가 가장 작은 벡터 \\(\\boldsymbol{v}_0\\) 와의 거리로 정의된다. 즉,\n\\[\nd(\\boldsymbol{v},\\,W) = \\min_{\\boldsymbol{w}\\in W} \\| \\boldsymbol{v} - \\boldsymbol{w}\\|\n\\]\n이다. 여기서 \\(d(\\boldsymbol{v},\\,W) = d(\\boldsymbol{v},\\, \\text{Proj}_W \\boldsymbol{v})\\) 임을 보이고자 한다. \\(W\\) 의 정규기저벡터 \\(\\{\\hat{\\boldsymbol{u}}_1,\\ldots,\\,\\hat{\\boldsymbol{u}}_n\\}\\) 을 생각하자. 그리고 \\(V\\) 의 정규기저 벡터는 앞의 \\(W\\) 의 정규기저벡터를 확장하여 \\(\\{\\hat{\\boldsymbol{u}}_1,\\ldots,\\,\\hat{\\boldsymbol{u}}_n,\\,\\hat{\\boldsymbol{f}}_1,\\ldots,\\,\\hat{\\boldsymbol{f}}_m\\}\\) 라 하자. 그렇다면 \\(\\langle \\hat{\\boldsymbol{u}}_i,\\, \\hat{\\boldsymbol{f}}_j \\rangle = 0\\), \\(\\langle \\hat{\\boldsymbol{f}}_i ,\\,\\hat{\\boldsymbol{f}}_j \\rangle = \\delta_{ij}\\) 이다. 이제 이 정규기저벡터로 \\(\\boldsymbol{v}\\in V\\) 를 표현하면,\n\\[\n\\boldsymbol{v} = a_1 \\hat{\\boldsymbol{u}}_1 + \\cdots + a_n \\hat{\\boldsymbol{u}}_n + b_1 \\hat{\\boldsymbol{f}}_1 + \\cdots + b_m \\hat{\\boldsymbol{f}}_m\n\\]\n이다. 임의의 \\(\\boldsymbol{w} = c_1 \\hat{\\boldsymbol{u}}_1 + \\cdots + c_n \\hat{\\boldsymbol{u}}_n \\in W\\) 와 \\(\\boldsymbol{v}\\) 와의 거리의 제곱은\n\\[\n\\begin{aligned}\n\\left(d(\\boldsymbol{v},\\,\\boldsymbol{w}) \\right)^2&= (a_1-c_1)^2 + \\cdots + (a_n - c_n)^2 + b_1^2 + \\cdots + b_m^2 \\ge b_1^2 + \\cdots +b_m ^2\n\\end{aligned}\n\\]\n이므로 \\(a_1=c_1, \\cdots , a_n = c_n\\) 일 때 \\(d(\\boldsymbol{v},\\,W)\\) 가 최소값이 됨을 알 수 있다. 즉,\n\\[\nd(\\boldsymbol{v},\\,W) = \\|\\boldsymbol{v}-\\text{Proj}_W \\boldsymbol{v}\\|\n\\]\n이다.\n선형 시스템에서 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 만족시키는 해가 없지만 \\(\\|\\boldsymbol{Ax}-\\boldsymbol{b}\\|\\) 를 최소화 하는 \\(\\boldsymbol{x}\\) 를 구하고자 할 경우를 생각하자. \\(\\boldsymbol{A}\\) 가 \\(m\\times n\\) 행렬이고 \\(\\boldsymbol{x}\\in \\mathbb{F}^n,\\,\\boldsymbol{b}\\in \\mathbb{F}^m\\) 이라 하자. \\(\\boldsymbol{A}\\) 의 \\(i\\) 번째 행을 \\(\\boldsymbol{a}_i\\) 라 하면 \\(\\boldsymbol{a}_i \\in \\mathbb{F}^m\\) 이며,\n\\[\n\\boldsymbol{A} = \\begin{bmatrix} \\boldsymbol{a}_1 & \\boldsymbol{a}_2 & \\cdots & \\boldsymbol{a}_n\\end{bmatrix}\n\\]\n이다. \\(\\boldsymbol{x} = \\begin{bmatrix}x_1 & x_2 & \\ldots & x_n\\end{bmatrix}^T\\) 이라 하면, \\(\\boldsymbol{Ax}= x_1 \\boldsymbol{a}_1 + \\cdots x_n \\boldsymbol{a}_n\\) 이다. 즉 \\(\\boldsymbol{Ax}\\) 는 \\(\\boldsymbol{a}_1,\\ldots,\\boldsymbol{a}_n\\) 에 의해 정해지는 \\(\\mathbb{F}^n\\) 의 부분공간의 한 벡터이며, \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 는 이 부분공간의 벡터 가운데 \\(\\boldsymbol{b}\\) 와 같도록 하는 계수 \\(x_1,\\ldots,\\,x_n\\) 을 찾는 것이라고 이해 할 수 있다.\n\n\\(m=n,\\, \\text{rank}(\\boldsymbol{A})=n\\) 이면 \\(\\boldsymbol{A}\\) 가 가역행렬이므로 \\(\\boldsymbol{x}\\) 의 정확한 해가 반드시 존재한다. QR 분해의 경우 \\(\\boldsymbol{R}\\) 은 대각성분이 양수인 \\(n \\times n\\) 상삼각행렬이 된다.\n\\(m&gt;n\\) 이라면 \\(\\text{rank}(\\boldsymbol{A}) \\le n&lt; m\\) 이므로 \\(\\boldsymbol{Ax}=\\boldsymbol{b}\\) 를 만족하는 해가 존재하지 않을 수 있다.\n\\(m&lt;n\\) 이라면\n\n– to be continued",
    "crumbs": [
      "수치해석 II",
      "최소자승 문제"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/12_ode_initial_value_problem_2.html",
    "href": "src/numerical_analysis_using_julia/12_ode_initial_value_problem_2.html",
    "title": "상미분 방정식의 초기값 문제-II",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]\n지금까지 상미분 방정식의 초기값 문제에 대한 기본적인 지식을 쌓았다. 이제 좀 더 정교한 미분방정식의 수치해석적 풀이법을 알아보고자 한다.",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-II"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/12_ode_initial_value_problem_2.html#runge-kutta-fehlberg-방법rkf",
    "href": "src/numerical_analysis_using_julia/12_ode_initial_value_problem_2.html#runge-kutta-fehlberg-방법rkf",
    "title": "상미분 방정식의 초기값 문제-II",
    "section": "1 Runge-Kutta-Fehlberg 방법(RKF)",
    "text": "1 Runge-Kutta-Fehlberg 방법(RKF)\n미분방정식의 초기값 문제\n\\[\nx'(t) = f(t, x),\\qquad x(t_0) = x_0\n\\]\n를 생각하자. 지금까지의 오일러 방법이나 RK2, RK4 에서는 정해진 간격 \\(h&gt;0\\) 에 대해 \\(t_k = t_0 + kh\\) 를 이용하였다. 이 경우\n\\[\nx_{k+1} = x_k + \\phi (t_k,\\,x_k) + O(h^{n+1})\n\\]\n의 꼴로 \\(x(t)\\) 의 경로를 얻었다. 이 때 오차는 방법에 따라 \\(n\\) 값이 정해지는 \\(ch^{n+1}\\) 이었다.\n그러나 이제는 간격 \\(h\\) 를 조절하고자 한다. 이렇게 수치해석에서 문제에 주어진 변수가 아닌 문제 해결에 사용되는 값(여기서는 \\(h\\)) 를 문제 해결 도중에 유연하게 변경하는 방법을 적응형(adaptive) 라고 한다. Runge-Kutta-Fehlberg 방법이 바로 적응형 수치해석 방법중 하나이다. 즉 지금까지는\n\\[\n\\begin{aligned}\nt_{k+1} &= t_k + h, \\\\\nx_{k+1} &= x_k + \\phi (t_k,\\, x_k)\n\\end{aligned}\n\\]\n로 문제를 풀어왔다면 이제는\n\\[\n\\begin{aligned}\nt_{k+1} & = t_k + h_k,\\\\\nx_{k+1} & = x_k + \\phi (t_k,\\, x_k,\\,h_k)\n\\end{aligned}\n\\]\n로 문제를 풀게 될 것이다. \\(h_k\\) 를 어떻게 정하는 지가 중요한 전환점이 될 것이다.\n이제 \\(t_k,\\,x_k\\) 와 정해진 \\(h\\) 에 따라 \\(x_{k+1}\\) 을 정하는 함수에 \\(\\phi\\) 와 \\(\\tilde{\\phi}\\) 의 두가지 방법이 있다고 하자. \\(\\phi\\) 의 trunction error 는 \\(O(h^{n+1})\\) 이며 \\(\\tilde{\\phi}\\) 의 truncation error 는 \\(O(h^{n+2})\\) 라고 하자. 각각의 방법을 구분하기 위해,\n\\[\n\\begin{aligned}\nx_{k+1} & = x_k + \\phi (t_k,\\,x_k,\\, h),\\\\\n\\tilde{x}_{k+1} &= \\tilde{x}_k + \\tilde{\\phi}(t_k,\\,\\tilde{x}_k,\\,h)\n\\end{aligned}\n\\]\n라고 하자. 이제 수치해석적인 방법으로 계산한 \\(x_{k}\\) 와 실제 해 \\(x(t_k)\\) 를 구분하기로 하자. \\(x_{k} \\approx \\tilde{x}_k \\approx x(t_k)\\) 라고 가정 할 때 \\(t_{k+1}\\) 에서의 실제 경로 \\(x(t_{k+1})\\) 과의 truncation error \\(\\tau_{k+1}(h)\\) 는\n\\[\n\\tau_{k+1}= \\vartau\n\\]",
    "crumbs": [
      "수치해석 II",
      "상미분 방정식의 초기값 문제-II"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/14_approximation.html",
    "href": "src/numerical_analysis_using_julia/14_approximation.html",
    "title": "Approximation",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 II",
      "Approximation"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/14_approximation.html#최소-자승법",
    "href": "src/numerical_analysis_using_julia/14_approximation.html#최소-자승법",
    "title": "Approximation",
    "section": "1 최소 자승법",
    "text": "1 최소 자승법\n\n1.1 선형 최소 자승법\n아래 그림과 같이 \\(\\{(x_i,\\,y_i) : i = 1,\\ldots, N\\}\\) 가 주어졌을 때 이 데이터에 가장 근접하는 선형 방정식 \\(y=ax+b\\) 를 찾아야 할 때가 있다.\n\n\n\n\n\n\n그림 1: 선형 최소 자승법\n\n\n\n아래와 같은 함수 \\(E(a,\\,b)\\) 를 보자. \\[\nE(a, b) = \\sum_{i=1}^N \\left[ y_i - (ax_i + b)\\right]^2\n\\]\n이 함수는 모든 구간에서 미분가능하다는 매우 좋은 성질을 가지고 있다. 또한 \\(a,\\,b\\) 값이 상당히 벗어날 경우 매우 커지며, \\(y_i\\) 가 \\(ax_i +b\\) 와 일치할 경우 \\(0\\) 이 된다.\n\\[\n\\begin{aligned}\n\\dfrac{\\partial E}{\\partial a} &= -2\\sum_{i=1}^N x_i(y_i - ax_i - b), \\\\\n\\dfrac{\\partial E}{\\partial b} &= -2\\sum_{i=1}^N (y_i - ax_i -b), \\\\\n\\dfrac{\\partial^2 E}{\\partial a \\partial a }& = \\sum_{i=1}^N 2{x_i}^2, \\\\\n\\dfrac{\\partial^2 E}{\\partial a \\partial b} &= \\sum_{i=1}^N 2x_i, \\\\\n\\dfrac{\\partial^2 E}{\\partial b \\partial b}&= 2N\n\\end{aligned}\n\\]\n이며 이로부터 해세 행렬 (Hassian matrix)\n\\[\n\\boldsymbol{H} = \\begin{bmatrix} \\dfrac{\\partial^2 E}{\\partial a^2 } & \\dfrac{\\partial^2 E}{\\partial a \\partial b} \\\\ \\dfrac{\\partial^2 E}{\\partial a \\partial b} & \\dfrac{\\partial^2 E}{\\partial b^2} \\end{bmatrix} = \\begin{bmatrix} {\\displaystyle \\sum_{i=1}^N 2{x_i}^2} & {\\displaystyle \\sum_{i=1}^N 2x_i}, \\\\ {\\displaystyle \\sum_{i=1}^N 2x_i} & 2N \\end{bmatrix}\n\\]\n을 얻는다. 위 해세 행렬의 행렬식은\n\\[\nD = \\det (\\boldsymbol{H}) = 4 \\left[N\\left(\\sum_i^N {x_i}^2 \\right)- \\left(\\sum_i^N x_i\\right)^2\\right]\n\\]\n이며, 코시-슈바르츠 부등식으로 \\(D\\ge 0\\) 이며 \\(D=0\\) 일 때는 \\(x_1 = \\cdots = x_i\\) 일 때임을 보일 수 있다. 이것을 제외하면 \\(D&gt;0\\) 이며 \\(E(a,\\,b)\\) 는 극소값을 가진다는 것을 안다. 극소값을 가질 때는 \\(\\dfrac{\\partial E}{\\partial a}= \\dfrac{\\partial E}{\\partial b}=0\\) 일 때이므로,\n\\[\n\\begin{aligned}\n\\dfrac{\\partial E}{\\partial a}= 0 &\\implies \\sum_i x_i y_i - a \\left(\\sum_i {x_i}^2\\right) - b \\left(\\sum_{i}x_i\\right) = 0, \\\\\n\\dfrac{\\partial E}{\\partial b} = 0 &\\implies \\sum_i y_i - a\\left(\\sum_i x_i\\right) - b N = 0\n\\end{aligned}\n\\]\n이다. 위 식은 \\(a,\\,b\\) 에 대한 연립방정식이며 이것을 풀면 다음과 같다.\n\\[\na = \\dfrac{N \\left( \\displaystyle \\sum_{i=1}^N x_i y_i\\right) - \\left(\\displaystyle \\sum_{i=1}^N x_i\\right) \\left( \\displaystyle \\sum_{i=1}^N y_i\\right)}{N \\left(\\displaystyle \\sum_{i=1}^N {x_i}^2\\right) - \\left(\\displaystyle \\sum_{i=1}^N x_i\\right)^2}, \\qquad\nb=  \\dfrac{\\left( \\displaystyle \\sum_{i=1}^N {x_i}^2\\right)  \\left( \\displaystyle \\sum_{i=1}^N y_i\\right) - \\left(\\displaystyle \\sum_{i=1}^N x_iy_i\\right) \\left( \\displaystyle \\sum_{i=1}^N x_i\\right)}{N \\left(\\displaystyle \\sum_{i=1}^N {x_i}^2\\right) - \\left(\\displaystyle \\sum_{i=1}^N x_i\\right)^2},\n\\]\n언뜻 복잡해 보이지만 프로그래밍 입장에서 보면 베열의 합, 베열의 곱의 합, 베열의 제곱의 합, 배열의 합의 제곱에 대한 사칙연산 뿐이며, 쉽게 코딩 할 수 있다.\n\n\n\n1.2 지수함수와 멱함수의 최소자승법\n\\(y=be^{ax}\\) 꼴의 지수함수의 경우 각각 \\(\\ln y = ax + \\ln b\\) 로 변형시킨 후 \\(\\overline{y}=\\ln y,\\, \\overline{b} = \\ln b\\) 로 놓으면\n\\[\n\\overline{y} = ax + \\overline{b}\n\\]\n의 선형방정식이 된다. 따라서 선형방정식의 최소자승법으로 \\(a\\) 와 \\(\\overline{b}\\) 를 구할 수 있기 때문에 지수함수에 대해서는 최소자승법을 사용 할 수 있다. \\(y=dx^c\\) 꼴의 멱함수의 경우에는 \\(\\ln y = \\ln d + c\\ln x\\) 이므로 \\(\\overline{y} = \\ln y\\), \\(\\overline{x} = \\ln x\\), \\(\\overline{d} = \\ln d\\) 로 놓으면\n\\[\n\\overline{y} = c \\overline{x} + \\overline{d}\n\\]\n를 이용해 역시 최소자승법으로 \\(c,\\, d\\) 를 구할 수 있다.\n\n\n\n1.3 다항식 최소 자승법\n데이터 \\(\\{(x_k,\\,y_k):k=1,\\,N\\}\\) (\\(x_1&lt;x_2&lt;\\cdots &lt; x_N\\)) 과 \\(n\\) 차 다항식\n\\[\np_n (x) = a_n x^n + a_{n-1}x^{n-1} + \\cdots + a_1 x + a_0\n\\]\n에 대해\n\\[\nE(a_n,\\ldots,\\,a_0) = \\sum_{k=1}^N \\left[ y_k - p_n(x_k)\\right]^2\n\\]\n를 최소로 하는 \\(a_0,\\ldots,\\,a_n\\) 을 찾는다. 이 경우 \\(N=n+1\\) 이면 다항식을 이용한 보간법이 되어 \\(E=0\\) 이 되도록 하는 \\(a_0,\\ldots,\\,a_n\\) 이 존재한다는 것을 안다. 보통 최소자승법은 \\(N&gt;n+1\\) 일 경우의 가장 좋은 다항식 \\(p_n\\) 을 찾는 것을 말한다. 여기서도 \\(n&lt;N-1\\) 임을 가정하겠다.\n\\[\n\\begin{aligned}\nE(a_n,\\ldots,\\,a_0) &= \\sum_{k=1}^N {y_k}^2 -2 \\sum_{k=1}^N y_k p_n(x_i) + \\sum_{k=1}^N (p_n(x_k))^2\\\\\n&= \\sum_{k=1}^N {y_k}^2 - 2 \\sum_{k=1}^N \\sum_{i=0}^n a_i y_k {x_k}^j + \\sum_{k=1}^N \\sum_{i,\\,j=1}^n a_i a_j {x_k}^{i+j} \\\\\n&= \\sum_{k=1}^N {y_k}^2 - 2\\sum_{i=0}^n \\left(\\sum_{k=1}^N (x_k)^i y_k\\right) a_i + \\sum_{i, j=0}^n \\left(\\sum_{k=1}^N {x_k}^{i+j}\\right) a_i a_j\n\\end{aligned}\n\\]\n이며 \\(E\\) 를 최소로 하는 극값은 \\(i=0,\\ldots,n\\) 에 대해\n\\[\n\\dfrac{\\partial E}{\\partial a_i}=0 \\implies \\sum_{k=1}^N \\left[ (x_k)^i y_k- \\sum_{j=0}^n  (x_k)^{i+j} a_j\\right] = 0\n\\tag{1}\\]\n를 만족해야 한다. 이제 행렬 \\(\\boldsymbol{X} \\in \\mathcal{M}_{N\\times (n+1)}(\\mathbb{F}),\\,\\boldsymbol{a} \\in\\mathcal{M}_{n+1}(\\mathbb{F}),\\,\\boldsymbol{y}\\in \\mathcal{M}_N(\\mathbb{F})\\) 를 다음과 같이 정의하자.\n\\[\nX_{ij} = (x_i)^{j-1},\\qquad \\boldsymbol{a} = \\begin{bmatrix} a_0 & \\cdots & a_n\\end{bmatrix}^T, \\qquad \\boldsymbol{y} = \\begin{bmatrix} y_1 & \\cdots & y_N \\end{bmatrix}\n\\]\n그렇다면,\n\\[\n(\\boldsymbol{X}^T\\boldsymbol{X})_{ij} = \\sum_{k=1}^N X_{ki}X_{kj} = \\sum_{k=1}^N x_{k}^{i+j-2}\n\\]\n이므로 식 1 는 다음과 같은 선형방정식이 된다. 아래와 같은 형태의 방정식을 정규 방정식(normal equation) 이라고 한다.\n\\[\n\\boldsymbol{X}^T\\boldsymbol{Xa} = \\boldsymbol{X}^T\\boldsymbol{y}\n\\tag{2}\\]\n\n\n\n연습문제 1 위에서 정의된 \\(\\boldsymbol{X}\\) 에 대해 다음을 보여라.\n  (\\(1\\)) \\(\\text{nullity} (\\boldsymbol{X})=0\\).\n  (\\(2\\)) \\(\\boldsymbol{X}^T\\boldsymbol{X}\\) 는 가역행렬이다.\n\n\n\n\n(해답). (\\(1\\)) \\(\\text{nullity} (\\boldsymbol{x}) &gt; 0\\) 이라면 어떤 nontrivial 한 \\(\\boldsymbol{a}\\) 에 대해 \\(\\boldsymbol{Xa}=\\boldsymbol{0}\\) 이어야 한다. 그렇다면, \\(\\sum_{i=0}^n {x_k}^i a_i = 0\\) (\\(i=1,\\ldots,\\,N\\)) 인데 \\(N&gt;n+1\\) 조건에서 \\(n\\) 차 방정식이 \\(N &gt; n+1\\) 개의 근을 갖는다는 이야기이므로 모순이다. 우리는 이미 \\(x_1 &lt; \\cdots &lt; x_N\\) 을 가정했다. 따라서 \\(\\boldsymbol{Xa}=\\boldsymbol{0}\\) 을 만족하는 \\(\\boldsymbol{a}\\) 는 \\(\\boldsymbol{0}\\) 뿐이다.\n(\\(2\\)) \\(\\boldsymbol{X}^T\\boldsymbol{X}\\) 는 대칭행렬이며 \\(\\text{nullity}(\\boldsymbol{X})=0\\) 이므로 non-trivial 한 벡터 \\(\\boldsymbol{a}\\) 에 대해 \\(\\boldsymbol{a}^T\\boldsymbol{X}^T\\boldsymbol{Xa} = \\|\\boldsymbol{Xa}\\|^2&gt;0\\) 이다. 따라서 \\(\\boldsymbol{X}^T\\boldsymbol{X}\\) 는 가역이다.\n\n\n연습문제 1 에 따라 \\(\\boldsymbol{a} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}\\) 로 정해진다. 이것을 julia 로 구현하면 다음과 같다. x, y 의 베열을 입력하면 다항식의 계수를 order 에 대한 오름차순으로 반환한다. 즉 반환값의 첫번째 성분은 상수이다.\nusing LinearAlgebra\n\nfunction least_square_poly(\n    x::AbstractVector{&lt;:Real}, \n    y::AbstractVector{&lt;:Real}, \n    order::Integer)\n    @assert length(x) == length(y)\n    @assert order ≥ 2 && order &lt; length(x)-1\n    X = [(xi)^i for xi in x, i in 0:order]\n    a = inv(X'*X) *X' * y\n    return a\nend\nNAJ.jl 에서는 같은 이름 least_square_poly 함수가 정의되어 있다. 시그너쳐는 같고 반환값은 계수가 아니라 SimplePolynomial 객체이다.\nx = collect(-3:0.1:5)\nr = polynomial_from_roots([-3, 1, 4])\ny = r.(x) \nscatter(x, y, label = \"Data\")\np = least_square_poly(x, y, 3)\nplot!(x, p.(x), label = \"LeastSquared Polynoimal\")\n\n\n\n\n\n\n그림 2: 다항식을 이용한 최소자승법",
    "crumbs": [
      "수치해석 II",
      "Approximation"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/14_approximation.html#직교-다항식을-이용한-최소자승법",
    "href": "src/numerical_analysis_using_julia/14_approximation.html#직교-다항식을-이용한-최소자승법",
    "title": "Approximation",
    "section": "2 직교 다항식을 이용한 최소자승법",
    "text": "2 직교 다항식을 이용한 최소자승법\n\n2.1 내적벡터공간으로서의 함수의 집합\n\n필드 \\(\\mathbb{F}\\) 에서 정의된 \\(x\\) 를 변수로 갖는 \\(n\\) 차 이하의 다항식의 집합을 \\(\\mathbb{F}_n [x]\\) 라고 하자. 또한 \\(x\\) 를 변수로 하는 다항식 전체의 집합을 \\(\\mathbb{F}[x]\\) 라고 하자. 실수에서 정의된 2차 이하의 다항식의 집합은 \\(\\mathbb{R}_2[x]\\) 로 쓸 수 있다.\n\\([a,\\,b]\\) 구간에서 연속인 함수의 집합 \\(C_{[a,\\,b]}\\) 가 벡터공간이기 때문에 선형대수학에의 중요한 개념인 벡터, 선형독립, 기저와 같은 개념을 그대로 쓸 수 있다.\n\n\n\n\n연습문제 2 \\(p_k(x)\\) 가 각각 \\(k\\) 차 다항식일 때 \\(\\{p_1(x),\\ldots,\\,p_n(x)\\}\\) 은 선형독립이다.\n\n\n\n\n(해답). \\(c_1p_1(x) + \\cdots + c_n p_n(x)=0\\) 이라고 하자. \\(n\\) 차항은 \\(p_n(x)\\) 밖에 없으므로 \\(c_n=0\\) 이다. \\(n-1\\) 차항은 \\(p_{n-1}\\) 박에 없으므로 \\(c_{n-1}=0\\) 이다. 이것을 반복하면 \\(c_1= \\cdots = c_n=0\\) 을 보인다.\n\n\n\\(C_[a,\\,b]\\) 에서 내적을 어떤 정해진 함수 \\(w(x)\\) 에 대해 다음과 같이 정의 할 수 있다.\n\\[\n\\langle \\varphi,\\, \\psi \\rangle := \\int_{a}^b \\varphi(x)\\, \\psi(x)\\, w(x)\\, dx\n\\]\n이 때 \\(w(x)\\) 를 무게 함수(weight function) 이라고 하며 (1) 모든 \\(x\\in [a,\\,b]\\) 에서 \\(w(x)\\ge 0\\) 이며 (2) \\([a,\\,b]\\) 에 포함되는 모든 \\([c,\\,d]\\) 에서 \\(w([c,\\,d]) \\ne \\{0\\}\\) 이다. 이렇게 정의된 내적에 대해 \\(\\langle \\varphi,\\, \\psi \\rangle = 0\\) 이면 \\(\\varphi\\) 와 \\(\\psi\\) 는 서로 직교한다(be orthoginal)고 한다. \\(\\{\\phi_1,\\ldots,\\,\\phi_n\\} \\subset C_{[a,\\,b]}\\) 에 대해 \\(\\langle \\phi_i,\\,\\phi_j\\rangle = c_i\\delta_{ij}\\), \\(c_{i} &gt; 0\\) 이면 \\(\\{\\phi_1,\\ldots,\\,\\phi_n\\}\\) 을 직교하는 함수의 집합이라고 하며, \\(\\langle \\phi_i,\\,\\phi_j\\rangle =\\delta_{ij}\\) 이면 정규직교집합 이라고 한다.\n\\(p(x) \\in C_{[a,\\,b]}\\) 가 직교하는 함수의 집합 \\(\\{\\phi_1,\\ldots,\\,\\phi_n\\}\\) 의 선형결합이라고 하자. \\(p(x) = \\sum_{i=1}^n a_n \\phi_n (x)\\) 이며 여기에 무게함수 \\(w(x)\\) 정의되어 있다면\n\\[\n\\langle p(x),\\, \\phi_k(x) \\rangle = a_k \\langle \\phi_k,\\, \\phi_k \\rangle\n\\]\n이다. \\(\\{\\phi_1,\\ldots,\\,\\phi_n\\}\\) 가 정규직교집합이라면 \\(c_k=1\\) 이므로 \\(a_k = \\langle p(x),\\,\\phi_k(x)\\rangle\\) 이다.\n\\(C_{[a,\\,b]}\\) 이 내적벡터공간이며 내적벡터공간에서의 정규직교벡터-여기서는 직교하는 함수의 집합-을 생각 할 수 있으므로 당연히 이 정규하는 함수의 집합으로부터 정규직교집합을 구성 할 수 있다. 그 방법중 한가지는 그람-슈미트 과정이다. \\(\\{\\phi_1,\\ldots,\\,\\phi_n\\}\\) 이 직교하는 함수의 집합이며 \\(\\|f\\|= \\sqrt{\\langle f,\\, f\\rangle}\\) 라고 할 때,\n\\[\n\\begin{aligned}\n\\psi_1(x) &= \\dfrac{\\phi_1(x)}{\\|\\phi_1(x)\\|}, \\\\\n\\varphi_k (x) &= \\phi_k(x) - \\sum_{j=1}^{k-1} \\langle \\phi_k,\\, \\psi_j\\rangle \\psi_k(x),\\\\\n\\psi_k (x) &= \\dfrac{\\varphi_k(x)}{\\|\\varphi_k(x)\\|}\n\\end{aligned}\n\\]\n라고 정의하면 \\(\\{\\psi_1(x),\\ldots,\\,\\psi_k(x)\\}\\) 는 정규직교집합이다.\n\n\n\n2.2 르장드르 다항식\n\n\n\n\n\n\n\n정의 1 (르장드르 다항식) 르장드르 다항식 \\(P_n(x)\\) \\((n=0,\\,1,\\ldots)\\) 는 \\([-1,\\,1]\\) 구간에서 다음과 같이 재귀적으로 정의되는 다항식이다.\n\\[\n\\begin{aligned}\nP_0(x) & = 1, \\\\\nP_1(x) & = x, \\\\\nnP_n(x) & = (2n−1)xP_{n−1}(x)−(n−1)P_{n−2}(x).\n\\end{aligned}\n\\]\n르장드르 다항식에 대한 무게함수는 \\(w(x)=1\\) 이다.\n\n\n\n\n\n\n\n\n\n\n그림 3: 르장드르 다항식\n\n\n\n르장드르 다항식은 르장드르 미분방정식이라고 불리는 2계 제차 미분방정식\n\\[\n(1-x^2) y'' -2xy' + n(n+1)y=0\n\\]\n의 해이며, 무게함수는 \\(w(x)=1\\) 이다. 이 때,\n\\[\n\\langle P_n(x),\\, P_m(x) \\rangle = \\int_{-1}^1 P_n(x)P_m(x)\\,dx =  \\dfrac{2}{2n+1}\\delta_{ij}\n\\tag{3}\\]\n이다.\n\n\n\n2.3 체비쇼프 다항식\n체비쇼프 다항식은 체비쇼프 미분방정식 의 해로 \\(T_n(1) = 1,\\, T_{n}(-1)= (-1)^n\\) 의 경계조건을 만족하는 해이다.\n\n\n\n\n\n\n\n정의 2 (체비쇼프 다항식) 1형 체비쇼프 다항식 \\(T_n(x)\\) \\((n=0,\\,1,\\ldots)\\) 는 \\([-1,\\,1]\\) 구간에서 다음과 같이 재귀적으로 정의되는 다항식이다.\n\\[\n\\begin{aligned}\nT_0(x) & = 1, \\\\\nT_1(x) & = x, \\\\\nT_{n}(x) & = 2xT_{n−1}(x) - T_{n-2}(x).\n\\end{aligned}\n\\]\n체비쇼프 다항식의 무게함수는 \\(w(x) = \\dfrac{1}{\\sqrt{1-x^2}}\\) 이다.\n\n\n\n\n체비쇼프 다항식은 1형과 2형이 있지만 여기서는 1형만 사용한다.\n\n\n\n\n\n그림 4: Chevyshev 다항식\n\n\n\n체비쇼프 다항식의 내적에 대해\n\\[\n\\langle T_m,\\, T_n \\rangle = \\int_{-1}^1 T_m(x) T_n(x) \\dfrac{1}{\\sqrt{1-x^2}}\\,dx = \\left\\{\\begin{array}{ll} 0, \\qquad &m \\ne n \\\\ \\dfrac{\\pi}{2},  & m = n \\neq 0 \\\\ \\pi, & m=n=0 \\\\ \\end{array}\\right.\n\\]\n이 성립한다. 체비쇼프 미분방정식은\n\\[\n(1-x)^2 \\dfrac{d^2T_n}{dx^2} - xT_n (x) = -n^2 T_n(x)\n\\]\n인데 여기서 \\(x=\\cos\\theta\\) 로 놓으면, 위의 미분방정식은\n\\[\n\\dfrac{d^2T_n}{d\\theta^2} + n^2 T_n = 0\n\\]\n이 된다. 즉 \\(T_n(\\theta) = n\\cos\\theta\\) 이므로\n\\[\nT_n (x) = \\cos (n \\arccos x)\n\\tag{4}\\]\n이다.\n식 4 으로부터 우리는 \\(T_n(x) = 0\\) 의 해는\n\\[\nx_k =  \\cos \\left(\\dfrac{2k\\pi+\\pi}{2n}\\right),\\qquad k=0,\\,1,\\ldots,, n-1\n\\]\n임을 알 수 있다. 또한\n\\[\n\\overline{x}_k = \\cos \\left(\\dfrac{k\\pi}{n}\\right),\\qquad k=0,\\ldots,\\, n\n\\]\n에 대해 \\(T_n (\\overline{x}_k) = (-1)^k\\) 임을 안다.",
    "crumbs": [
      "수치해석 II",
      "Approximation"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_collection_of_proofs.html",
    "href": "src/numerical_analysis_using_julia/A_collection_of_proofs.html",
    "title": "수학적 증명",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "수학적 증명"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_collection_of_proofs.html#sec-mathematical_proof",
    "href": "src/numerical_analysis_using_julia/A_collection_of_proofs.html#sec-mathematical_proof",
    "title": "수학적 증명",
    "section": "1 수학적 증명",
    "text": "1 수학적 증명\n\nHölder’s inequality and Minkowski ineqality\n\n명제 1 (Young’s Inequality) \\(a\\ge 0,\\, b\\ge 0, p&gt;1,\\, q&gt;1\\) 이며 \\(1/p+1/q=1\\) 일 때 다음이 성립한다.\n\\[\nab \\le \\dfrac{a^p}{p} + \\dfrac{b^q}{q}.\n\\]\n등호는 \\(a^p=b^q\\) 일 때 성립한다.\n\n\n\n\n(증명). 우선 \\(p&gt;1,\\,q&gt;1\\) 에 대해 다음이 성립한다. \\[\n\\int_0^a x^{p-1}\\, dx + \\int_0^b y^{q-1}\\, dy = \\dfrac{a^p}{p} + \\dfrac{b^q}{q}\n\\]\n\\(\\dfrac{1}{p}+ \\dfrac{1}{q}\\) 로부터 \\(p-1 = \\dfrac{1}{q-1}\\) 임을 안다. 따라서, \\(y=x^{p-1}\\) 이면 \\(x=y^{q-1}\\) 이다. 이제 \\(\\displaystyle \\int_0^a x^{p-1}\\, dx + \\int_0^b y^{q-1}\\, dy\\) 를 생각하자. 아래 그림을 생각하면, 위의 적분이 \\(ab\\) 보다 항상 크다는 것을 알 수 있다. \\(\\square\\)\n\n\n\n\nPlots for Young’s inequality\n\n\n\n\n\n명제 2 (Hölder’s inequality and Minkowski ineqality) \\(p\\ge 1\\) 이며 \\(1/p+1/q =1\\) 일 때 다음이 성립한다. \\[\n\\begin{aligned}\n\\text{H\\\"older's inequality} : & \\sum_{i=1}^n |u_i v_i| \\le \\left(\\sum_{i=1}^n |u_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |v_i|^q\\right)^{1/q}, \\\\\n\\text{Mincowski inequality}  : & \\left(\\sum_{i=1}^n |u_i+v_i|^p\\right)^{1/p} \\le \\left(\\sum_{i=1}^n |u_i|^p\\right)^{1/p} + \\left(\\sum_{i=1}^n |v_i|^q\\right)^{1/q}.\n\\end{aligned}\n\\]\n\n\n\n\n(증명). 우선 횔더 부등식을 보이자. 영의 부등식 \\(ab &lt; \\dfrac{a^p}{p}+ \\dfrac{b^q}{q}\\) 를 이용한다. \\(\\boldsymbol{u},\\,\\boldsymbol{v}\\in \\mathbb{F}^n\\) 에 대해 \\(\\displaystyle \\|\\boldsymbol{u}\\|_p = \\left(\\sum_{i=1}^n |u_i|^p\\right)^{1/p},\\, \\|\\boldsymbol{v}\\|_q = \\left(\\sum_{i=1}^n |v_i|^q\\right)^{1/q}\\) 라 하자. \\(a_i,\\,b_i\\) 를 각각\n\\[\na_i = \\dfrac{|u_i|}{\\|\\boldsymbol{u}\\|_p},\\, \\qquad b_i = \\dfrac{|v_i|}{\\|\\boldsymbol{v}\\|_q}\n\\]\n라 하면, 영의 부등식에 의해\n\\[\na_i b_i = \\dfrac{|u_iv_i|}{\\|\\boldsymbol{u}\\|_p \\|\\boldsymbol{v}\\|_q} \\le \\dfrac{|u_i|^p}{p (\\|\\boldsymbol{u}\\|_p)^p} +  \\dfrac{|v_i|^p}{q(\\|\\boldsymbol{v}\\|_q)^q}\n\\]\n이며,\n\\[\n\\sum_{i=1}^n a_ib_i = \\dfrac{\\sum_{i=1}^n |u_i v_i|}{\\|\\boldsymbol{u}\\|_p \\|\\boldsymbol{v}\\|_q} \\le \\dfrac{\\sum_{i=1}^n|u_i|^p}{p (\\|\\boldsymbol{u}\\|_p)^p} +  \\dfrac{\\sum_{i=1}^n |v_i|^p}{q(\\|\\boldsymbol{v}\\|_q)^q}  =  \\dfrac{1}{p} + \\dfrac{1}{q} = 1\n\\]\n이므로,\n\\[\n\\sum_{i=1}^n |u_i v_i| \\le \\|\\boldsymbol{u}\\|_p \\| \\boldsymbol{v}\\|_q = \\left( \\sum_{i=1}^n |u_i|^p)^{1/p} \\right)^{1/p} \\left( \\sum_{i=1}^n |v_i|^q)^{1/q} \\right)^{1/q}\n\\]\n이다.\n이제 민코프스키 부등식을 보이자. \\(p&gt;1\\) 이므로 삼각부등식에 의해 다음이 성립한다.\n\\[\n|u_i+v_i|^p = |u_i + v_i| |u_i + v_i|^{p-1} \\le (|u_i|+|v_i|)|u_i + v_i|^{p-1} = |u_i||u_i+v_i|^{p-1} + |v_i||u_i+v_i|^{p-1}\n\\]\nHölder’s inequality 에 의해,\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n |u_i| |u_i + v_i|^{p-1} &\\le \\left(\\sum_{i=1}^n |u_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |u_i+v_i|^{q(p-1)}\\right)^{1/q}, \\\\\n\\sum_{i=1}^n |v_i| |u_i + v_i|^{p-1} &\\le \\left(\\sum_{i=1}^n |v_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |u_i+v_i|^{q(p-1)}\\right)^{1/q},\n\\end{aligned}\n\\]\n\\(1/p + 1/q=1\\) 로부터 \\(q(p-1)=p\\) 임을 안다. 따라서,\n\\[\n\\sum_{i=1}^n (|u_i|+|v_i|) |u_i+v_i|^{p-1} \\le \\left[\\left(\\sum_{i=1}^n |u_i|^p\\right)^{1/p} + \\left(\\sum_{i=1}^n |v_i|^p\\right)^{1/p}\\right]\\left(\\sum_{i=1}^n |u_i+v_i|^{p}\\right)^{1/q}\n\\]\n이므로,\n\\[\n\\begin{aligned}\n\\left(\\sum_{i=1}^n |u_i|^p\\right)^{1/p} + \\left(\\sum_{i=1}^n |v_i|^p\\right)^{1/p} &\\ge \\dfrac{\\displaystyle\\sum_{i=1}^n (|u_i|+|v_i|) |u_i+v_i|^{p-1}}{\\left(\\displaystyle \\sum_{i=1}^n |u_i+v_i|^{p}\\right)^{1/q}} \\ge \\dfrac{\\displaystyle \\sum_{i=1}^n |u_i + v_i|^p}{\\left(\\displaystyle \\sum_{i=1}^n |u_i+v_i|^{p}\\right)^{1/q}}, \\\\\n& = \\left(\\sum_{i=1}^n |u_i + v_i|^p\\right)^{1-1/q} = \\left(\\sum_{i=1}^n |u_i + v_i|^p\\right)^{1/p}\n\\end{aligned}\n\\]\n이다. 따라서 민코프스키 부등식이 성립한다.",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "수학적 증명"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_ode.html",
    "href": "src/numerical_analysis_using_julia/A_ode.html",
    "title": "상미분 방정식",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "상미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_ode.html#기본-개념",
    "href": "src/numerical_analysis_using_julia/A_ode.html#기본-개념",
    "title": "상미분 방정식",
    "section": "1 기본 개념",
    "text": "1 기본 개념\n\n하나의 독립변수에 대한 미분방정식을 상미분 방정식 (ordinary differential equation, ODE) 이라고 하며, 여러 독립변수에 대한 미분방정식을 편미분 방정식 (partial differential equation, PDE) 라고 한다.\n미분 연산을 포함하는 연산자를 미분 연산자 (differential operator) 라고 하고 여기서는 \\(\\mathcal{L}\\) 로 쓴다. 미분 연산자\\(\\mathcal{L}\\) 가 미분 가능한 함수 \\(f,\\,g\\) 와 상수 \\(a\\) 에 대해 \\[\n\\mathcal{L}(af(x) + g(x)) = a \\mathcal{L}(f(x)) + \\mathcal{L}(g(x))\n\\] 를 만족하면 \\(\\mathcal{L}\\) 을 선형 연산자(linear operator) 라고 한다.\n미분방정식은 미분연산자 \\(\\mathcal{L}\\) 과 함수 \\(F(x)\\) 에 대해 다음의 꼴로 주어진다. \\[\n\\mathcal{L}(f(x)) = F(x)\n\\tag{1}\\]\n연산자 \\(\\mathcal{L}\\) 에 포함하는 최대계의 미분이 \\(n\\) 계 미분일 때 식 1 을 \\(n\\) 계 미분 방정식 이라고 한다.\nhomogenouse 는 두가지 의미가 있으며 아래에 설명한다.\n\n\n\n1.1 Homogeneous\n\n일반적인 함수의 경우\n\\(n\\) 개의 독립변수를 가진 함수 \\(f(x_1,\\ldots,\\,x_n)\\) 가 어떤 \\(k\\) 에 대해 \\(f(ax_1,\\ldots,\\,ax_k) = a^k f(x_1,\\ldots,\\,x_k)\\) 를 만족할 때 이 함수를 계수가 \\(k\\) 인 동차 함수(homogeneous function of order \\(k\\)) 라고 한다.\n\n\n미분방정식의 경우\n미분 방정식에서 구하고자 하는 함수 \\(f\\) 와 \\(f\\) 의 모든 도함수의 곱의 계수(degree) 가 모든 방정식의 항에서 같을 때 이를 제차미분방정식 혹은 동차 미분 방정식 이라고 하며 영어로는 homogeneous differential equation 라고 한다. 미분연산자 \\(\\mathcal{L}\\) 를 구하고자 하는 함수 (\\(y\\)) 와 도함수들 (\\(y',\\, y'',\\ldots\\)) 에 대한 함수로 봤을 때, 즉 \\(\\mathcal{L} = \\mathcal{L}(y,\\,y',\\,y'',\\ldots)\\) 일 때 \\(\\mathcal{L}\\) 이 동차함수이면 제차미분방정식이라고 한다.예를 들면 \\((y')^2 + yy' =0\\) 은 제차미분방정식이지만 \\(y' + y + c=0\\) 은 \\(1\\) 차항 \\(y',\\,y\\) 와 \\(0\\) 차항 \\(c\\) 때문에 제차미분방정식이 아니다. 또한 \\(y'' + yy' = 0\\) 도 역시 제차미분방정식이 아니다. 여기서는 형용사처럼 쓸 경우 homogeneous 하다 라고 쓰겠다.\n\n\n\n\n1.2 LHODE (Linear homogeneous oridinary differential equation)\n선형미분방정식이 미분연산자 \\(\\mathcal{L}\\) 과 함수 \\(F\\) 에 대해 다음과 같이 주어졌다고 하자.\n\\[\n\\mathcal{L}(f(x)) = F(x).\n\\]\n\\(\\mathcal{L}\\) 이 선형일 경우 \\(F(x)=0\\) 이면 이 미분방정식은 homogeneous 하다. 그리고 많은 경우 선형이고 homogeneous 한 미분방정식을 얻는 것이 매우 중요하다. 그 이유중 하나는 위의 미분방정식의 해 \\(\\psi\\) 를 얻었고, homogeneous 한 방정식의 해 \\(\\phi\\) 를 얻었다면 \\(\\phi + \\psi\\) 도 항상 위의 방정식의 해이기 때문이다. 이제 linear homogeneous ordinary differential equation 을 LHODE 라고 줄여 쓰기로 하자. 그리고 linear homogeneous ordinary differential operator 를 LHODO 라고 줄여 쓰기로 하자.\n\n\n\n1.3 Superposition principle\nLHODE \\(\\mathcal{L}(f(x))=0\\) 의 해는 보통 여러개이며 그것의 선형결합 역시 해이다. 즉 \\(\\mathcal{L}\\psi = 0\\), \\(\\mathcal{L}\\phi  = 0\\) 이면 임의의 \\(a,\\,b\\) 에 대해 \\(\\mathcal{L}(a\\psi + b \\phi) = 0\\) 이다. 이것을 superposition principle 이라고 한다.",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "상미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_ode.html#계수가-상수인-ode-의-경우",
    "href": "src/numerical_analysis_using_julia/A_ode.html#계수가-상수인-ode-의-경우",
    "title": "상미분 방정식",
    "section": "2 계수가 상수인 ODE 의 경우",
    "text": "2 계수가 상수인 ODE 의 경우\n미분방정식이 다음과 같은 꼴로 주어졌다고 하자.\n\\[\n\\dfrac{d^ny}{dx^n} + a_{n-1} \\dfrac{d^{n-1}y}{dx^{n-1}} + \\cdots + a_1 \\dfrac{dy}{dx} + a_0 y = F(x).\n\\tag{2}\\]\n위 미분방정식은 \\(n\\)-계 선형 미분방정식이다. 이 경우 \\(y=e^{mx}\\) 를 놓으면 일반해, 즉 \\(F(x)=0\\) 일 경우의 해를 구할 수 있다.\n\\[\n(m^n + a_{n-1}m^{n-1} + \\cdots + a_n m + a_0)e^{mx}=0\n\\]\n이며 이를 만족 하는 \\(m\\) 은 복소수에서 \\(n\\) 개 존재한다.\n\n\n\n예제 1 (단순조화진동자) LHODE \\(y'' + \\omega^2 y = 0\\) 을 보자. \\(y=e^{\\lambda x}\\) 라고 놓으면, \\(\\lambda^2+\\omega^2 = 0\\) 이어야 하므로 \\(\\lambda = \\pm \\omega i\\) 이다. 임의의 \\(a,\\,b \\in \\mathbb{C}\\) 에 대해 \\(a e^{i\\omega x} + be^{-i\\omega x}\\) 는 미분방정식의 해이다.",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "상미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_ode.html#계-선형-ode",
    "href": "src/numerical_analysis_using_julia/A_ode.html#계-선형-ode",
    "title": "상미분 방정식",
    "section": "3 2계 선형 ODE",
    "text": "3 2계 선형 ODE\n\n3.1 Singularity\n가장 간단한 다음 꼴의 2계 ODE 를 보자.\n\\[\ny''  + p(x) y' + q(x) y  = 0\n\\tag{3}\\]\n이 식은 모든 항에서 \\(y\\) 와 그 도함수가 1차이므로 homogeneous 하다. \\(p(x)\\) 와 \\(q(x)\\) 가 유한한 값을 가질 때 이 \\(x\\) 를 ordinary point 라고 하며 그렇지 않을 때는 singular point 라고 한다. singular point 가운데 \\(\\lim_{x\\to x_0} |(x-x_0)p(x)|&lt;\\infty\\), \\(\\lim_{x\\to x_0} |(x-x_0)q(x)|&lt;\\infty\\) 이면 regular singularity 라고 하며 그렇지 않을 때 irregular singularity 혹은 essential singularity 라고 한다.\n\\(x\\to \\infty\\) 나 \\(x \\to -\\infty\\) 의 경우에는 \\(z=1/x\\) 로 치환하여 미분방정식을 \\(z\\) 에 대한 미분방정식으로 바꾼다. \\(w(z) = y(z^{-1})\\) 이라면,\n\\[\n\\begin{aligned}\ny'(x) &= \\dfrac{dy}{dz} \\dfrac{dz}{dx} = - \\dfrac{1}{x}^2 \\dfrac{dw}{dz} = -z^2w', \\\\\ny'' (x) &= \\dfrac{dy'}{dz}\\dfrac{dz}{dx} = z^4 w'' + 2z^3 w'\n\\end{aligned}\n\\]\n이므로 식 3 는 다음과 같이 변형된다.\n\\[\n4zw'' + \\left[ 2z^3 - z^2 p(1/z) \\right] w' + q(z^{-1}) w = 0.\n\\tag{4}\\]\n\n\n\n\n표 1: 중요한 2계 선형 ODE 와 singularity\n\n\n\n\n\n\n\n\n\n\n\n이름\n방정식\nregular singularity\nirregular singularity\n\n\n\n\nHypergeometric\n\\(x(x-1) y'' + [(1+a+b)x + c]\\,y' +aby=0\\)\n\\(0, 1 \\infty\\)\n\n\n\nLegendre\n\\((1-x^2) y'' -2xy' + l(l+1)y=0\\)\n\\(-1, 1, \\infty\\)\n\n\n\nChebyshev\n\\((1-x^2) y'' -xy' + n^2y = 0\\)\n\\(-1, 1, \\infty\\)\n\n\n\nConfluent hypoergeometryc\n\\(xy'' + (c-x)y' -ay = 0\\)\n\\(0\\)\n\\(\\infty\\)\n\n\nBessel\n\\(x^2y'' + xy' + (x^2-n^2)y = 0\\)\n\\(0\\)\n\\(\\infty\\)\n\n\nLaguerre\n\\(xy'' + (1-x)y' + a y = 0\\)\n\\(0\\)\n\\(\\infty\\)\n\n\nSHO\n\\(y'' + \\omega^2 y' = 0\\)\n\n\\(\\infty\\)\n\n\nHermite\n\\(y'' -2xy' + 2\\alpha y = 0\\)\n\n\\(\\infty\\)\n\n\n\n\n\n\n대표적으로 르장드르 함수 \\((1-x^2) y'' -2xy' + l(l+1)y=0\\) 를 보자. 르장드르 함수는\n\\[\ny'' - \\dfrac{2xy}{1-x^2} y' + \\dfrac{l(l+1)}{1-x^2} y = 0\n\\]\n으로 \\(x=\\pm 1\\) 에서 regular singularity 를 가진다.\n\n\n\n3.2 급수해 (Frobenius 방법)\n식 3 꼴로 주어진 2계 LHODE 를 생각하자. 이 경우 \\(y\\) 를 \\(x\\) 의 급수로 놓고 해를 구하는 것을 Frobenius 방법이라고 한다. 즉,\n\\[\ny(x) = \\sum_{i=0}^\\infty a_i x^{s+i} ,\\qquad a_0 \\ne 0\n\\tag{5}\\]\n로 놓고 미분방정식에 대입하여 계수 \\(a_i\\) 와 \\(s\\) 를 구한다. 이 때 \\(a_0 \\ne 0\\) 임에 유의하라.\n\n\n\n예제 2 (단순조화진동자의 경우) 예를 들어 \\(y'' + \\omega^2 y = 0\\) 의 경우,\n\\[\n\\begin{aligned}\n&\\sum_{i=0}^\\infty a_i (s+i)(s+i-1)x^{s+i-2} + \\omega^2 \\sum_{i=0}^\\infty a_j x^{s+i} = 0 \\\\\n\\implies & a_0 s(s-1)x^{s-2} + a_1 (s+1)s x^{s-1} + \\sum_{i=0}^\\infty \\left[a_{i+2}(s+i+2)(s+i+1) +\\omega^2 a_j\\right]x^{s+i} = 0\n\\end{aligned}\n\\]\n이로부터 우리는 세가지 조건을 얻는다.\n  (\\(1\\)) \\(a_0 s(s-1) =0\\),\n  (\\(2\\)) \\(a_1s(s+1) = 0\\),\n  (\\(3\\)) \\(a_{i+2}(s+i+2)(s+i+1) + \\omega^2 a_j = 0\\)\n\\(a_0\\ne 0\\) 이므로 (\\(1\\)) 의 조건에 의해 \\(s(s-1)=0\\) 이어야 한다. 이 방정식은 \\(s\\) 를 결정하는 중요한 방정식으로 indicial equation 이라고 한다. (\\(2\\)) 의 조건에서 \\(s=-1\\) 일 경우 \\(a_0 = 0\\) 이어야 하므로 조건에 위배된다. 따라서 남은 조건은 \\(s=0\\) 이고 \\(a_1\\) 을 자유로 두거나, \\(s=1\\) 이며 \\(a_1=0\\) 으로 두는 것 밖에 없다. 그리고 (\\(3\\)) 조건에 의해,\n\\[\na_{i+2} = \\dfrac{-\\omega^2}{(i+1)(i+2)}a_i \\implies  a_{2k} = (-1)^n \\dfrac{\\omega^{2k}}{(2k)!} a_0,\\, a_{2k+1} = (-1)^n \\dfrac{\\omega^{2k}}{(2k+1)!} a_1\n\\]\n를 만족해야 한다. \\(s=0\\) 일 경우,\n\\[\n\\begin{aligned}\ny(x){\\large\\mid}_{s=0} &= a_0 \\left[ \\sum_{k=0}^\\infty \\dfrac{(-1)^k (\\omega)^{2k}}{(2k)!} x^{2k} \\right] + a_1 \\left[ \\sum_{k=0}^\\infty \\dfrac{(-1)^k (\\omega)^{2k}}{(2k+1)!} x^{2k+1} \\right] \\\\\n&= a_0 \\cos (\\omega x) + \\dfrac{a_1}{\\omega} \\sin (\\omega x)\n\\end{aligned}\n\\]\n이다. \\(s=1\\) 인 경우는 같은 방법으로\n\\[\ny(x) {\\large \\mid}_{s=1} =  \\dfrac{a_0}{\\omega}\\sin (\\omega x)\n\\]\n임을 알 수 있다.\n\n\n\n\n\n3.3 \\(x_0\\) 주위로의 멱급수 전개\n식 5 는 \\(x_0=0\\) 근처의 전개이다. 이것을 임의의 \\(x_0\\) 근처에서 다음과 같이 전개하여 미분방정식의 해를 구하는 것도 가능하다.\n\\[\ny(x) = \\sum_{i=0}^\\infty a_i (x-x_0)^{s+i} ,\\qquad a_0 \\ne 0\n\\]\n\\(x_0\\) 를 regular singularity 로 잡으면 많은 경우 미분방정식의 해를 찾는 데 유용하지만 irregular siglularity 의 경우는 Frobenius 방법으로 해를 구할 수 없다.\n\n\n\n3.4 Fuchs’ theorem\n\n정리 1 (Fuch 의 정리) 아래와 같은 꼴의 미분방정식을 생각하자.\n\\[\n\\displaystyle y''+p(x)y'+q(x)y=F(x)\n\\]\n이 방정식은 \\(p(x),\\, q(x),\\, F(x)\\) 가 \\(x=a\\) 에서 해석적\\(^1\\) 이거나 \\(a\\) 가 regular singular point 라면 프로베니우스 방법에 의한 해를 가진다. 즉 이 미분방정식의 \\(x=a\\) 근처의 해는 어떤 양수 \\(s\\) 에 대해  \\[\ny_1=\\sum _{n=0}^{\\infty }a_{n}(x-a)^{n+s},\\quad a_{0}\\ne 0\n\\]\\(^1\\) 어떤 함수가 \\(a\\) 근처에서 수렴하는 멱급수로 표현 할 수 있을 때 이 함수를 \\(a\\) 에서 해석적 함수라고 하며 전체 구간에서 해석적일 때 해석적 함수라고 한다.\n이거나 \\(y_1\\) 함수에 \\(\\ln (x-a)\\) 를 곱한것과 다른 \\(y_1\\) 꼴의 함수를 더한 아래의 형태이다.\n\\[\ny=y_1\\ln(x-a)+\\sum _{n=0}^{\\infty }b_{n}(x-a)^{n+r},\\quad b_{0}\\neq 0.\n\\]",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "상미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_ode.html#알고-있는-해로부터-해를-더-구하는-법",
    "href": "src/numerical_analysis_using_julia/A_ode.html#알고-있는-해로부터-해를-더-구하는-법",
    "title": "상미분 방정식",
    "section": "4 알고 있는 해로부터 해를 더 구하는 법",
    "text": "4 알고 있는 해로부터 해를 더 구하는 법\n\n\n4.1 함수의 벡터공간\n구간 \\(I\\) 에서 \\(n-1\\) 번 미분 가능한함수의 집합은 벡터공간 \\(X\\) 을 이루며 따라서 함수에 대한 선형 독립을 정의 할 수 있다. 또한 \\(X\\) 에 적절한 내적을 정하여 내적벡터공간으로 구성하면 함수 사이의 직교성(orthogonality) 도 생각 할 수 있다.\n\n\n\n\n\n\n\n정의 1 (Wronskian) 어떤 구간 \\(I\\) 에서 정의된 \\(n-1\\) 번 미분 가능한 함수 \\(\\varphi_1,\\ldots,\\,\\varphi_n\\) 에 대해 다음 과 같이 정의된 행렬식을 Wronskian 이라고 한다.\n\\[\nW[\\varphi_1,\\ldots,\\,\\varphi_n] = W[\\{\\varphi_i\\}]:= \\det \\left(\\begin{bmatrix} \\varphi_1 & \\varphi_2 & \\cdots &\\varphi_n \\\\ \\varphi'_1 & \\varphi'_2 & \\cdots & \\varphi'_n \\\\ \\vdots  & & &\\vdots \\\\ \\varphi^{(n-1)}_1 & \\varphi^{(n-1)}_2 & \\cdots & \\varphi^{(n-1)}_n\\end{bmatrix}\\right)\n\\]\n\n\n\n\n\n\n\n명제 1 정의 1 의 Wronskian 에 대해 다음이 성립한다.\n  (\\(1\\)) \\(W[\\{\\varphi_i\\}]\\ne 0\\) 이면 \\(\\{\\varphi_i\\}\\) 는 선형독립이다.\n  (\\(2\\)) \\(W[\\{\\varphi_i\\}] = 0\\) 이면 \\(\\{\\varphi_i\\}\\) 는 선형종속이다.\n\n\n\n\n(증명). \\(\\{\\varphi_i\\}\\) 에 대해\n\\[\na_1 \\varphi_1  + \\cdots + a_n \\varphi_n = 0\n\\]\n이라고 하자. 그렇다면 \\(k=1,\\ldots,\\, n-1\\) 에 대해\n\\[\na_1\\varphi_1^{(k)} + \\cdots + a_n \\varphi_n^{(k)} = 0\n\\]\n을 만족하며 따라서 \\(\\begin{bmatrix} a_1 & \\cdots & a_n \\end{bmatrix}^T\\) 는 다음의 선형방정식의 해이다.\n\\[\n\\begin{bmatrix} \\varphi_1 & \\varphi_2 & \\cdots &\\varphi_n \\\\ \\varphi'_1 & \\varphi'_2 & \\cdots & \\varphi'_n \\\\ \\vdots  & & &\\vdots \\\\ \\varphi^{(n-1)}_1 & \\varphi^{(n-1)}_2 & \\cdots & \\varphi^{(n-1)}_n\\end{bmatrix} \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n\\]\n즉 \\(W[\\{\\varphi_i\\}] = 0\\) 이면 nontrivial \\(a_1,\\ldots,\\,a_n\\) 이 존재하며 따라서 \\(\\{\\varphi_i\\}\\) 는 선형 종속이다. \\(W[\\{\\varphi_i\\}] \\ne 0\\) 이면 \\(a_1=\\cdots = a_n = 0\\) 이므로 \\(\\{\\varphi_i\\}\\) 는 선형 독립이다. \\(\\square\\)\n\n\n\n\n예제 3 예제 2 에서 우리는 \\(y'' + \\omega^2 y = 0\\) 의 해 \\(\\sin \\omega x,\\,\\cos\\omega x\\) 를 구했다. 여기서\n\\[\nW[\\sin \\omega x,\\, \\cos\\omega x] = \\det \\left(\\begin{bmatrix} \\cos \\omega x & \\sin \\omega x \\\\ - \\sin \\omega x & \\cos \\omega x\\end{bmatrix}\\right) = 1\n\\]\n이므로 두 해는 선형독립임을 안다.\n\n\n\n\n\n정리 2 2계 LHODE 는 최대 두개의 독립적인 해를 가진다.\n\n\n\n\n(증명). 2계 LHODE\n\\[\ny'' + p(x) y' + q(x) y = 0\n\\tag{6}\\]\n를 생각하자. \\(y_1,\\,y_2,\\,y_3\\) 가 이 방정식의 해라고 하자. 그리고 \\(W_{ij} = W[y_i, y_j]\\) 라고 하면 \\(W_{ij}=  y_i y'_j - y'_i y_j\\) 이다. 그렇다면\n\\[\nW'_{ij} = \\dfrac{dW_{ij}}{dx} = y'_i y'_j + y_i y''_j - y''_iy_j - y'_i y'_j = y_i y''_j - y''_i y_j\n\\tag{7}\\]\n이다. 식 6 로 부터,\n\\[\n\\dfrac{y''_i}{y_i} + p(x) \\dfrac{y'_i}{y_i} = -q(x) = \\dfrac{y''_j}{y_j} + p(x) \\dfrac{y_j'}{y_j}\n\\]\n이며 여기에 \\(y_i y_j\\) 를 곱하면\n\\[\nW'_{ij} = -p(x) W_{ij}\n\\tag{8}\\]\n를 얻는다. 이제\n\\[\n\\begin{aligned}\nW(y_1, y_2, y_3) &= \\det\\left(\\begin{bmatrix} y_1 & y_2 & y_3 \\\\ y'_1 & y'_2 & y'_3 \\\\ y''_1 & y''_2 & y''_3 \\end{bmatrix}\\right) = - y'_1 W_{23}' - y'_2 W'_{31} - y'_3 W'_{12} \\\\\n&= -p(x) (y_1W_23 + y_2 W_31 + y_3 W_12) = \\det\\left(\\begin{bmatrix} y_1 & y_2 & y_3 \\\\ y'_1 & y'_2 & y'_3 \\\\ y'_1 & y'_2 & y'_3 \\end{bmatrix}\\right) = 0\n\\end{aligned}\n\\]\n이다. 따라서 2계 LHODE 의 선형 독립인 해는 많아야 2개이다. \\(\\square\\)\n\n\n이제 식 6 의 꼴의 미분방정식의 한 해를 알고 있다고 하자. Frobenius 방법으로든, 직관으로 파악했든 하나의 해를 알았다면 이제 두번째 해, 즉 첫번째 해에 대해 선형독립인 나머지 해를 찾을 수 있음을 보이고자 한다. 두 해의 Wronskian \\(W[y_1,\\,y_2] = W\\) 은 \\(x\\) 에 대한 함수이다. 식 8 로부터,\n\\[\n\\dfrac{d}{dx}W(x) = -p(x) W(x) \\implies W(x) = W(a) e^{-\\int_{a}^x p(t)\\,dt}\n\\]\n를 얻는다. 또한\n\\[\nW(x) = y_1y_2' - y_1'y_2 = y_1^2 \\dfrac{d}{dx}\\left(\\dfrac{y_2}{y_1}\\right)\n\\]\n이므로,\n\\[\n\\begin{aligned}\ny_2(x) &= y_1(x) \\int_b^x \\dfrac{W(x')}{[y_1(x')]^2} \\, dx' - \\dfrac{y_2(b)}{y_1(b)}y_1(x) \\\\\n&= y_1(x) W(a) \\int_b^x \\dfrac{\\exp \\left(-\\int_a^{x'} p(t)\\, dt\\right)}{[y_1(x')]}\\, dx'- \\dfrac{y_2(b)}{y_1(b)}y_1(x)\n\\end{aligned}\n\\]\n이다. 여기서 \\(a,\\,b\\) 는 임의의 상수이다. 그런데 우리는 \\(y_1\\) 과 선형독립인 \\(y_2\\) 를 찾는 것이므로 뒤의 \\(y_2(b)y_1(x)/y_2(b)\\) 는 없어도 무관하다. 따라서,\n\\[\ny_2(x) = y_1(x) W(a) \\int_b^x \\dfrac{\\exp \\left(-\\int_a^{x'} p(t)\\, dt\\right)}{[y_1(x')]}\\, dx'\n\\]\n만 생각해도 된다.\n\n\n\n예제 4 (단순조화진동자(SHO) 의 경우) 우리는 \\(y'' + \\omega^2 y=0\\) 의 한 해가 \\(\\cos \\omega x\\) 임을 안다. \\(p(x) = 0\\) 이므로 \\(W(x) = W_0 = \\text{const.}\\) 이다. 따라서,\n\\[\ny_2 (x) = \\cos(\\omega x) W_0 \\int_{0}^x \\dfrac{1}{\\cos ^2 \\omega x'}\\, dx' = \\dfrac{W_0}{\\omega} \\cos (\\omega x) \\tan (\\omega x) = \\dfrac{W_0}{\\omega} \\sin (\\omega x)\n\\]",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "상미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_ode.html#sturm-liouville-theory",
    "href": "src/numerical_analysis_using_julia/A_ode.html#sturm-liouville-theory",
    "title": "상미분 방정식",
    "section": "5 Sturm-Liouville theory",
    "text": "5 Sturm-Liouville theory\n2계 LHODE 연산자 \\(\\mathcal{L}\\) 에 대해\n\\[\n\\mathcal{L}\\psi (x) = \\lambda \\psi (x)\n\\]\n는 미분방정식을 고유값 문제로 바꾸게 된다. 이 경우 \\(\\mathcal{L}\\) 은\n\\[\n\\mathcal{L}\\psi (x) = \\left[p_0 (x) \\dfrac{d}{dx^2} + p_1(x) \\dfrac{d}{dx} + p_2(x)\\right]\\psi(x)\n\\tag{9}\\]\n꼴로 주어진다. 특정한 물리적 상황에 대한 미분방정식은, 그 해가 가저야 할 조건이 주어지며 많은 경우 그 해, 혹은 그 해의 도함수가 가져야할 경계조건으로 주어진다. 어떤 이계 미분 방정식이 \\(x=a,\\,b\\) 에서 \\(f(a)=f(b) =0\\) 이거나 \\(f'(a) = f'(b)=0\\) 이어야 한다면, 이 조건을 만족하는 미분가능한 함수의 집합 \\(X\\) 는 힐베르트 공간을 이룬다. 즉 우리는 미분방정식을 벡터공간의 입장에서 다룰 수 있다. 만약 적절한 내적(inner product) 가 주어진 상황에서 \\(\\mathcal{L}\\) 연산자가 hermitian 이라면 고유값은 항상 실수이며, 전체 벡터공간을 고유벡터의 선형결합으로 표현 할 수 있다.\n\n\n5.1 Hermitian operator\n식 9 형태로 주어진 연산자 \\(\\mathcal{L}\\) 에서 \\(p'_0 (x) = p_1(x)\\) 일 경우를 생각하자. 그렇다면,\n\\[\n\\mathcal{L} \\psi (x) = \\left[\\dfrac{d}{dx} \\left(p_0(x) \\dfrac{d}{dx}\\right) + p_2(x)\\right]\\psi (x) = [p_0(x) \\psi'(x)]' + p_2(x) ] \\psi(x)\n\\]\n이다. 여기서 내적이 \\(\\langle u(x),\\, v(x) \\rangle = \\displaystyle \\int_{a}^b v^\\ast(x) u(x)\\, dx\\) 로 정의되었다면,\n\\[\n\\begin{aligned}\n\\langle \\mathcal{L}u(x),\\, v(x)\\rangle &= \\int_{a}^b \\left(v^\\ast ( p_0 u')' + v^\\ast p_2 u\\right) \\, dx \\\\\n&= {\\large \\left[ \\right.}v^\\ast p_0 u' {\\large \\left.\\right]}_a^b +\\int_a^b \\left( -(v^\\ast)' p_0 u' + v^\\ast p_2 u\\right) \\, dx \\\\\n&= {\\large \\left[ \\right.}v^\\ast p_0 u'  - (v^\\ast)'p_0 u {\\large \\left.\\right]}_a^b + \\int_a^b \\left[ (p_0 {v^\\ast}')' u + v^\\ast p_2 u\\right]\\, dx\\\\\n&= {\\large \\left[ \\right.}v^\\ast p_0 u'  - (v^\\ast)'p_0 u {\\large \\left.\\right]}_a^b + \\int_a^b (\\mathcal{L}v)^\\ast u\\, dx \\\\\n&= {\\large \\left[ \\right.}v^\\ast p_0 u'  - (v^\\ast)'p_0 u {\\large \\left.\\right]}_a^b + \\langle u(x),\\, \\mathcal{L}v(x)\\, \\rangle\n\\end{aligned}\n\\]\n이다. 만약 \\({\\large \\left[ \\right.}v^\\ast p_0 u'  - (v^\\ast)'p_0 u {\\large \\left.\\right]}_a^b = 0\\) 인 경계조건을 가지고 있다면,\n\\[\n\\langle \\mathcal{L}u(x),\\, v(x)\\rangle = \\langle u(x),\\, \\mathcal{L}v(x)\\, \\rangle\n\\]\n이며 이것은 벡터공간에서의 에르미트 연산자의 조건이다. 즉 일반적인 내적 벡터공간에서의 에르미트 연산자와는 달리 미분방정식에서의 에르미트 조건은 경계조건을 포함한다. 보통 \\(u(a)=u(b) = v(a)=v(b) = 0\\) 이거나 \\(u'(a) =u'(b) = v'(a) = v'(b) = 0\\) 인 조건을 가지게 되며 전자를 디리클레 경계조건 (Dirichlet boundary condition) 이라고 하고 후자를 노이만 경계 조건 (Neumann boundary condition) 이라고 한다.\n이제 디리클레 경계조건, 혹은 노이만 경계 조건을 만족하는 \\(C^2\\) 함수의 집합을 \\(X\\) 라고 하면 \\(X\\) 는 벡터공간며 여기에 내적 \\(\\langle u,\\,v\\rangle = \\displaystyle \\int_a^b v^\\ast(x)u(x)\\,dx\\) 이 정의되어 있다면 \\(X\\) 는 내적 벡터 공간이 된다. \\(\\mathcal{L}\\) 연산자는 에르미트 연산자이므로 \\(\\mathcal{L}\\) 의 고유함수로 \\(X\\) 의 기저를 구성 할 수 있다. \\(\\varphi_i,\\, \\varphi_j\\) 가 각각 \\(\\lambda_i,\\, \\lambda_j\\) 를 고유값으로 갖는 고유함수이며 \\(\\lambda_i\\neq \\lambda_j\\) 라면 당연히 \\(\\langle \\varphi_i,\\,\\varphi_j \\rangle = 0\\) 이다.\n\n\n\n5.2 미분방정식을 Hermitian 으로 변경하기\n위에서 설명했듯이 미분연산자가 hermitian 이라면 매우 좋은 성질을 갖게 된다. 만약 자체로는 hermtian 이 아니지만 약간 변형하여 hermitian 으로 만들 수 있다면? 식 9 형태의 미분방정식이 hermitian 이 될 조건은 아래의 두가지 이다.\n  (1) \\(p_0'(x) = p_1(x)\\),\n  (2) 경계조건 \\({\\large \\left[ \\right.}v^\\ast p_0 u'  - (v^\\ast)'p_0 u {\\large \\left.\\right]}_a^b =  0\\) .\n만약 어떤 함수 \\(w(x)\\) 를 곱하여,\n\\[\n\\mathcal{L}'\\psi(x) = w(x) \\mathcal{L} \\psi(x) = \\left[w(x) p_0 (x) \\dfrac{d}{dx^2} + w(x) p_1(x) \\dfrac{d}{dx} + w(x) p_2(x)\\right]\\psi(x)\n\\]\n의 \\(\\mathcal{L}'\\) 이 hermitian 이 되려면 \\((wp_0)' =wp_1\\) 이 되어야 한다. \\((wp_0)' = (wp_0)\\dfrac{p_1}{p_0}\\) 이므로,\n\\[\nw(x)p_0(x) = w(a)p_0(a) \\exp \\left(\\int_a^x \\dfrac{p_1(t)}{p_0(t)}\\, dt\\right)\n\\tag{10}\\]\n이며,\n\\[\nw(x) = \\dfrac{1}{p_0(x)} \\exp \\left(\\int_a^x \\dfrac{p_1(t)}{p_0(t)}\\, dt\\right)\n\\]\n로 놓을 수 있다. \\(w(a)p_0(a)\\) 는 상수로 미분방정식에 영항을 끼치지 않아서 \\(1\\) 로 놓았다. 실제로는 계산하기 편하도록 어떤 0 이 아닌 상수를 곱하여도 상관 없다. 이제 \\[\n\\overline{p}_0 = \\exp \\left(\\int^x \\dfrac{p_1(t)}{p_0(t)}\\, dt\\right),\\qquad \\overline{p}_1 = \\dfrac{p_1(x)}{p_0(x)}\\exp \\left(\\int^x \\dfrac{p_1(t)}{p_0(t)}\\, dt\\right)\n\\] 에 대해\n\\[\n\\mathcal{L}'\\psi (x) = \\left[\\overline{p}_0 (x) \\dfrac{d^2}{dx^2} + \\overline{p}_1(x) \\dfrac{d}{dx} + w(x)p_2(x)\\right]\\psi(x)\n\\]\n의 미분방정식을 얻었다.\n이제 에르미트 조건을 알아보자.\n\\[\n\\begin{aligned}\n\\int_a^b v^\\ast (x) \\mathcal{L}'u(x)\\, dx &= \\int_a^b v^\\ast (x) w(x)\\mathcal{L}u(x)\\, dx \\\\\n&=  {\\large \\left[ \\right.} v^\\ast \\overline{p}_0 u' - (v^\\ast)' \\overline{p}_0 u {\\large \\left. \\right]}_a^b + \\int_a^b (\\mathcal{L}'v(x))^\\ast u(x)\\, dx \\\\\n&=  {\\large \\left[ \\right.} v^\\ast \\overline{p}_0 u' - (v^\\ast)' \\overline{p}_0 u {\\large \\left. \\right]}_a^b + \\int_a^b w(x)(\\mathcal{L}v(x))^\\ast u(x)\\, dx\n\\end{aligned}\n\\tag{11}\\]\n이제 내적을\n\\[\n\\langle u(x),\\, v(x)\\rangle = \\int_a^b v^\\ast (x) u(x) w(x)\\, dx\n\\tag{12}\\]\n로 정의하자. 이 때 \\(w(x)\\) 를 무게 함수(weight function) 이라고 한다. 그렇다면 식 11 은\n\\[\n\\langle \\mathcal{L}u(x),\\, v(x)\\rangle =  {\\large \\left[ \\right.} v^\\ast \\overline{p}_0 u' - (v^\\ast)' \\overline{p}_0 u {\\large \\left. \\right]}_a^b  + \\langle u,\\, \\mathcal{L}v(x)\\rangle\n\\]\n이 된다. 즉 내적의 정의가 변하며 연산자 \\(\\mathcal{L}\\) 이 hermitian 이 된다.\n\n\n\n예제 5 (르장드르 함수 (Legendre function)) 르장드르 미분연산자\n\\[\n\\mathcal{L} = (1-x^2) \\dfrac{d^2}{dx^2} -2x\\dfrac{d}{dx}\n\\]\n에 대해 \\(\\mathcal{L}P_l(x) = -l(l+1)P_l(x)\\) 를 푸는 것이 표 1 의 르장드르 미분방정식의 해이다. 자연수 \\(l\\) 에 대한 미분방정식의 해는 \\(l\\) 차 다항식임이 알려저 있다. \\(\\dfrac{d}{dx}(1-x^2)=-2x\\) 이므로 무게함수는 \\(w(x)=1\\) 이다.\n\n\n\n\n\n예제 6 (체비쇼프 함수 (Legendre function)) 체비쇼프 미분연산자\n\\[\n\\mathcal{L} = (1-x^2) \\dfrac{d^2}{dx^2} -x\\dfrac{d}{dx}\n\\]\n에 대해 \\(\\mathcal{L}T_n(x) = -n^2T_n(x)\\) 를 푸는 것이 표 1 의 르장드르 미분방정식의 해이다. 자연수 \\(n\\) 에 대한 미분방정식의 해는 \\(n\\) 차 다항식임이 알려저 있다. \\(\\dfrac{d}{dx}(1-x^2)=-2x\\) 이므로 무게함수는\n\\[\nw(x) = \\dfrac{1}{1-x^2} \\exp \\left(\\int_{a}^x \\dfrac{t}{1-t^2}\\, dt \\right)= \\dfrac{1}{\\sqrt{1-x^2}}\n\\]\n이다. 적분시 \\(a\\) 에 대한 항은 상수곱이 되므로 무시했다.\n\n\n\n\n\n예제 7 (라게르 함수(Laguerre function)) 아래의 미분방정식 연산자 \\(\\mathcal{L}\\) 에 대한 고유값 문제 \\(\\mathcal{L}\\psi =\\lambda \\psi\\) 를 풀고자 한다.\n\\[\n\\mathcal{L} = x \\dfrac{d^2}{dx^2} + (1-x)\\dfrac{d}{dx}.\n\\tag{13}\\]\n이 때 \\(\\psi\\) 는 \\(0\\le x &lt;\\infty\\) 에서 nonsingular 하며, \\(\\psi (x\\to \\infty) = 0\\) 이라고 하자. \\(p'_0\\ne p_1\\) 이므로 weighting function \\(w(x)\\) 를 식 10 으로 계산하면,\n\\[\nw(x) = \\dfrac{1}{x} \\exp \\left(\\int^x \\dfrac{1-t}{t}\\,dt\\right) = \\dfrac{1}{x} e^{\\ln x - x} = e^{-x}\n\\]\n이 미분방정식이 hermitian 이 되려면 경계조건\n\\[\n0 = {\\large \\left[ \\right.} v^\\ast \\overline{p}_0 u' - (v^\\ast)' \\overline{p}_0 u {\\large \\left. \\right]}_a^b =  {\\large \\left[ \\right.} xe^{-x} (v^\\ast  u' - (v^\\ast)'  u) {\\large \\left. \\right]}_a^b\n\\]\n을 만족해야 한다.",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "상미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/A_ode.html#직교하는-다항식",
    "href": "src/numerical_analysis_using_julia/A_ode.html#직교하는-다항식",
    "title": "상미분 방정식",
    "section": "6 직교하는 다항식",
    "text": "6 직교하는 다항식\n\n6.1 로드리게스 공식\nSturm-Liouville 형태의 미분방정식\n\\[\np(x) y'' + q(x)y' + \\lambda y = 0\n\\tag{14}\\]\n를 생각하자. 많은 경우 \\(p(x),\\, q(x)\\) 는 각각 2차, 1차 다항식 형태이다.\n\\[\np(x) = \\alpha x^2 + \\beta x + \\gamma,\\qquad q(x) = \\mu x + \\nu\n\\]\n이 미분방정식의 \\(n\\) 차 다항식 해를 \\(y_n = \\sum_{k=0}^n c_k x^k\\) 라고 이 해에 대한 \\(\\lambda\\) 를 \\(\\lambda_n\\) 이라고 하자. \\(c_n\\neq 0\\) 이다. 그렇다면 식 14 는 최고차항에서\n\\[\n\\lambda_n = -n(n-1)\\alpha - n\\mu\n\\tag{15}\\]\n을 만족해야 한다. 무게함수 \\(w(x)\\) 를 생각하면\n\\[\n\\dfrac{d}{dx}[w(x)p(x)y'] + \\lambda w(x)y = 0\n\\]\n을 만족해야 한다. 이것을 이용하여 아래의 정리 3 를 증명 할 수 있다. Arfken 의 수리물리학에서 Rodriguess formular 를 찾아보면 지루한 증명을 찾아 볼 수 있으나 여기서는 생략하기로 한다.\n\n\n\n정리 3 아래와 같이 정의된 \\(y_n(x)\\) 는 \\(\\lambda\\) 가 식 15 의 \\(\\lambda_n\\) 인 식 14 의 해이다. \\[\ny_n(x) = \\dfrac{1}{w(x)} \\left(\\dfrac{d}{dx}\\right)^n \\left[w(x)p(x)^n\\right].\n\\tag{16}\\]\n\n\n\n\n\n\n\n6.2 Schlaefli 적분\n식 16 을 만족한다면 코시 적분 공식를 사용하여\n\\[\ny_n (x) = \\dfrac{1}{2}\\dfrac{n!}{2\\pi i} \\oint_C \\dfrac{w(z)[p(z)]^n}{(z-x)^{n+1}} \\, dz\n\\tag{17}\\]\n를 얻을 수 있다. 이를 Schlaefli 적분 이라고 한다.\n\n\n\n6.3 생성 함수\n\n\n\n\n\n\n\n정의 2 우리가 원하는 함수의 집합 \\(\\{f_n(x)\\}\\) 가 \\(x\\) 와 보조변수 \\(t\\) 에 대해 정의된 어떤 함수 \\(g(x,\\,t)\\) 에 대해\n\\[\ng(x,\\,t) = \\sum_{n} c_n f_n(x) t^n\n\\]\n의 형태가 될 때 \\(g(x,\\,t)\\) 를 생성 함수(generating function) 이라고 한다.\n\n\n\n\n\\(n = 0,\\,1,\\ldots\\) 일 때는 \\(t\\) 에 대한 테일러 전개로 간주 할 수 있으며 \\(n\\) 이 음의 정수를 포함한다면 로랑 전개(Laurant series) 로 간주 할 수 있다. 복소함수의 유수 정리(residue theorem) 을 사용한다면\n\\[\nc_n f_n(x) = \\dfrac{1}{2\\pi i} \\oint \\dfrac{g(x,\\,t)}{t^{n+1}}\\, dt\n\\]\n를 통해 얻을 수 있다. 여기서 적분 경로(contour) 는 \\(0\\) 을 포함하며 \\(0\\) 이외의 다른 특이점이 없어야 한다.\n생성함수는 식 17 을 이용하면 다음과 같이 얻을 수 있다.\n\\[\ng(x,\\,t) = \\dfrac{1}{w(x)} \\sum_{n=0}^\\infty t^n \\dfrac{n!}{2\\pi i} \\oint_C \\dfrac{w(z)[p(z)]^n}{(z-x)^{n+1}} \\, dz\n\\tag{18}\\]\n\n\n\n\n표 2: 중요한 2계 선형 ODE 의 생성 함수\n\n\n\n\n\n\n\n\n\n\n이름\n방정식\n생성 함수\n\n\n\n\nLegendre\n\\(\\displaystyle (1-x^2) y'' -2xy' + n(n+1)y=0\\)\n\\(\\displaystyle (1-2xt+t^2)^{-1/2} = \\sum_{n=0}^\\infty P_n(x)t^n\\)\n\n\nChebyshev type \\(I\\)\n\\((1-x^2) y'' -xy' + n^2y = 0\\)\n\\(\\displaystyle \\dfrac{1-t^2}{1-2xt+t^2} = T_0 (x) + 2\\sum_{n=1}^\\infty T_n(x)t^n\\)\n\n\nLaguerre\n\\(xy'' + (1-x)y' + n y = 0\\)\n\\(\\displaystyle \\dfrac{e^{-xt/(1-t)}}{1-t} = \\sum_{n=0}^\\infty L_n (x) t^n\\)\n\n\nHermite\n\\(y'' -2xy' + 2n y = 0\\)\n\\(\\displaystyle e^{-t^2+2xt} = \\sum_{n=0}^\\infty \\dfrac{1}{n!} H_n(x)t^n\\)",
    "crumbs": [
      "수치해석 II",
      "수학적 자료",
      "상미분 방정식"
    ]
  },
  {
    "objectID": "src/numerical_analysis_using_julia/index_part1.html",
    "href": "src/numerical_analysis_using_julia/index_part1.html",
    "title": "수치해석 I",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]\n\n\n여기서는 기본적인 수치해석에 대해 다룬다. 수치해석에 필요한 배경 지식을 다루며, julia 언어로 구현하는데 필요한 내용을 설명한다. 선형대수학과 행렬 계산에 대해 다루며, 보간법과 다항식, 1변수 미분과 적분, 그리고 1변수 방정식의 해를 구하는 방법에 대해 알아본다.\n\n알고리즘은 julia 코드로 최대한 읽고 이해하기 쉬운 코드로 제시한다. 이 julia 코드는 자체로서 알고리즘이 원하는 동작을 하는 코드이다.\n이 책에서 많은 경우 앞부분에서 사용했던 알고리즘이나 방법을 뒤에서 사용하는 경우가 많이 있다. 이를 위해 여기서 구현한 것들을 julia 패키지로 만들었고 github 에 올려두었다. NAJ.jl 이며 julia 에 설치 할 수 있다.\n\njulia&gt; ]\n(@v1.10) pkg&gt; add https://github.com/Julia-KAERI/NAJ.jl.git\n\n\n참고자료\n수치해석 입문을 위해 읽어볼 만한 자료들은 다음과 같다.\n\n계산수학의 태동과 발전 : KAIST 수학과 이창옥 교수님의 강연 요약. 수치해석의 다양한 토픽을 역사적, 실제적 맥락에서 다루었다.\nSome disasters attributable to bad numerical computing : 캐나다 몬트리올 대학 Max Mignotte 교수님의 개인 사이트. 제목대로 수치해석의 오류로 인한 대형사고 3가지를 다룬다.\n수치해석을 모르면 생기는 사고들 : 한국어 자료",
    "crumbs": [
      "수치해석 I"
    ]
  },
  {
    "objectID": "src/topics/nonlinear_least_square_fit.html",
    "href": "src/topics/nonlinear_least_square_fit.html",
    "title": "Levenberg-Marquardt 방법",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\]",
    "crumbs": [
      "다양한 주제들",
      "Levenberg-Marquardt 방법"
    ]
  },
  {
    "objectID": "src/topics/nonlinear_least_square_fit.html#참고-문헌",
    "href": "src/topics/nonlinear_least_square_fit.html#참고-문헌",
    "title": "Levenberg-Marquardt 방법",
    "section": "1 참고 문헌",
    "text": "1 참고 문헌\n\n\nGavin, Henri P. 2024. “The Levenberg-Marquardt Algorithm for Nonlinear Least Squares Curve-Fitting Problems.” 2024. https://people.duke.edu/~hpgavin/lm.pdf.",
    "crumbs": [
      "다양한 주제들",
      "Levenberg-Marquardt 방법"
    ]
  },
  {
    "objectID": "src/topics/nonlinear_least_square_fit.html#비선형-최소-제곱-문제nonlinear-least-squares-problems",
    "href": "src/topics/nonlinear_least_square_fit.html#비선형-최소-제곱-문제nonlinear-least-squares-problems",
    "title": "Levenberg-Marquardt 방법",
    "section": "2 비선형 최소 제곱 문제(nonlinear least squares problems)",
    "text": "2 비선형 최소 제곱 문제(nonlinear least squares problems)\n실험에서 측정되거나 계산된 데이터가 독립변수 \\(x_1,\\ldots,\\,x_n\\) 과 종속변수 \\(y_1,\\ldots,\\,y_n\\) 으로 이루어 졌다고 하자. 그리고 이 데이터를 어떤 모델 함수 \\(f(x)\\) 와 비교하여 이 데이터를 가장 잘 설명하는 혹은 이 데이터와 가장 잘 맞는, 아직 정해지지 않은 파라미터 \\(\\boldsymbol{p} = \\{p_1,\\ldots,\\,p_m\\}\\) 를 결정해야 한다고 하자. 이제 함수를 \\(f(x;\\boldsymbol{p})\\) 라고 표기하자. 예를 들어 모델 함수가 다음과 같다고 하자.\n\\[\nf(x) = ae^{-\\lambda x} + b.\n\\]\n여기서 결정해야 하는 값이 \\(a,\\, \\lambda,\\,b\\) 라면 \\(\\boldsymbol{p}=(a,\\lambda,\\,b)\\) 이다. 어떤 이유로 \\(b\\) 값이 고정된다면 \\(\\boldsymbol{p}=(a,\\,\\lambda)\\) 이다. \\(\\boldsymbol{p}=(a,\\lambda,\\,b)\\) 인 경우 다음과 같이 표기 할 수 있다.\n\\[\nf(x;\\boldsymbol{p}) = p_1 e^{-p_2 x} + p_3\n\\]\n데이터와 모델을 비교할 때 아주 이상적인 상태라면 \\(y_i = f(x_i;\\,\\boldsymbol{p})\\) 인 \\(\\boldsymbol{p}\\) 를 찾겠지만 이런 경우는 거의 없으며 실제로는 \\(\\boldsymbol{y} = y_1,\\ldots,\\,y_n\\) 과 \\(\\hat{\\boldsymbol{y}}=f(x_1;\\boldsymbol{p}),\\ldots,f(x_n;\\boldsymbol{p})\\) 의 차이를 최소화 하는 \\(\\boldsymbol{p}\\) 값을 찾아야 하는 경우가 대부분이다. 이 때 아래와 같이 정의된 함수 \\(E(\\boldsymbol{p})\\) 를 최소화 하는 \\(\\boldsymbol{p}^\\ast\\) 를 찾는 것을 최소제곱법(least square method) 이라고 한다. \\[\nE(\\boldsymbol{p}) = \\sum_{i=1}^n \\left[\\dfrac{y_i - f(x_i;\\boldsymbol{p})}{\\sigma_{y_i}}\\right]^2\n\\tag{1}\\]\n여기서 \\(\\sigma_{y_i}\\) 는 \\(y_i\\) 측정에 대한 측정 오차로 예를 들어 방사능 측정이나 입자 측정 실험에서는 많은 경우 측정값이 푸아송 분포를 따르며 이 때 \\(\\sigma_{y_i}= \\sqrt{y_i}\\) 이다. 식 1 를 좀 다르게 표현하면 \\(w_i = (1/\\sigma_{y_i})^2\\) 에 대해\n\\[\nE(\\boldsymbol{p}) = \\sum_{i=1}^n w_i\\left[y_i - f(x_i;\\boldsymbol{p})\\right]^2\n\\tag{2}\\]\n이며 식 2 를 최소로 하는 \\(\\boldsymbol{p}^\\ast\\) 는 다음과 같이 표현 할 수 있다.\n\\[\n\\boldsymbol{p}^\\ast = \\arg \\min \\sum_{i=1}^{n} w_i (y_i - f(x_i;\\boldsymbol{p}))^2\n\\tag{3}\\]\n\n이제 측정값을 열벡터\\(\\boldsymbol{y}=\\begin{bmatrix} y_1 & \\cdots &y_n\\end{bmatrix}^T\\), 모델값을 열벡터 \\(\\hat{\\boldsymbol{y}}(\\boldsymbol{p}) = \\begin{bmatrix} f(x_1;\\boldsymbol{p}) & \\cdots &f(x_n;\\boldsymbol{p})\\end{bmatrix}^T\\) 라고 하자. 또한 \\(w_i\\) 를 표현하는 가중치 행렬 \\(\\boldsymbol{W}\\) 를 \\(W_{ij}= \\dfrac{\\delta_{ij}}{\\sigma_{y_i}^2}\\) 라고 하면\n\\[\nE(\\boldsymbol{p}) = (\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}))^T \\cdot \\boldsymbol{W} \\cdot (\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}))\n\\tag{4}\\]\n이다. 여기서 \\(\\boldsymbol{W}\\) 는 대각행렬이며 따라서 대칭행렬이다.",
    "crumbs": [
      "다양한 주제들",
      "Levenberg-Marquardt 방법"
    ]
  },
  {
    "objectID": "src/topics/nonlinear_least_square_fit.html#gradient-descent-method",
    "href": "src/topics/nonlinear_least_square_fit.html#gradient-descent-method",
    "title": "Levenberg-Marquardt 방법",
    "section": "3 Gradient-Descent Method",
    "text": "3 Gradient-Descent Method\n\\(E(\\boldsymbol{p})\\) 의 최소값은 \\(E(\\boldsymbol{p})\\) 의 미분이 \\(0\\) 인 점에서 나타난다. \\(E(\\boldsymbol{p})\\) 가 벡터에 대한 이차형식이며 \\(\\boldsymbol{W}\\) 가 대칭행렬이므로 \\[\n\\begin{aligned}\n\\dfrac{\\partial E(\\boldsymbol{p})}{\\partial p_i} &= -2(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}))^T \\cdot \\boldsymbol{W} \\cdot \\dfrac{\\partial \\hat{\\boldsymbol{y}}(\\boldsymbol{p})}{\\partial p_i}\n\\end{aligned}\n\\]\n로 나타 낼 수 있다. 즉 \\(\\hat{\\boldsymbol{y}}(\\boldsymbol{p})\\) 에 대한 야코비 행렬 \\(\\boldsymbol{J}\\) 에 대해\n\\[\n\\nabla E(\\boldsymbol{p}) =  -2(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}))^T  \\boldsymbol{WJ} ,\\qquad \\text{where }J_{ij} = \\dfrac{\\partial \\hat{y}_i(\\boldsymbol{p})}{\\partial p_j}\n\\]\n이다. \\(\\nabla E(\\boldsymbol{p})\\) 는 \\(1\\times m\\) 행렬, 즉 행벡터임에 유의하자. 우리는 \\(-(\\nabla E(\\boldsymbol{p}))^T\\) 방향이 \\(E(\\boldsymbol{p})\\) 를 감소시키는 방향임을 안다. 그러나 \\(-\\nabla E(\\boldsymbol{p})\\) 값이 크다면 최소값 지점을 넘어갈 수 있으므로 보통 작은 값을 곱해서 조금씩 진전시키며 이것을 반복하여 \\(E(\\boldsymbol{p})\\) 를 최소로 하는 값을 찾는다. 보통 초기값 \\(\\boldsymbol{p}_0\\) 는 사용자에 의해 주어지며, 곱해지는 값 \\(\\lambda_k\\) 는 각 반복에서 결정된다. \\(k\\) 차의 \\(\\boldsymbol{p}_k\\) 가 정해졌을 때 \\(\\boldsymbol{p}_{k+1}\\) 은 다음과 같다.\n\\[\n\\boldsymbol{p}_{k+1} = \\boldsymbol{p}_k + \\boldsymbol{h}_{gd}= \\boldsymbol{p}_k - \\lambda_k\\left(\\nabla E(\\boldsymbol{p})\\right)^T = \\boldsymbol{p}_k + \\lambda_k  \\boldsymbol{J}^T\\boldsymbol{W}(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p})).\n\\tag{5}\\]\n여기서 \\(\\boldsymbol{h}_{gd}\\) 는 gradient-descent 방법을 통해 \\(\\boldsymbol{p}\\) 가 변화하는 값이다. \\(\\lambda_k\\) 가 너무 작다면 \\(E(\\boldsymbol{p}_{k+1}) \\approx E(\\boldsymbol{p}_k)\\) 일 겻이며 \\(\\lambda_k\\) 가 지나치게 크다면 \\(E(\\boldsymbol{p}_{k+1}) \\ge E(\\boldsymbol{p}_{k})\\) 가 될 수 있다. 보통 \\(\\lambda_k\\) 를 다소 큰 값으로 잡은 후 \\(E(\\boldsymbol{p}_{k+1}) \\ge E(\\boldsymbol{p}_{k})\\) 일 때 \\(\\lambda_k\\) 값을 줄여 다시 식 5 를 수행하게 된다.",
    "crumbs": [
      "다양한 주제들",
      "Levenberg-Marquardt 방법"
    ]
  },
  {
    "objectID": "src/topics/nonlinear_least_square_fit.html#가우스-뉴턴-방법",
    "href": "src/topics/nonlinear_least_square_fit.html#가우스-뉴턴-방법",
    "title": "Levenberg-Marquardt 방법",
    "section": "4 가우스-뉴턴 방법",
    "text": "4 가우스-뉴턴 방법\n\\(\\hat{\\boldsymbol{y}}(\\boldsymbol{p})\\) 가 최적해 근처에 있다고 하면\n\\[\n\\hat{\\boldsymbol{y}}(\\boldsymbol{p}+\\boldsymbol{h}) \\approx \\hat{\\boldsymbol{y}}(\\boldsymbol{p}) + \\boldsymbol{J}\\boldsymbol{h},\\qquad \\text{where } J_{ij}= \\dfrac{\\partial y_i(\\boldsymbol{p})}{\\partial p_j}\n\\]\n이며, 따라서 식 4 를 생각하면 (잠시 \\(\\hat{\\boldsymbol{y}}(\\boldsymbol{p})=\\hat{\\boldsymbol{y}}\\) 라고 쓰자.)\n\\[\n\\begin{aligned}\nE(\\boldsymbol{p}+\\boldsymbol{h}) & = (\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}+\\boldsymbol{h}))^T\\boldsymbol{W}(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}+\\boldsymbol{h})) \\\\[0.3em]\n&\\approx (\\boldsymbol{y}-\\hat{\\boldsymbol{y}}-\\boldsymbol{Jh})^T\\boldsymbol{W}(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}-\\boldsymbol{Jh}) \\\\[0.3em]\n&= \\boldsymbol{y}^T\\boldsymbol{Wy} - \\hat{\\boldsymbol{y}}^T\\boldsymbol{Wy} - \\boldsymbol{yW}\\hat{\\boldsymbol{y}} - (\\boldsymbol{y}-\\hat{\\boldsymbol{y}})^T\\boldsymbol{WJh} - \\boldsymbol{h}^T\\boldsymbol{J}^T\\boldsymbol{W}(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}) + \\boldsymbol{h}^T\\boldsymbol{J}^T\\boldsymbol{WJh}\n\\end{aligned}\n\\]\n이다. 그렇다면\n\\[\n\\nabla E(\\boldsymbol{p}+\\boldsymbol{h}) = -2(\\boldsymbol{y}-\\hat{\\boldsymbol{y}})^T\\boldsymbol{WJ} + 2 \\boldsymbol{h}^T\\boldsymbol{J}^T\\boldsymbol{WJ}\n\\]\n이며, 따라서 최소값이 되는 \\(\\boldsymbol{h}_m\\) 은 \\(\\nabla E(\\boldsymbol{p}+\\boldsymbol{h}_m)=\\boldsymbol{0}\\) 을 만족해야 하므로\n\\[\n\\boldsymbol{h}_m = \\left(\\boldsymbol{J}^T\\boldsymbol{WJ}\\right)^{-1}\\boldsymbol{J}^T\\boldsymbol{W}(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}))\n\\tag{6}\\]\n이다. \\(\\boldsymbol{p}_k\\) 가 주어졌을 때 \\[\n\\boldsymbol{p}_{k+1} = \\boldsymbol{p}_k+\\boldsymbol{h}_m = \\boldsymbol{p}_k + \\left(\\boldsymbol{J}^T\\boldsymbol{WJ}\\right)^{-1}\\boldsymbol{J}^T\\boldsymbol{W}(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}))\n\\tag{7}\\]\n로 진행 시킬 수 있다.",
    "crumbs": [
      "다양한 주제들",
      "Levenberg-Marquardt 방법"
    ]
  },
  {
    "objectID": "src/topics/nonlinear_least_square_fit.html#levenberg-marquardt-방법",
    "href": "src/topics/nonlinear_least_square_fit.html#levenberg-marquardt-방법",
    "title": "Levenberg-Marquardt 방법",
    "section": "5 Levenberg-Marquardt 방법",
    "text": "5 Levenberg-Marquardt 방법\nLevenberg-Marquardt 방법은 Gradient-Descent와 Gauss-Newton가 서로 보완하는 형태의 알고리즘이다.\n\nGradient-Descent 방식은 1차 미분으로 이동 방향을 잘 설정하지만, 이동 크기는 잘 설정하지 못한다.\nGauss-Newton 방식은 2차 미분으로 해 주변에서 크기를 잘 설정하지만, 극소점이 아닌 극대점으로도 수렴 할 수 있는 문제를 갖고 있다.\nLevenberg–Marquardt 방법은 가우스-뉴턴법보다 안정적으로 해를 찾을 수 있으며(초기값이 해로부터 멀리 떨어진 경우에도 해를 찾을 확률이 높음) 비교적 빠르게 해에 수렴하기 때문에 비선형 최소자승문제에 있어서는 대부분 Levenberg–Marquardt 방법이 사용된다.\n\nLevenberg-Marquardt 방법은 식 5 과 식 7 을 합쳐서\n\\[\n\\boldsymbol{p}_{k+1} = \\boldsymbol{p}_k + \\boldsymbol{h}_{lm} = \\boldsymbol{p}_k + \\left[\\boldsymbol{J}^T\\boldsymbol{WJ} + \\lambda \\cdot\\text{diag}(\\boldsymbol{J}^T\\boldsymbol{WJ})\\right]^{-1}\\boldsymbol{J}^T\\boldsymbol{W}(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}))\n\\tag{8}\\]\n를 사용한다. 여기서 \\(\\text{diag}(\\boldsymbol{J}^T\\boldsymbol{WJ})\\) 는 \\(\\boldsymbol{J}^T\\boldsymbol{WJ}\\) 의 대각성분만으로 이루어진 행렬이며, \\(\\boldsymbol{h}_{lm}\\) 은 Levenberg-Marquardt 방법에 의해 \\(\\boldsymbol{p}\\) 값을 update 하는 값이다.\n많은 경우 \\(\\boldsymbol{W}=\\boldsymbol{I}\\) 로 간주할 경우가 많으며 이때는\n\\[\n\\boldsymbol{p}_{k+1} = \\boldsymbol{p}_k + \\boldsymbol{h}_{lm} = \\boldsymbol{p}_k + \\left[\\boldsymbol{J}^T\\boldsymbol{J} + \\lambda \\cdot \\text{diag}(\\boldsymbol{J}^T\\boldsymbol{J})\\right]^{-1}\\boldsymbol{J}^T(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p}))\n\\tag{9}\\]\n이다.",
    "crumbs": [
      "다양한 주제들",
      "Levenberg-Marquardt 방법"
    ]
  },
  {
    "objectID": "src/topics/nonlinear_least_square_fit.html#구현",
    "href": "src/topics/nonlinear_least_square_fit.html#구현",
    "title": "Levenberg-Marquardt 방법",
    "section": "6 구현",
    "text": "6 구현\n\n6.1 Iteration 조건의 구현\nGavin (2024) 에서 제시된 방법을 따른다. 식 8 를 다시 보자. 여기서 \\(\\boldsymbol{D}=\\text{diag}(\\boldsymbol{J}^T\\boldsymbol{WJ})\\) 라고 할때 일종의 메트릭 \\(\\rho_k\\) 를 다음과 같이 정의한다.\n\\[\n\\begin{aligned}\n\\rho_k &:= \\dfrac{E(\\boldsymbol{p}_k)- E(\\boldsymbol{p}_{k+1})}{\\|\\boldsymbol{h}^T \\cdot \\lambda_k \\text{diag}(\\boldsymbol{D})\\cdot \\boldsymbol{h} + \\boldsymbol{J}^T\\boldsymbol{W}(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p_k}))\\|},\\\\[0.3em]\n&\\qquad \\qquad \\qquad \\text{where }\\boldsymbol{h}=\\boldsymbol{h}_{lm}=\\left[\\boldsymbol{J}^T\\boldsymbol{J} + \\lambda \\cdot \\text{diag}(\\boldsymbol{J}^T\\boldsymbol{J})\\right]^{-1}\\boldsymbol{J}^T(\\boldsymbol{y}-\\hat{\\boldsymbol{y}}(\\boldsymbol{p})).\n\\end{aligned}\n\\]\n우리는 \\(E(\\boldsymbol{p}_k)&gt; E(\\boldsymbol{p}_{k+1})\\) 일 것을 기대하며 또한 충분히 클것을 기대한다. 어떤 기준 \\(\\epsilon_4&gt;0\\) 에 대해 \\(\\rho_k &gt; \\epsilon_4\\) 이면 \\(\\boldsymbol{p}_{k+1}\\) 을 잘 얻은것으로 간주한다. 그렇지 않다면 \\(\\boldsymbol{p}_{k+1}=\\boldsymbol{p}_k\\) 로 유지시키고 \\(\\lambda_{k+1}\\) 를 \\(\\lambda_k\\) 에 비해 적은 값으로 정한다. Gavin (2024) 에서는 이 과정에 대한 세가지 방법이 제시되며 이 가운데 첫번째 방법이 가장 좋은 결과를 보인다고 한다. 여기서도 이 방법을 따르기로 한다. 이 과정은 다음과 같다.\n (\\(1\\)) \\(\\lambda_0, \\,\\epsilon_4,\\, L_{\\uparrow},\\, L_\\downarrow\\) 를 이용자가 정한다.\n (\\(2\\)) 식 8 따라 \\(\\boldsymbol{p}_{k+1}\\) 을 계산한다.\n (\\(4\\)) \\(\\rho_k&gt;\\epsilon_4\\) 일 경우 \\(\\boldsymbol{p}_{k+1}\\) 을 채택하며 \\(\\lambda_{k+1} = \\max \\{\\lambda_k/L_{\\downarrow},\\, 10^{-7}\\}\\) 로 정한다.\n (\\(5\\)) \\(\\rho_k \\le \\epsilon_4\\) 일 경우 \\(\\boldsymbol{p}_{k+1}=\\boldsymbol{p}\\) 로 유지하고 \\(\\lambda_{k+1} =  \\min \\{\\lambda_k L^{\\uparrow},\\, 10^7\\}\\) 로 정한다.\n\nGavin (2024) 에 따르면 \\(L_{\\uparrow}\\approx 11\\) 일 때, \\(L_{\\downarrow} \\approx 9\\) 에서 가장 좋은 성능을 보인다고 한다.\n\n\n\n6.2 야코비 행렬\n야코비 행렬은 수치해석적으로 전방 차분을 이용하여\n\\[\nJ_{ij} = \\dfrac{f(x_i;\\, \\boldsymbol{p}+\\delta \\boldsymbol{e}_j)-f(x_i;\\, \\boldsymbol{p})}{\\delta}\n\\tag{10}\\]\n로 구하거나 중앙차분을 이용하여\n\\[\nJ_{ij} = \\dfrac{f(x_i;\\, \\boldsymbol{p}+\\delta \\boldsymbol{e}_j)-f(x_i;\\, \\boldsymbol{p}-\\delta \\boldsymbol{e}_j)}{2\\delta}\n\\tag{11}\\]\n를 통해 구한다. 두 경우 모두 피팅 매개변수 \\(\\boldsymbol{p}\\) 의 개수가 많을 경우, \\(\\boldsymbol{y}\\) 가 많을 경우 야코비 행렬의 계산량이 급격하게 증가한다. Broyden rank-1 update formula 를 사용하면\n\\[\n\\boldsymbol{J}_{k+1}=\\boldsymbol{J}_k + \\dfrac{1}{\\boldsymbol{h}^T\\boldsymbol{h}}\\left[\\hat{\\boldsymbol{y}}(\\boldsymbol{p}_{k+1}) - \\hat{\\boldsymbol{y}}(\\boldsymbol{p}_k) - \\boldsymbol{J}_k(\\boldsymbol{p}_{k+1}-\\boldsymbol{p}_k)\\right]\\boldsymbol{h}^T\n\\]\n이다.",
    "crumbs": [
      "다양한 주제들",
      "Levenberg-Marquardt 방법"
    ]
  }
]