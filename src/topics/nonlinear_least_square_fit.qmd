---
title: "Levenberg-Marquardt 방법"

number-sections: true
number-depth: 2
crossref:
  chapters: false
---


{{< include ../../latexmacros.qmd >}}

</br>

## 참고 문헌 

(@Gavin2024, )

::: {#refs}



:::

</br>



## 비선형 최소 제곱 문제(nonlinear least squares problems)

측정된 데이터 $y_i$, $(i=1,\ldots,\, n)$ 와 모델 함수 $y=f(x;\bf{p})$ 사이의 편차를 최소로 하는 함수의 파라메터 $\bf{p}=(p_1,\ldots,\,p_m)$ 을 구하고 싶다고 하자. 이 때

$$
E(\bf{p}) = \sum_{i=1}^n \left[\dfrac{y_i - f(x_i;\bf{p})}{\sigma_{y_i}}\right]^2
$${#eq-levmar_error_function_with_sigma}

은 $\bf{p}$ 에 대한 함수이며 $E(\bf{p})$ 를 최소화 하는 $\bf{p}^\ast$ 를 찾는 것이 비선형 최소자승법이다. 여기서 $\sigma_{y_i}$ 는 $y_i$ 측정에 대한 측정 오차이다. 예를 들어 방사능이나 입자 실험에서 많은 경우처럼 측정값이 푸아송 분포를 따르는 경우 $\sigma_{y_i}= \sqrt{y_i}$ 가 될 수 있다. @eq-levmar_error_function_with_sigma 를 좀 다르게 표현하면 $w_i = (1/\sigma_{y_i})^2$ 에 대해

$$
\bf{p}^\ast = \arg \min \sum_{i=1}^{n} w_i (y_i - f(x_i;\bf{p}))^2
$${#eq-levmar_error_function_mathematical_form}

를 찾는것이라고 할 수 있다. 

</bf>

이제 측정값을 $\bf{y}=\begin{bmatrix} y_1 & \cdots &y_n\end{bmatrix}^T$, 모델값을 $\hat{\bf{y}}(\bf{p}) = \begin{bmatrix} f(x_1;\bf{p}) & \cdots &f(x_n;\bf{p})\end{bmatrix}^T$ 라고 하자. 또한 가중치 행렬 $\bf{W}$ 를 $W_{ij}= \dfrac{\delta_{ij}}{\sigma_{y_i}^2}$ 라고 하면

$$
E(\bf{p}) = (\bf{y}-\hat{\bf{y}}(\bf{p}))^T \cdot \bf{W} \cdot (\bf{y}-\hat{\bf{y}}(\bf{p})) 
$${#eq-levmar_error_function_matrix_form}

이다. 참고로 $\bf{W}$ 는 대각행렬이며 (따라서) 대칭행렬이다.




</br>

## Gradient-Descent Method

$E(\bf{p})$ 의 최소값은 $E(\bf{p})$ 의 미분이 $0$ 인 점에서 나타난다. $E(\bf{p})$ 가 벡터에 대한 이차형식이며 $\bf{W}$ 가 대칭행렬이므로 
$$
\begin{aligned}
\dfrac{\partial E(\bf{p})}{\partial p_i} &= -2(\bf{y}-\hat{\bf{y}}(\bf{p}))^T \cdot \bf{W} \cdot \dfrac{\partial \hat{\bf{y}}(\bf{p})}{\partial p_i} 
\end{aligned}
$$

로 나타 낼 수 있다. 즉 $\hat{\bf{y}}(\bf{p})$ 에 대한 야코비안 행렬 $\bf{J}$ 에 대해 

$$
\nabla E(\bf{p}) =  -2(\bf{y}-\hat{\bf{y}}(\bf{p}))^T  \bf{WJ} ,\qquad \text{where }J_{ij} = \dfrac{\partial \hat{y}_i(\bf{p})}{\partial p_j}
$$

이다. $\bf{y} = (y_1,\ldots,\,y_n),\, \bf{p}=(p_1,\ldots,\,p_m)$ 이므로 $\nabla E(\bf{p})$ 는 $1\times m$ 행렬임에 유의하자. 즉 $\bf{p}$ 에 대해 $-(\nabla E(\bf{p}))^T$ 방향이 $E(\bf{p})$ 를 감소시키는 방향이다. 그러나 $-\nabla E(\bf{p})$ 값이 크다면 최소값 지점을 넘어갈 수 있으므로 보통 작은 값 $\lambda$ 를 곱해서 조금씩 진전시킨다. 이제 파라미터 $\bf{p}$ 에 대한 초기값 $\bf{p}_0$ 가 주어졌다면

$$
\bf{p}_{k+1} = \bf{p}_k - \lambda\left(\nabla E(\bf{p})\right)^T = \bf{p}_k + \lambda  \bf{J}^T\bf{W}(\bf{y}-\hat{\bf{y}}(\bf{p}))
$$ {#eq-levmar_gradient_decent_solution}

를 통해 $E(\bf{p})$ 를 최소화 시키는 $\bf{p}^\ast$ 를 찾을 수 있다.

</br>

## 뉴턴 방법

뉴턴 방법 혹은 뉴턴-랩슨 방법은 [수치해석 I-뉴턴 방법](../numerical_analysis_using_julia/07_finding_root.qmd#newton-method) 에서 보았듯이 함수 $f(x)$ 에 대해 $f(x)=0$ 의 해를

$$
x_{k+1} = x_{k} - \dfrac{f(x_k)}{f'(x_k)}
$$

를 반복하여 수열 $\langle x_k\rangle$ 이 수렴하는 점으로 구하는 방법이다.  그러나 이 방법은 1변수 함수에 대한 방법이므로 다변수 함수에 대해 알아보자

</br>

### 다변수 함수에 대한 뉴턴-랩슨 방법

$f:\R^n \to \R$ 이 $C^2$ 급 함수라고 하자. 이 함수에 대한 2차 테일러 전개는 $f$ 에 대한 $\bf{x}$ 에서의 헤세 행렬(Hessian matrix) $\bf{H}$ 에 대해

$$
f(\bf{x}+\bf{h}) \approx f(\bf{x})+ \nabla f(\bf{x}) \cdot \bf{h} + \dfrac{1}{2}\bf{h}^T \cdot \bf{H}\cdot \bf{h}
$$ {#eq-levenmar_multivariable_1}

이다. 여기서 $f$ 에 대한 $\bf{x}$ 에서의 헤세 행렬(Hessian matrix) $\bf{H}$ 는 $H_{ij} = \dfrac{\partial^2 f(\bf{x})}{\partial x_i x_j}$ 이며 따라서 대칭행렬이다. 

이제 대칭행렬 $\bf{C} \in \mathcal{M}_{n \times n}(\R)$ 과 $\bf{b}\in \mathcal{M}_{n}(\R)$ 이 주어졌을 때 $\bf{x}\in \mathcal{M}_{n}(\R)$ 에 대한 아래와 같은 방정식을 생각하자.

$$
f(\bf{x})= a + \bf{b}^T \bf{x} + \bf{x}^T\bf{C}\bf{x}
$$

그렇다면

$$
\nabla f(\bf{x}) = \bf{b}^T + 2 \bf{x}^T\bf{C}
$$

이며, $\bf{x}^T = - \dfrac{1}{2}\bf{b}^T\bf{C}^{-1}$ 일 때 $f(\bf{x})=0$ 이므로

$$
\bf{x} = - \dfrac{1}{2}\bf{C}^{-1}\bf{b}
$$

가 $f(\bf{x})$ 의 해 이다. (행렬 $\bf{C}$ 가 대칭이며 가역이라면 $\bf{CC}^{-1}=\bf{I}$ 이므로 $(\bf{C}^{-1})^T\bf{C}^T=\bf{I}^T=\bf{I}$ 이다. $\bf{C}^T=\bf{C}$ 이므로 $(\bf{C}^{-1})^T=\bf{C}^{-1}$ 이다.)


이제 @eq-levenmar_multivariable_1 로 돌아가고 $\bf{H}$ 가 positive definite 임을 가정하자. 즉 임의의 $\bf{x}\in \mathcal{M}_n$ 에 대해 $\bf{x}^T\bf{Hx} \ge 0$ 이다. 그렇다면 

$$
\bf{x}+\bf{h} = \bf{x}-\bf{H}^{-1} \cdot \left(\nabla f(\bf{x})\right)^T
$$

일 때 $f(\bf{x}+\bf{h})$ 는 최소값을 가진다. 


</br>

이제 이를 $E(\bf{p})$ 에 적용해 보면, 뉴턴 방법은

$$
\bf{p}_{k+1} = \bf{p}_k - \bf{H}^{-1}(E(\bf{p})) \cdot (\nabla_\bf{p} E(\bf{p}))^T
$$

이며, 수렴 속도를 조절하기 위해 $0<\lambda <1$ 에 대해

$$
\bf{p}_{k+1} = \bf{p}_k -  \lambda \bf{H}^{-1}(E(\bf{p})) \cdot (\nabla_\bf{p} E(\bf{p}))^T
$$


로 전행한다.


</br>

## 가우스-뉴턴 방법

$\hat{\bf{y}}(\bf{p})$ 가 최적해 근처에 있다고 하면

$$
\hat{\bf{y}}(\bf{p}+\bf{h}) \approx \hat{\bf{y}}(\bf{p}) + \bf{J}\bf{h},\qquad \text{where } J_{ij}= \dfrac{\partial y_i(\bf{p})}{\partial p_j} 
$$

이며, 따라서 @eq-levmar_error_function_matrix_form 를 생각하면 (잠시 $\hat{\bf{y}}(\bf{p})=\hat{\bf{y}}$ 라고 쓰자.)


$$
\begin{aligned}
E(\bf{p}+\bf{h}) & = (\bf{y}-\hat{\bf{y}}(\bf{p}+\bf{h}))^T\bf{W}(\bf{y}-\hat{\bf{y}}(\bf{p}+\bf{h})) \\[0.3em]
&\approx (\bf{y}-\hat{\bf{y}}-\bf{Jh})^T\bf{W}(\bf{y}-\hat{\bf{y}}-\bf{Jh}) \\[0.3em]
&= \bf{y}^T\bf{Wy} - \hat{\bf{y}}^T\bf{Wy} - \bf{yW}\hat{\bf{y}} - (\bf{y}-\hat{\bf{y}})^T\bf{WJh} - \bf{h}^T\bf{J}^T\bf{W}(\bf{y}-\hat{\bf{y}}) + \bf{h}^T\bf{J}^T\bf{WJh}
\end{aligned}
$$

이다. 그렇다면

$$
\nabla_\bf{p}E(\bf{p}+\bf{h}) = -2(\bf{y}-\hat{\bf{y}})^T\bf{WJ} + 2 \bf{h}^T\bf{J}^T\bf{WJ} 
$$

이며 최소값이 되는 $\bf{h}_m$ 은

$$
\bf{h}_m = \left(\bf{J}^T\bf{WJ}\right)^{-1}\bf{J}^T\bf{W}(\bf{y}-\hat{\bf{y}}(\bf{p}))
$${#eq-levmar_gauss_newton_solution_0}

이며 따라서 $\bf{p}_k$ 가 주어졌을 때 
$$
\bf{p}_{k+1} = \bf{p}_k+\bf{h}_m = \bf{p}_k + \left(\bf{J}^T\bf{WJ}\right)^{-1}\bf{J}^T\bf{W}(\bf{y}-\hat{\bf{y}}(\bf{p}))
$${#eq-levmar_gauss_newton_solution}


로 진행 시킬 수 있다.


<br>

## Levenberg-Marquardt 방법

Levenberg-Marquardt 방법은 Gradient-Descent와 Gauss-Newton가 서로 보완하는 형태의 알고리즘이다.

- Gradient-Descent 방식은 1차 미분으로 이동 방향을 잘 설정하지만, 이동 크기는 잘 설정하지 못한다.

- Gauss-Newton 방식은 2차 미분으로 해 주변에서 크기를 잘 설정하지만, 극소점이 아닌 극대점으로도 수렴 할 수 있는 문제를 갖고 있다.

- Levenberg–Marquardt 방법은 가우스-뉴턴법보다 안정적으로 해를 찾을 수 있으며(초기값이 해로부터 멀리 떨어진 경우에도 해를 찾을 확률이 높음) 비교적 빠르게 해에 수렴하기 때문에 비선형 최소자승문제에 있어서는 대부분 Levenberg–Marquardt 방법이 사용된다.


이제 @eq-levmar_gradient_decent_solution 과 @eq-levmar_gauss_newton_solution 을 합쳐서

$$
\bf{p}_{k+1} = \bf{p}_k + \left[\bf{J}^T\bf{WJ} + \lambda \cdot\text{diag}(\bf{J}^T\bf{WJ})\right]^{-1}\bf{J}^T\bf{W}(\bf{y}-\hat{\bf{y}}(\bf{p}))
$$ {#eq-levmar_levenberg_marquardt_with_weight}

이다. 많은 경우 $\bf{W}=\bf{I}$ 이며 이 경우

$$
\bf{p}_{k+1} = \bf{p}_k + \left[\bf{J}^T\bf{J} + \lambda \cdot \text{diag}(\bf{J}^T\bf{J})\right]^{-1}\bf{J}^T(\bf{y}-\hat{\bf{y}}(\bf{p}))
$${#eq-levmar_levenberg_marquardt_without_weight}




```julia
function jacobian(f::Function, xarr::AbstractVector{<:Real}, params::AbstractVector{T}, ϵ = 0.001) where T<:Real

    J = Array{Float64}(undef, (length(xarr), length(params)))
    @inbounds for j ∈ 1:length(params), i ∈ 1:length(xarr)
        p1, p2 = params[:], params[:]
        p1[j] += ϵ
        p2[j] -= ϵ
        J[i, j] = (f(xarr[i], p1)-f(xarr[i], p2))/(2*ϵ)
    end
    return J

end

function err(xarr::AbstractVector{<:Real}, yarr::AbstractVector{<:Real}, f::Function, params)
    h(x) = f(x, params)
    return sum((yarr .- h.(xarr)).^2)
end

function LM(f::Function, xarr::AbstractArray{<:Real}, yarr::AbstractArray{<:Real}, params; λ::Real = 1.0e-3, ϵ::Real=1.0e-3, MaxIteration = 10000)
    Nitter = 0
    p = params[:]
    ee = err(xarr, yarr, f, p)
    
    qs = 0.0
    # J = jacobian(f, xarr, p, ϵ)
    
    
    while Nitter < MaxIteration
        Nitter += 1
        J = jacobian(f, xarr, p, ϵ)
        JtJ = J'*J
        p = p .+ inv(JtJ .+ λ .* Diagonal(diag(JtJ))) * J' * (yarr .- [f(t, p) for t in xarr])
        
        ee0 = err(xarr, yarr, f, p)
        qs = abs(ee - ee0)/ee
        if qs < ϵ 
            break
        else 
            ee = ee0
        end
        println( (p, qs, Nitter))
    end
    return (p, qs, Nitter)
end
```

```julia
q(x, p) = p[1]*sin(x) + p[2]
t=0.0:0.1:10.0
data = 5.0*sin.(t) .+ 7.0 .+ (0.3.*rand(length(t)))

# jacobian(q, t, [2.0, 1], 0.001)
LM(q, collect(t), data, [1.0, -1.0], λ = 0.001, ϵ = 0.000001 , MaxIteration = 100)
```

```txt
([5.012957019746288, 7.14253888593573], 0.9999272905261456, 1)
([5.013960904057669, 7.150493532354603], 0.010698591624230576, 2)

([5.013958759822676, 7.150501863003215], 1.058330305524245e-8, 3)
```