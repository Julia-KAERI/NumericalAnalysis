---
title: "Approximation"

number-sections: true
number-depth: 3
crossref:
  chapters: false
---

## 최소 자승법

### 선형 최소 자승법

아래 그림과 같이 $\{(x_i,\,y_i) : i = 1,\ldots, N\}$ 가 주어졌을 때 이 데이터에 가장 근접하는 선형 방정식 $y=ax+b$ 를 찾아야 할 때가 있다.

![선형 최소 자승법](figure/linear_approx/fig-linear_approximation-1.png){#fig-linear_lieast_seuare width=250}

아래와 같은 함수 $E(a,\,b)$ 를 보자.
$$
E(a, b) = \sum_{i=1}^N \left[ y_i - (ax_i + b)\right]^2
$$

이 함수는 모든 구간에서 미분가능하다는 매우 좋은 성질을 가지고 있다. 또한 $a,\,b$ 값이 상당히 벗어날 경우 매우 커지며, $y_i$ 가 $ax_i +b$ 와 일치할 경우 $0$ 이 된다. 

$$
\begin{aligned}
\dfrac{\partial E}{\partial a} &= -2\sum_{i=1}^N x_i(y_i - ax_i - b), \\
\dfrac{\partial E}{\partial b} &= -2\sum_{i=1}^N (y_i - ax_i -b), \\
\dfrac{\partial^2 E}{\partial a \partial a }& = \sum_{i=1}^N 2{x_i}^2, \\
\dfrac{\partial^2 E}{\partial a \partial b} &= \sum_{i=1}^N 2x_i, \\
\dfrac{\partial^2 E}{\partial b \partial b}&= 2N
\end{aligned}
$$

이며 이로부터 해세 행렬 (Hassian matrix)

$$
\boldsymbol{H} = \begin{bmatrix} \dfrac{\partial^2 E}{\partial a^2 } & \dfrac{\partial^2 E}{\partial a \partial b} \\ \dfrac{\partial^2 E}{\partial a \partial b} & \dfrac{\partial^2 E}{\partial b^2} \end{bmatrix} = \begin{bmatrix} {\displaystyle \sum_{i=1}^N 2{x_i}^2} & {\displaystyle \sum_{i=1}^N 2x_i}, \\ {\displaystyle \sum_{i=1}^N 2x_i} & 2N \end{bmatrix}
$$

을 얻는다. 위 해세 행렬의 행렬식은

$$
D = \det (\boldsymbol{H}) = 4 \left[N\left(\sum_i^N {x_i}^2 \right)- \left(\sum_i^N x_i\right)^2\right]
$$


이며, 코시-슈바르츠 부등식으로 $D\ge 0$ 이며 $D=0$ 일 때는 $x_1 = \cdots = x_i$ 일 때임을 보일 수 있다. 이것을 제외하면 $D>0$ 이며 $E(a,\,b)$ 는 극소값을 가진다는 것을 안다. 극소값을 가질 때는 $\dfrac{\partial E}{\partial a}= \dfrac{\partial E}{\partial b}=0$ 일 때이므로, 

$$
\begin{aligned}
\dfrac{\partial E}{\partial a}= 0 &\implies \sum_i x_i y_i - a \left(\sum_i {x_i}^2\right) - b \left(\sum_{i}x_i\right) = 0, \\
\dfrac{\partial E}{\partial b} = 0 &\implies \sum_i y_i - a\left(\sum_i x_i\right) - b N = 0
\end{aligned}
$$

이다. 위 식은 $a,\,b$ 에 대한 연립방정식이며 이것을 풀면 다음과 같다.

$$
a = \dfrac{N \left( \displaystyle \sum_{i=1}^N x_i y_i\right) - \left(\displaystyle \sum_{i=1}^N x_i\right) \left( \displaystyle \sum_{i=1}^N y_i\right)}{N \left(\displaystyle \sum_{i=1}^N {x_i}^2\right) - \left(\displaystyle \sum_{i=1}^N x_i\right)^2}, \qquad 
b=  \dfrac{\left( \displaystyle \sum_{i=1}^N {x_i}^2\right)  \left( \displaystyle \sum_{i=1}^N y_i\right) - \left(\displaystyle \sum_{i=1}^N x_iy_i\right) \left( \displaystyle \sum_{i=1}^N x_i\right)}{N \left(\displaystyle \sum_{i=1}^N {x_i}^2\right) - \left(\displaystyle \sum_{i=1}^N x_i\right)^2}, 
$$

언뜻 복잡해 보이지만 프로그래밍 입장에서 보면 베열의 합, 베열의 곱의 합, 베열의 제곱의 합, 배열의 합의 제곱에 대한 사칙연산 뿐이며, 쉽게 코딩 할 수 있다. 

</br>

### 지수함수와 멱함수의 최소자승법

$y=be^{ax}$ 꼴의 지수함수의 경우 각각 $\ln y = ax + \ln b$ 로 변형시킨 후 $\overline{y}=\ln y,\, \overline{b} = \ln b$ 로 놓으면

$$
\overline{y} = ax + \overline{b}
$$

의 선형방정식이 된다. 따라서 선형방정식의 최소자승법으로 $a$ 와 $\overline{b}$ 를 구할 수 있기 때문에 지수함수에 대해서는 최소자승법을 사용 할 수 있다. $y=dx^c$ 꼴의 멱함수의 경우에는 $\ln y = \ln d + c\ln x$ 이므로 $\overline{y} = \ln y$, $\overline{x} = \ln x$, $\overline{d} = \ln d$ 로 놓으면

$$
\overline{y} = c \overline{x} + \overline{d}
$$

를 이용해 역시 최소자승법으로 $c,\, d$ 를 구할 수 있다.

</br>

### 다항식 최소 자승법

데이터 $\{(x_k,\,y_k):k=1,\,N\}$ ($x_1<x_2<\cdots < x_N$) 과 $n$ 차 다항식

$$
p_n (x) = a_n x^n + a_{n-1}x^{n-1} + \cdots + a_1 x + a_0
$$

에 대해 

$$
E(a_n,\ldots,\,a_0) = \sum_{k=1}^N \left[ y_k - p_n(x_k)\right]^2
$$

를 최소로 하는 $a_0,\ldots,\,a_n$ 을 찾는다. 이 경우 $N=n+1$ 이면 다항식을 이용한 보간법이 되어 $E=0$ 이 되도록 하는 $a_0,\ldots,\,a_n$ 이 존재한다는 것을 안다. 보통 최소자승법은 $N>n+1$ 일 경우의 가장 좋은 다항식 $p_n$ 을 찾는 것을 말한다. 여기서도 $n<N-1$ 임을 가정하겠다. 

$$
\begin{aligned}
E(a_n,\ldots,\,a_0) &= \sum_{k=1}^N {y_k}^2 -2 \sum_{k=1}^N y_k p_n(x_i) + \sum_{k=1}^N (p_n(x_k))^2\\
&= \sum_{k=1}^N {y_k}^2 - 2 \sum_{k=1}^N \sum_{i=0}^n a_i y_k {x_k}^j + \sum_{k=1}^N \sum_{i,\,j=1}^n a_i a_j {x_k}^{i+j} \\
&= \sum_{k=1}^N {y_k}^2 - 2\sum_{i=0}^n \left(\sum_{k=1}^N (x_k)^i y_k\right) a_i + \sum_{i, j=0}^n \left(\sum_{k=1}^N {x_k}^{i+j}\right) a_i a_j
\end{aligned} 
$$

이며 $E$ 를 최소로 하는 극값은 $i=0,\ldots,n$ 에 대해 

$$
\dfrac{\partial E}{\partial a_i}=0 \implies \sum_{k=1}^N \left[ (x_k)^i y_k- \sum_{j=0}^n  (x_k)^{i+j} a_j\right] = 0
$$ {#eq-least_square_for_polynomial}

를 만족해야 한다. 이제 행렬 $\boldsymbol{X} \in \mathcal{M}_{N\times (n+1)}(\mathbb{F}),\,\boldsymbol{a} \in\mathcal{M}_{n+1}(\mathbb{F}),\,\boldsymbol{y}\in \mathcal{M}_N(\mathbb{F})$ 를 다음과 같이 정의하자.

$$
X_{ij} = (x_i)^{j-1},\qquad \boldsymbol{a} = \begin{bmatrix} a_0 & \cdots & a_n\end{bmatrix}^T, \qquad \boldsymbol{y} = \begin{bmatrix} y_1 & \cdots & y_N \end{bmatrix}
$$

그렇다면,

$$
(\boldsymbol{X}^T\boldsymbol{X})_{ij} = \sum_{k=1}^N X_{ki}X_{kj} = \sum_{k=1}^N x_{k}^{i+j-2}
$$

이므로 @eq-least_square_for_polynomial 는 다음과 같은 선형방정식이 된다. 아래와 같은 형태의 방정식을 **정규 방정식(normal equation)** 이라고 한다. 

$$
\boldsymbol{X}^T\boldsymbol{Xa} = \boldsymbol{X}^T\boldsymbol{y}
$$ {#eq-normal_equation}






</br>

::: {#exr-nullity_of_X}

위에서 정의된 $\boldsymbol{X}$ 에 대해 다음을 보여라.

&emsp; ($1$) $\text{nullity} (\boldsymbol{X})=0$.

&emsp; ($2$) $\boldsymbol{X}^T\boldsymbol{X}$ 는 가역행렬이다.

:::

::: {.solution}

($1$) $\text{nullity} (\boldsymbol{x}) > 0$ 이라면 어떤 nontrivial 한 $\boldsymbol{a}$ 에 대해 $\boldsymbol{Xa}=\boldsymbol{0}$ 이어야 한다. 그렇다면, $\sum_{i=0}^n {x_k}^i a_i = 0$ ($i=1,\ldots,\,N$) 인데 $N>n+1$ 조건에서 $n$ 차 방정식이 $N > n+1$ 개의 근을 갖는다는 이야기이므로 모순이다. 우리는 이미 $x_1 < \cdots < x_N$ 을 가정했다. 따라서 $\boldsymbol{Xa}=\boldsymbol{0}$ 을 만족하는 $\boldsymbol{a}$ 는 $\boldsymbol{0}$ 뿐이다.

($2$) $\boldsymbol{X}^T\boldsymbol{X}$ 는 대칭행렬이며 $\text{nullity}(\boldsymbol{X})=0$ 이므로 non-trivial 한 벡터 $\boldsymbol{a}$ 에 대해 $\boldsymbol{a}^T\boldsymbol{X}^T\boldsymbol{Xa} = \|\boldsymbol{Xa}\|^2>0$ 이다. 따라서 $\boldsymbol{X}^T\boldsymbol{X}$ 는 가역이다.
:::

</br>

@exr-nullity_of_X 에 따라 $\boldsymbol{a} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$ 로 정해진다. 이것을 julia 로 구현하면 다음과 같다. `x`, `y` 의 베열을 입력하면 다항식의 계수를 order 에 대한 오름차순으로 반환한다. 즉 반환값의 첫번째 성분은 상수이다.

```julia
using LinearAlgebra

function least_square_poly(
    x::AbstractVector{<:Real}, 
    y::AbstractVector{<:Real}, 
    order::Integer)
    @assert length(x) == length(y)
    @assert order ≥ 2 && order < length(x)-1
    X = [(xi)^i for xi in x, i in 0:order]
    a = inv(X'*X) *X' * y
    return a
end
```

`NAJ.jl` 에서는 같은 이름 `least_square_poly` 함수가 정의되어 있다. 시그너쳐는 같고 반환값은 계수가 아니라 `SimplePolynomial` 객체이다. 

```julia
x = collect(-3:0.1:5)
r = polynomial_from_roots([-3, 1, 4])
y = r.(x) 
scatter(x, y, label = "Data")
p = least_square_poly(x, y, 3)
plot!(x, p.(x), label = "LeastSquared Polynoimal")
```

![다항식을 이용한 최소자승법](figure/linear_approx/least_square_poly.png){#fig-NA1_least_square_poly width=600}



</br>

## 직교 다항식을 이용한 최소자승법




### 내적벡터공간으로서의 함수의 집합

- 필드 $\mathbb{F}$ 에서 정의된 $x$ 를 변수로 갖는 $n$ 차 이하의 다항식의 집합을 $\mathbb{F}_n [x]$ 라고 하자. 또한 $x$ 를 변수로 하는 다항식 전체의 집합을 $\mathbb{F}[x]$ 라고 하자. 실수에서 정의된 2차 이하의 다항식의 집합은 $\mathbb{R}_2[x]$ 로 쓸 수 있다. 

- $[a,\,b]$ 구간에서 연속인 함수의 집합 $C_{[a,\,b]}$ 가 벡터공간이기 때문에 선형대수학에의 중요한 개념인 벡터, 선형독립, 기저와 같은 개념을 그대로 쓸 수 있다. 


</br>

::: {#exr-linearly_independency_of_different_order_polynomials}

$p_k(x)$ 가 각각 $k$ 차 다항식일 때 $\{p_1(x),\ldots,\,p_n(x)\}$ 은 선형독립이다.

:::


::: {.solution}

$c_1p_1(x) + \cdots + c_n p_n(x)=0$ 이라고 하자. $n$ 차항은 $p_n(x)$ 밖에 없으므로 $c_n=0$ 이다. $n-1$ 차항은 $p_{n-1}$ 박에 없으므로 $c_{n-1}=0$ 이다. 이것을 반복하면 $c_1= \cdots = c_n=0$ 을 보인다.

:::

</br>

$C_[a,\,b]$ 에서 내적을 어떤 정해진 함수 $w(x)$ 에 대해 다음과 같이 정의 할 수 있다.

$$
\langle \varphi,\, \psi \rangle := \int_{a}^b \varphi(x)\, \psi(x)\, w(x)\, dx
$$

이 때 $w(x)$ 를 **무게 함수(weight function)** 이라고 하며 (1) 모든 $x\in [a,\,b]$ 에서 $w(x)\ge 0$ 이며 (2) $[a,\,b]$ 에 포함되는 모든 $[c,\,d]$ 에서 $w([c,\,d]) \ne \{0\}$ 이다. 이렇게 정의된 내적에 대해 $\langle \varphi,\, \psi \rangle = 0$ 이면 $\varphi$ 와 $\psi$ 는 서로 직교한다(be orthoginal)고 한다. $\{\phi_1,\ldots,\,\phi_n\} \subset C_{[a,\,b]}$ 에 대해 $\langle \phi_i,\,\phi_j\rangle = c_i\delta_{ij}$, $c_{i} > 0$ 이면 $\{\phi_1,\ldots,\,\phi_n\}$ 을 직교하는 함수의 집합이라고 하며, $\langle \phi_i,\,\phi_j\rangle =\delta_{ij}$ 이면 **정규직교집합** 이라고 한다.


$p(x) \in C_{[a,\,b]}$ 가 직교하는 함수의 집합 $\{\phi_1,\ldots,\,\phi_n\}$ 의 선형결합이라고 하자. $p(x) = \sum_{i=1}^n a_n \phi_n (x)$ 이며 여기에 무게함수 $w(x)$ 정의되어 있다면

$$
\langle p(x),\, \phi_k(x) \rangle = a_k \langle \phi_k,\, \phi_k \rangle
$$

이다. $\{\phi_1,\ldots,\,\phi_n\}$ 가 정규직교집합이라면 $c_k=1$ 이므로 $a_k = \langle p(x),\,\phi_k(x)\rangle$ 이다. 

$C_{[a,\,b]}$ 이 내적벡터공간이며 내적벡터공간에서의 정규직교벡터-여기서는 직교하는 함수의 집합-을 생각 할 수 있으므로 당연히 이 정규하는 함수의 집합으로부터 정규직교집합을 구성 할 수 있다. 그 방법중 한가지는 그람-슈미트 과정이다. $\{\phi_1,\ldots,\,\phi_n\}$ 이 직교하는 함수의 집합이며 $\|f\|= \sqrt{\langle f,\, f\rangle}$ 라고 할 때, 

$$
\begin{aligned}
\psi_1(x) &= \dfrac{\phi_1(x)}{\|\phi_1(x)\|}, \\
\varphi_k (x) &= \phi_k(x) - \sum_{j=1}^{k-1} \langle \phi_k,\, \psi_j\rangle \psi_k(x),\\
\psi_k (x) &= \dfrac{\varphi_k(x)}{\|\varphi_k(x)\|}
\end{aligned}
$$

라고 정의하면 $\{\psi_1(x),\ldots,\,\psi_k(x)\}$ 는 정규직교집합이다.

</br>

### 르장드르 다항식

::: {.callout-note appearance="minimal"}

::: {#def-legendre_polynomials}
#### 르장드르 다항식


르장드르 다항식 $P_n(x)$ $(n=0,\,1,\ldots)$ 는 $[-1,\,1]$ 구간에서 다음과 같이 재귀적으로 정의되는 다항식이다.

$$
\begin{aligned}
P_0(x) & = 1, \\
P_1(x) & = x, \\
nP_n(x) & = (2n−1)xP_{n−1}(x)−(n−1)P_{n−2}(x).
\end{aligned}
$$

르장드르 다항식에 대한 무게함수는 $w(x)=1$ 이다.
:::
:::


![르장드르 다항식](figure/linear_approx/fig-Legendre_polynoimal.png){#fig-NA1_legendre_polynomials width=300}

르장드르 다항식은 르장드르 미분방정식이라고 불리는 2계 제차 미분방정식

$$
(1-x^2) y'' -2xy' + n(n+1)y=0
$$

의 해이며, 무게함수는 $w(x)=1$ 이다. 이 때, 

$$
\langle P_n(x),\, P_m(x) \rangle = \int_{-1}^1 P_n(x)P_m(x)\,dx =  \dfrac{2}{2n+1}\delta_{ij}
$$ {#eq-inner_product_of_legendre_polynomial}


이다. 

</br>

### 체비쇼프 다항식

체비쇼프 다항식은 [체비쇼프 미분방정식](A_ode.qmd#tbl-2nd_ode_singularity) 의 해로 $T_n(1) = 1,\, T_{n}(-1)= (-1)^n$ 의 경계조건을 만족하는 해이다.

::: {.callout-note appearance="minimal"}

::: {#def-chevyshev_polynomials}
#### 체비쇼프 다항식



1형 체비쇼프 다항식 $T_n(x)$ $(n=0,\,1,\ldots)$ 는 $[-1,\,1]$ 구간에서 다음과 같이 재귀적으로 정의되는 다항식이다.[체비쇼프 다항식은 1형과 2형이 있지만 여기서는 1형만 사용한다.]{.aside}

$$
\begin{aligned}
T_0(x) & = 1, \\
T_1(x) & = x, \\
T_{n}(x) & = 2xT_{n−1}(x) - T_{n-2}(x).
\end{aligned}
$$

체비쇼프 다항식의 무게함수는 $w(x) = \dfrac{1}{\sqrt{1-x^2}}$ 이다. 

:::
:::

![Chevyshev 다항식](figure/linear_approx/fig-Chevyshev_polynoimal.png){#fig-NA1_chevyshev_polynomials width=350}

체비쇼프 다항식의 내적에 대해

$$
\langle T_m,\, T_n \rangle = \int_{-1}^1 T_m(x) T_n(x) \dfrac{1}{\sqrt{1-x^2}}\,dx = \left\{\begin{array}{ll} 0, \qquad &m \ne n \\ \dfrac{\pi}{2},  & m = n \neq 0 \\ \pi, & m=n=0 \\ \end{array}\right.
$$

이 성립한다. 체비쇼프 미분방정식은

$$
(1-x)^2 \dfrac{d^2T_n}{dx^2} - xT_n (x) = -n^2 T_n(x)
$$

인데 여기서 $x=\cos\theta$ 로 놓으면, 위의 미분방정식은

$$
\dfrac{d^2T_n}{d\theta^2} + n^2 T_n = 0
$$

이 된다. 즉 $T_n(\theta) = n\cos\theta$ 이므로 

$$
T_n (x) = \cos (n \arccos x)
$$ {#eq-chevyshov_cos_form}

이다. 

@eq-chevyshov_cos_form 으로부터 우리는 $T_n(x) = 0$ 의 해는

$$
x_k =  \cos \left(\dfrac{2k\pi+\pi}{2n}\right),\qquad k=0,\,1,\ldots,, n-1
$$

임을 알 수 있다. 또한

$$
\overline{x}_k = \cos \left(\dfrac{k\pi}{n}\right),\qquad k=0,\ldots,\, n
$$

에 대해 $T_n (\overline{x}_k) = (-1)^k$ 임을 안다. 

